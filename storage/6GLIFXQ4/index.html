<!DOCTYPE html>
<html class="client-js" dir="ltr" lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<title>Feature learning - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Feature_learning","wgTitle":"Feature learning","wgCurRevisionId":846656918,"wgRevisionId":846656918,"wgArticleId":38870173,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Wikipedia articles needing clarification from June 2017","Machine learning"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Feature_learning","wgRelevantArticleId":38870173,"wgRequestId":"W2t8TgpAADoAAJH12VUAAADM","wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikiEditorEnabledModules":[],"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFExpandAllSectionsUserOption":true,"wgMFEnableFontChanger":true,"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q17013334","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":true,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@1dqfd7l",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});mw.loader.load(["ext.cite.a11y","ext.math.scripts","site","mediawiki.page.startup","mediawiki.user","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.3d","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});</script>
<link rel="stylesheet" href="load_002.css">
<script async="" src="load_002.php"></script>
<meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="load.css">
<link rel="stylesheet" href="load_003.css">
<meta name="generator" content="MediaWiki 1.32.0-wmf.15">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-crossorigin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="noindex,nofollow">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1200px-Kernel_Machine.svg.png">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit">
<link rel="edit" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit">
<link rel="apple-touch-icon" href="https://en.wikipedia.org/static/apple-touch/wikipedia.png">
<link rel="shortcut icon" href="https://en.wikipedia.org/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="https://en.wikipedia.org/w/opensearch_desc.php" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://en.wikipedia.org/w/api.php?action=rsd">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="https://en.wikipedia.org/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Feature_learning">
<link rel="dns-prefetch" href="https://login.wikimedia.org/">
<link rel="dns-prefetch" href="https://meta.wikimedia.org/">
<!--[if lt IE 9]><script src="/w/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
<script src="load.php"></script></head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-Feature_learning rootpage-Feature_learning skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>
			<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div><div class="mw-indicators mw-body-content">
</div>
<h1 id="firstHeading" class="firstHeading" lang="en">Feature learning</h1>			<div id="bodyContent" class="mw-body-content">
				<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>				<div id="contentSub"><div class="mw-revision"><div id="mw-revision-info-current"><table id="revision-info-current" class="plainlinks fmbox fmbox-system" role="presentation"><tbody><tr><td class="mbox-text"><b>This is the <a href="https://en.wikipedia.org/wiki/Help:Page_history" title="Help:Page history">current revision</a> of this page, as edited by <span id="mw-revision-name"><a href="https://en.wikipedia.org/w/index.php?title=User:Tbatta&amp;action=edit&amp;redlink=1" class="new mw-userlink" title="User:Tbatta (page does not exist)"><bdi>Tbatta</bdi></a> <span class="mw-usertoollinks">(<a href="https://en.wikipedia.org/wiki/User_talk:Tbatta" class="mw-usertoollinks-talk" title="User talk:Tbatta">talk</a>&nbsp;| <a href="https://en.wikipedia.org/wiki/Special:Contributions/Tbatta" class="mw-usertoollinks-contribs" title="Special:Contributions/Tbatta">contribs</a>)</span></span> at <span id="mw-revision-date">03:45, 20 June 2018</span><span id="mw-revision-summary"> <span class="comment">(<a href="#Neural_networks">→</a>‎<span dir="auto"><span class="autocomment">Neural networks</span></span>)</span></span>. The present address (URL) is a <a href="https://en.wikipedia.org/wiki/Help:Permanent_link" title="Help:Permanent link">permanent link</a> to this version.</b></td></tr></tbody></table><div id="revision-info-current-plain" style="display: none;">Revision as of 03:45, 20 June 2018 by <a href="https://en.wikipedia.org/w/index.php?title=User:Tbatta&amp;action=edit&amp;redlink=1" class="new mw-userlink" title="User:Tbatta (page does not exist)"><bdi>Tbatta</bdi></a> <span class="mw-usertoollinks">(<a href="https://en.wikipedia.org/wiki/User_talk:Tbatta" class="mw-usertoollinks-talk" title="User talk:Tbatta">talk</a>&nbsp;| <a href="https://en.wikipedia.org/wiki/Special:Contributions/Tbatta" class="mw-usertoollinks-contribs" title="Special:Contributions/Tbatta">contribs</a>)</span> <span class="comment">(<a href="#Neural_networks">→</a>‎<span dir="auto"><span class="autocomment">Neural networks</span></span>)</span></div>
</div><div id="mw-revision-nav">(<a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;diff=prev&amp;oldid=846656918" title="Feature learning">diff</a>) <a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;direction=prev&amp;oldid=846656918" title="Feature learning">← Previous revision</a>&nbsp;| Latest revision (diff)&nbsp;| Newer revision → (diff)</div></div></div>
				<div id="jump-to-nav"></div>				<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
				<a class="mw-jump-link" href="#p-search">Jump to search</a>
				<div id="mw-content-text" dir="ltr" class="mw-content-ltr" lang="en"><div class="mw-parser-output"><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br><a href="https://en.wikipedia.org/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="https://en.wikipedia.org/wiki/File:Kernel_Machine.svg" class="image"><img alt="Kernel Machine.svg" src="220px-Kernel_Machine.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="233" width="220" height="100"></a></td></tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame1"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems<a class="NavToggle" id="NavToggle1" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="https://en.wikipedia.org/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>
<li><a href="https://en.wikipedia.org/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a class="mw-selflink selflink">Feature learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="https://en.wikipedia.org/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame2"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="padding:0.1em 0;line-height:1.2em;"><a href="https://en.wikipedia.org/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br><span style="font-weight:normal;"><span style="font-size:85%;">(<b><a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&nbsp;• <b><a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div><a class="NavToggle" id="NavToggle2" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a> (<a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a>, <a href="https://en.wikipedia.org/wiki/Boosting_%28machine_learning%29" title="Boosting (machine learning)">Boosting</a>, <a href="https://en.wikipedia.org/wiki/Random_forest" title="Random forest">Random forest</a>)</li>
<li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Artificial neural network">Neural networks</a></li>
<li><a href="https://en.wikipedia.org/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="https://en.wikipedia.org/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame3"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a><a class="NavToggle" id="NavToggle3" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="https://en.wikipedia.org/wiki/CURE_data_clustering_algorithm" class="mw-redirect" title="CURE data clustering algorithm">CURE</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="https://en.wikipedia.org/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br><a href="https://en.wikipedia.org/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="https://en.wikipedia.org/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame4"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a><a class="NavToggle" id="NavToggle4" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Canonical_correlation_analysis" class="mw-redirect" title="Canonical correlation analysis">CCA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame5"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a><a class="NavToggle" id="NavToggle5" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Graphical_model" title="Graphical model">Graphical models</a> (<a href="https://en.wikipedia.org/wiki/Bayesian_network" title="Bayesian network">Bayes net</a>, <a href="https://en.wikipedia.org/wiki/Conditional_random_field" title="Conditional random field">CRF</a>, <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" title="Hidden Markov model">HMM</a>)</li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame6"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a><a class="NavToggle" id="NavToggle6" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_classification" class="mw-redirect" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame7"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Artificial neural network">Neural nets</a><a class="NavToggle" id="NavToggle7" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="https://en.wikipedia.org/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a> (<a href="https://en.wikipedia.org/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a>, <a href="https://en.wikipedia.org/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a>)</li>
<li><a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="https://en.wikipedia.org/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a> (<a href="https://en.wikipedia.org/wiki/U-Net" title="U-Net">U-Net</a>)</li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame8"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a><a class="NavToggle" id="NavToggle8" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame9"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory<a class="NavToggle" id="NavToggle9" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Bias-variance_dilemma" class="mw-redirect" title="Bias-variance dilemma">Bias-variance dilemma</a></li>
<li><a href="https://en.wikipedia.org/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame10"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues<a class="NavToggle" id="NavToggle10" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a></li>
<li><a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="https://en.wikipedia.org/wiki/Machine_Learning_%28journal%29" title="Machine Learning (journal)">ML</a></li>
<li><a href="https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a rel="nofollow" class="external text" href="https://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame11"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a><a class="NavToggle" id="NavToggle11" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>
</div></div></div></td>
</tr><tr><td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0" id="NavFrame12"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles<a class="NavToggle" id="NavToggle12" href="#">[show]</a></div><div class="NavContent" style="font-size: 105%; padding: 0.2em 0px 0.4em; text-align: center; display: none;"><div class="hlist">
<ul><li><a href="https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research" class="mw-redirect" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="https://en.wikipedia.org/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>
</div></div></div></td>
</tr><tr><td class="plainlist" style="padding:0.3em 0.4em 0.3em;font-weight:bold;border-top: 1px solid #aaa; border-bottom: 1px solid #aaa;border-top:1px solid #aaa;border-bottom:1px solid #aaa;">
<ul><li><a href="https://en.wikipedia.org/wiki/File:Portal-puzzle.svg" class="image"><img alt="Portal-puzzle.svg" src="16px-Portal-puzzle.png" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" data-file-width="32" data-file-height="28" width="16" height="14"></a> <a href="https://en.wikipedia.org/wiki/Portal:Machine_learning" title="Portal:Machine learning">Machine learning portal</a></li></ul></td></tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="https://en.wikipedia.org/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="https://en.wikipedia.org/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>
<p>In <a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine learning">machine learning</a>, <b>feature learning</b> or <b>representation learning</b><sup id="cite_ref-pami_1-0" class="reference"><a href="#cite_note-pami-1">[1]</a></sup>
 is a set of techniques that allows a system to automatically discover 
the representations needed for feature detection or classification from 
raw data. This replaces manual <a href="https://en.wikipedia.org/wiki/Feature_engineering" title="Feature engineering">feature engineering</a> and allows a machine to both learn the features  and use them to perform  a specific task.
</p><p>Feature learning is motivated by the fact that machine learning tasks such as <a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">classification</a>
 often require input that is mathematically and computationally 
convenient to process. However, real-world data such as images, video, 
and sensor data has not yielded to attempts to algorithmically define 
specific features. An alternative is to discover such features or 
representations through examination, without relying on explicit 
algorithms.  
</p><p>Feature learning can be either supervised or unsupervised.
</p>
<ul><li>In <a href="https://en.wikipedia.org/wiki/Supervised_learning" title="Supervised learning">supervised feature learning</a>, features are learned using labeled input data. Examples include <a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Artificial neural network">supervised neural networks</a>, <a href="https://en.wikipedia.org/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptron</a> and (supervised) <a href="https://en.wikipedia.org/wiki/Dictionary_learning" class="mw-redirect" title="Dictionary learning">dictionary learning</a>.</li>
<li>In <a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised feature learning</a>, features are learned with unlabeled input data.  Examples include dictionary learning, <a href="https://en.wikipedia.org/wiki/Independent_component_analysis" title="Independent component analysis">independent component analysis</a>, <a href="https://en.wikipedia.org/wiki/Autoencoder" title="Autoencoder">autoencoders</a>, <a href="https://en.wikipedia.org/wiki/Matrix_decomposition" title="Matrix decomposition">matrix factorization</a><sup id="cite_ref-2" class="reference"><a href="#cite_note-2">[2]</a></sup> and various forms of <a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">clustering</a>.<sup id="cite_ref-coates2011_3-0" class="reference"><a href="#cite_note-coates2011-3">[3]</a></sup><sup id="cite_ref-4" class="reference"><a href="#cite_note-4">[4]</a></sup><sup id="cite_ref-jurafsky_5-0" class="reference"><a href="#cite_note-jurafsky-5">[5]</a></sup></li></ul>
<div id="toc" class="toc"><input role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" type="checkbox"><div class="toctitle" dir="ltr" lang="en"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Supervised"><span class="tocnumber">1</span> <span class="toctext">Supervised</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Supervised_dictionary_learning"><span class="tocnumber">1.1</span> <span class="toctext">Supervised dictionary learning</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Neural_networks"><span class="tocnumber">1.2</span> <span class="toctext">Neural networks</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-4"><a href="#Unsupervised"><span class="tocnumber">2</span> <span class="toctext">Unsupervised</span></a>
<ul>
<li class="toclevel-2 tocsection-5"><a href="#K-means_clustering"><span class="tocnumber">2.1</span> <span class="toctext"><i>K</i>-means clustering</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#Principal_component_analysis"><span class="tocnumber">2.2</span> <span class="toctext">Principal component analysis</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Local_linear_embedding"><span class="tocnumber">2.3</span> <span class="toctext">Local linear embedding</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Independent_component_analysis"><span class="tocnumber">2.4</span> <span class="toctext">Independent component analysis</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Unsupervised_dictionary_learning"><span class="tocnumber">2.5</span> <span class="toctext">Unsupervised dictionary learning</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#Multilayer/deep_architectures"><span class="tocnumber">3</span> <span class="toctext">Multilayer/deep architectures</span></a>
<ul>
<li class="toclevel-2 tocsection-11"><a href="#Restricted_Boltzmann_machine"><span class="tocnumber">3.1</span> <span class="toctext">Restricted Boltzmann machine</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Autoencoder"><span class="tocnumber">3.2</span> <span class="toctext">Autoencoder</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-13"><a href="#See_also"><span class="tocnumber">4</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-14"><a href="#References"><span class="tocnumber">5</span> <span class="toctext">References</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Supervised">Supervised</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=1" title="Edit section: Supervised">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Supervised feature learning is learning features from labeled data. 
The data label allows the system to compute an error term, the degree to
 which the system fails to produce the label, which can then be used as 
feedback to correct the learning process (reduce/minimize the error). 
Approaches include:
</p>
<h3><span class="mw-headline" id="Supervised_dictionary_learning">Supervised dictionary learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=2" title="Edit section: Supervised dictionary learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Dictionary learning develops a set (dictionary) of representative 
elements from the input data such that each data point can be 
represented as a weighted sum of the representative elements. The 
dictionary elements and the weights may be found by minimizing the 
average representation error (over the input data), together with <a href="https://en.wikipedia.org/wiki/Regularization_%28mathematics%29" title="Regularization (mathematics)"><i>L1</i> regularization</a> on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights).
</p><p>Supervised dictionary learning exploits both the structure 
underlying the input data and the labels for optimizing the dictionary 
elements. For example, a supervised dictionary learning technique<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">[6]</a></sup>
 applied dictionary learning on classification problems by jointly 
optimizing the dictionary elements, weights for representing data 
points, and parameters of the classifier based on the input data. In 
particular, a minimization problem is formulated, where the objective 
function consists of the classification error, the representation error,
 an <i>L1</i> regularization on the representing weights for each data point (to enable sparse representation of data), and an <i>L2</i> regularization on the parameters of the classifier.
</p>
<h3><span class="mw-headline" id="Neural_networks">Neural networks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=3" title="Edit section: Neural networks">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/Artificial_neural_networks" class="mw-redirect" title="Artificial neural networks">Neural networks</a>
 are a family of learning algorithms that use a "network" consisting of 
multiple layers of inter-connected nodes. It is inspired by the animal 
nervous system, where the nodes are viewed as neurons and edges are 
viewed as synapses. Each edge has an associated weight, and the network 
defines computational rules for passing input data from the network's 
input layer to the output layer. A network function associated with a 
neural network characterizes the relationship between input and output 
layers, which is parameterized by the weights. With appropriately 
defined network functions, various learning tasks can be performed by 
minimizing a cost function over the network function (weights).
</p><p>Multilayer <a href="https://en.wikipedia.org/wiki/Neural_network" title="Neural network">neural networks</a>
 can be used to perform feature learning, since they learn a 
representation of their input at the hidden layer(s) which is 
subsequently used for classification or regression at the output layer. 
The most popular network architecture of this type is <a href="https://en.wikipedia.org/w/index.php?title=Siamese_networks&amp;action=edit&amp;redlink=1" class="new" title="Siamese networks (page does not exist)">Siamese networks</a>.
</p>
<h2><span class="mw-headline" id="Unsupervised">Unsupervised</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=4" title="Edit section: Unsupervised">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Unsupervised feature learning is learning features from unlabeled 
data. The goal of unsupervised feature learning is often to discover 
low-dimensional features that captures some structure underlying the 
high-dimensional input data. When the feature learning is performed in 
an unsupervised way, it enables a form of <a href="https://en.wikipedia.org/wiki/Semisupervised_learning" class="mw-redirect" title="Semisupervised learning">semisupervised learning</a>
 where features learned from an unlabeled dataset are then employed to 
improve performance in a supervised setting with labeled data.<sup id="cite_ref-liang_7-0" class="reference"><a href="#cite_note-liang-7">[7]</a></sup><sup id="cite_ref-turian_8-0" class="reference"><a href="#cite_note-turian-8">[8]</a></sup> Several approaches are introduced in the following.
</p>
<h3><span class="mw-headline" id="K-means_clustering"><i>K</i>-means clustering</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=5" title="Edit section: K-means clustering">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/K-means_clustering" title="K-means clustering"><i>K</i>-means clustering</a> is an approach for vector quantization. In particular, given a set of <i>n</i> vectors, <i>k</i>-means
 clustering groups them into k clusters (i.e., subsets) in such a way 
that each vector belongs to the cluster with the closest mean. The 
problem is computationally <a href="https://en.wikipedia.org/wiki/NP-hard" class="mw-redirect" title="NP-hard">NP-hard</a>, although suboptimal <a href="https://en.wikipedia.org/wiki/Greedy_algorithm" title="Greedy algorithm">greedy algorithms</a> have been developed.
</p><p>K-means clustering can be used to group an unlabeled set of inputs into <i>k</i> clusters, and then use the <a href="https://en.wikipedia.org/wiki/Centroid" title="Centroid">centroids</a> of these clusters to produce features. These features can be produced in several ways. The simplest is to add <i>k</i> binary features to each sample, where each feature <i>j</i> has value one <a href="https://en.wikipedia.org/wiki/If_and_only_if" title="If and only if">iff</a> the <i>j</i>th centroid learned by <i>k</i>-means is the closest to the sample under consideration.<sup id="cite_ref-coates2011_3-1" class="reference"><a href="#cite_note-coates2011-3">[3]</a></sup> It is also possible to use the distances to the clusters as features, perhaps after transforming them through a <a href="https://en.wikipedia.org/wiki/Radial_basis_function" title="Radial basis function">radial basis function</a> (a technique that has been used to train <a href="https://en.wikipedia.org/wiki/Radial_basis_function_network" title="Radial basis function network">RBF networks</a><sup id="cite_ref-schwenker_9-0" class="reference"><a href="#cite_note-schwenker-9">[9]</a></sup>). Coates and <a href="https://en.wikipedia.org/wiki/Andrew_Ng" title="Andrew Ng">Ng</a> note that certain variants of <i>k</i>-means behave similarly to <a href="https://en.wikipedia.org/wiki/Sparse_coding" class="mw-redirect" title="Sparse coding">sparse coding</a> algorithms.<sup id="cite_ref-Coates2012_10-0" class="reference"><a href="#cite_note-Coates2012-10">[10]</a></sup>
</p><p>In a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that <i>k</i>-means
 clustering with an appropriate transformation outperforms the more 
recently invented auto-encoders and RBMs on an image classification 
task.<sup id="cite_ref-coates2011_3-2" class="reference"><a href="#cite_note-coates2011-3">[3]</a></sup> <i>K</i>-means also improves performance in the domain of <a href="https://en.wikipedia.org/wiki/Natural_language_processing" title="Natural language processing">NLP</a>, specifically for <a href="https://en.wikipedia.org/wiki/Named-entity_recognition" title="Named-entity recognition">named-entity recognition</a>;<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">[11]</a></sup> there, it competes with <a href="https://en.wikipedia.org/wiki/Brown_clustering" title="Brown clustering">Brown clustering</a>, as well as with distributed word representations (also known as neural word embeddings).<sup id="cite_ref-turian_8-1" class="reference"><a href="#cite_note-turian-8">[8]</a></sup>
</p>
<h3><span class="mw-headline" id="Principal_component_analysis">Principal component analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=6" title="Edit section: Principal component analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a> (PCA) is often used for dimension reduction. Given an unlabeled set of <i>n</i> input data vectors, PCA generates <i>p</i> (which is much smaller than the dimension of the input data) <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition" class="mw-redirect" title="Singular value decomposition">right singular vectors</a> corresponding to the <i>p</i> largest singular values of the data matrix, where the <i>k</i>th row of the data matrix is the <i>k</i>th input data vector shifted by the <a href="https://en.wikipedia.org/wiki/Sample_mean_and_sample_covariance" class="mw-redirect" title="Sample mean and sample covariance">sample mean</a> of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the <a href="https://en.wikipedia.org/wiki/Eigenvector" class="mw-redirect" title="Eigenvector">eigenvectors</a> corresponding to the <i>p</i> largest eigenvalues of the <a href="https://en.wikipedia.org/wiki/Sample_mean_and_sample_covariance" class="mw-redirect" title="Sample mean and sample covariance">sample covariance matrix</a> of the input vectors. These <i>p</i>
 singular vectors are the feature vectors learned from the input data, 
and they represent directions along which the data has the largest 
variations.
</p><p>PCA is a linear feature learning approach since the <i>p</i> singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with <i>p</i> iterations. In the <i>i</i>th iteration, the projection of the data matrix on the <i>(i-1)</i>th eigenvector is subtracted, and the <i>i</i>th singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix.
</p><p>PCA has several limitations. First, it assumes that the 
directions with large variance are of most interest, which may not be 
the case. PCA only relies on orthogonal transformations of the original 
data, and it exploits only the first- and second-order <a href="https://en.wikipedia.org/wiki/Moment_%28mathematics%29" title="Moment (mathematics)">moments</a>
 of the data, which may not well characterize the data distribution. 
Furthermore, PCA can effectively reduce dimension only when the input 
data vectors are correlated (which results in a few dominant 
eigenvalues).
</p>
<h3><span class="mw-headline" id="Local_linear_embedding">Local linear embedding</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=7" title="Edit section: Local linear embedding">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction" title="Nonlinear dimensionality reduction">Local linear embedding</a>
 (LLE) is a nonlinear learning approach for generating low-dimensional 
neighbor-preserving representations from (unlabeled) high-dimension 
input. The approach was proposed by Roweis and Saul (2000).<sup id="cite_ref-RowSau00_12-0" class="reference"><a href="#cite_note-RowSau00-12">[12]</a></sup><sup id="cite_ref-SauRow00_13-0" class="reference"><a href="#cite_note-SauRow00-13">[13]</a></sup>
 The general idea of LLE is to reconstruct the original high-dimensional
 data using lower-dimensional points while maintaining some geometric 
properties of the neighborhoods in the original data set. 
</p><p>LLE consists of two major steps. The first step is for "neighbor-preserving", where each input data point <i>Xi</i> is reconstructed as a weighted sum of <a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>K</i> nearest neighbor</a>
 data points, and the optimal weights are found by minimizing the 
average squared reconstruction error (i.e., difference between an input 
point and its reconstruction) under the constraint that the weights 
associated with each point sum up to one. The second step is for 
"dimension reduction," by looking for vectors in a lower-dimensional 
space that minimizes the representation error using the optimized 
weights in the first step. Note that in the first step, the weights are 
optimized with fixed data, which can be solved as a <a href="https://en.wikipedia.org/wiki/Least_squares" title="Least squares">least squares</a>
 problem. In the second step, lower-dimensional points are optimized 
with fixed weights, which can be solved via sparse eigenvalue 
decomposition.
</p><p>The reconstruction weights obtained in the first step capture the
 "intrinsic geometric properties" of a neighborhood in the input data.<sup id="cite_ref-SauRow00_13-1" class="reference"><a href="#cite_note-SauRow00-13">[13]</a></sup> It is assumed that original data lie on a smooth lower-dimensional <a href="https://en.wikipedia.org/wiki/Manifold" title="Manifold">manifold</a>,
 and the "intrinsic geometric properties" captured by the weights of the
 original data are also expected to be on the manifold. This is why the 
same weights are used in the second step of LLE. Compared with PCA, LLE 
is more powerful in exploiting the underlying data structure.
</p>
<h3><span class="mw-headline" id="Independent_component_analysis">Independent component analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=8" title="Edit section: Independent component analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/Independent_component_analysis" title="Independent component analysis">Independent component analysis</a> (ICA) is a technique for forming a data representation using a weighted sum of independent non-Gaussian components.<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">[14]</a></sup> The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow <a href="https://en.wikipedia.org/wiki/Normal_distribution" title="Normal distribution">Gaussian</a> distribution.
</p>
<h3><span class="mw-headline" id="Unsupervised_dictionary_learning">Unsupervised dictionary learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=9" title="Edit section: Unsupervised dictionary learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Unsupervised dictionary learning does not utilize data labels and 
exploits the structure underlying the data for optimizing dictionary 
elements. An example of unsupervised dictionary learning is sparse 
coding, which aims to learn basis functions (dictionary elements) for 
data representation from unlabeled input data. Sparse coding can be 
applied to learn overcomplete dictionaries, where the number of 
dictionary elements is larger than the dimension of the input data.<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">[15]</a></sup> Aharon et al. proposed algorithm <a href="https://en.wikipedia.org/wiki/K-SVD" title="K-SVD">K-SVD</a> for learning a dictionary of elements that enables sparse representation.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">[16]</a></sup>
</p>
<h2><span id="Multilayer.2Fdeep_architectures"></span><span class="mw-headline" id="Multilayer/deep_architectures">Multilayer/deep architectures</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=10" title="Edit section: Multilayer/deep architectures">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The hierarchical architecture of the biological neural system inspires <a href="https://en.wikipedia.org/wiki/Deep_learning" title="Deep learning">deep learning</a> architectures for feature learning by stacking multiple layers of learning nodes.<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">[17]</a></sup> These architectures are often designed based on the assumption of <a href="https://en.wikipedia.org/wiki/Distributed_representation" class="mw-redirect" title="Distributed representation">distributed representation</a>:
 observed data is generated by the interactions of many different 
factors on multiple levels. In a deep learning architecture, the output 
of each intermediate layer can be viewed as a representation of the 
original input data. Each level uses the representation produced by 
previous level as input, and produces new representations as output, 
which is then fed to higher levels. The input at the bottom layer is raw
 data, and the output of the final layer is the final low-dimensional 
feature or representation.
</p>
<h3><span class="mw-headline" id="Restricted_Boltzmann_machine">Restricted Boltzmann machine</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=11" title="Edit section: Restricted Boltzmann machine">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machines</a> (RBMs) are often used as a building block for multilayer learning architectures.<sup id="cite_ref-coates2011_3-3" class="reference"><a href="#cite_note-coates2011-3">[3]</a></sup><sup id="cite_ref-Hinton2006_18-0" class="reference"><a href="#cite_note-Hinton2006-18">[18]</a></sup> An RBM can be represented by an undirected bipartite graph consisting of a group of <a href="https://en.wikipedia.org/wiki/Binary_variable" class="mw-redirect" title="Binary variable">binary</a> <a href="https://en.wikipedia.org/wiki/Latent_variable" title="Latent variable">hidden variables</a>, a group of visible variables, and edges connecting the hidden and visible nodes. It is a special case of the more general <a href="https://en.wikipedia.org/wiki/Boltzmann_machine" title="Boltzmann machine">Boltzmann machines</a>
 with the constraint of no intra-node connections. Each edge in an RBM 
is associated with a weight. The weights together with the connections 
define an <a href="https://en.wikipedia.org/wiki/Energy_function" class="mw-redirect" title="Energy function">energy function</a>, based on which a <a href="https://en.wikipedia.org/wiki/Joint_distribution" class="mw-redirect" title="Joint distribution">joint distribution</a>
 of visible and hidden nodes can be devised. Based on the topology of 
the RBM, the hidden (visible) variables are independent, conditioned on 
the visible (hidden) variables.<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="visible hidden? (June 2017)">clarification needed</span></a></i>]</sup> Such conditional independence facilitates computations.
</p><p>An RBM can be viewed as a single layer architecture for 
unsupervised feature learning. In particular, the visible variables 
correspond to input data, and the hidden variables correspond to feature
 detectors. The weights can be trained by maximizing the probability of 
visible variables using <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Hinton</a>'s <a href="https://en.wikipedia.org/wiki/Contrastive_divergence" class="mw-redirect" title="Contrastive divergence">contrastive divergence</a> (CD) algorithm.<sup id="cite_ref-Hinton2006_18-1" class="reference"><a href="#cite_note-Hinton2006-18">[18]</a></sup>
</p><p>In general training RBM by solving the maximization problem tends to result in non-sparse representations. Sparse RBM<sup id="cite_ref-Lee2008_19-0" class="reference"><a href="#cite_note-Lee2008-19">[19]</a></sup> was proposed to enable sparse representations. The idea is to add a <a href="https://en.wikipedia.org/wiki/Regularization_%28mathematics%29" title="Regularization (mathematics)">regularization</a>
 term in the objective function of data likelihood, which penalizes the 
deviation of the expected hidden variables from a small constant <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle p}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>p</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p}</annotation>
  </semantics>
</math></span><img src="81eac1e205430d1f40810df36a0edffdc367af36.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;" alt="p"></span>.
</p>
<h3><span class="mw-headline" id="Autoencoder">Autoencoder</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=12" title="Edit section: Autoencoder">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An <a href="https://en.wikipedia.org/wiki/Autoencoder" title="Autoencoder">autoencoder</a>
 consisting of an encoder and a decoder is a paradigm for deep learning 
architectures. An example is provided by Hinton and Salakhutdinov<sup id="cite_ref-Hinton2006_18-2" class="reference"><a href="#cite_note-Hinton2006-18">[18]</a></sup>
 where the encoder uses raw data (e.g., image) as input and produces 
feature or representation as output and the decoder uses the extracted 
feature from the encoder as input and reconstructs the original input 
raw data as output. The encoder and decoder are constructed by stacking 
multiple layers of RBMs. The parameters involved in the architecture 
were originally trained in a <a href="https://en.wikipedia.org/wiki/Greedy_algorithm" title="Greedy algorithm">greedy</a>
 layer-by-layer manner: after one layer of feature detectors is learned,
 they are fed up as visible variables for training the corresponding 
RBM. Current approaches typically apply end-to-end training with <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" title="Stochastic gradient descent">stochastic gradient descent</a> methods. Training can be repeated until some stopping criteria are satisfied.
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=13" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Automated_machine_learning" title="Automated machine learning">Automated machine learning</a> (AutoML)</li>
<li><a href="https://en.wikipedia.org/wiki/Basis_function" title="Basis function">Basis function</a></li>
<li><a href="https://en.wikipedia.org/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_detection_%28computer_vision%29" title="Feature detection (computer vision)">Feature detection (computer vision)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_extraction" title="Feature extraction">Feature extraction</a></li>
<li><a href="https://en.wikipedia.org/wiki/Kernel_trick" class="mw-redirect" title="Kernel trick">Kernel trick</a></li>
<li><a href="https://en.wikipedia.org/wiki/Vector_quantization" title="Vector quantization">Vector quantization</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit&amp;section=14" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-pami-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-pami_1-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Y. Bengio; A. Courville; P. Vincent (2013). "Representation Learning: A Review and New Perspectives". <i>IEEE Trans. PAMI, special issue Learning Deep Architectures</i>. <b>35</b>: 1798–1828. <a href="https://en.wikipedia.org/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="https://arxiv.org/abs/1206.5538">1206.5538</a> <img alt="Freely accessible" src="9px-Lock-green.png" title="Freely accessible" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" width="9" height="14"></span>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109/tpami.2013.50">10.1109/tpami.2013.50</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Trans.+PAMI%2C+special+issue+Learning+Deep+Architectures&amp;rft.atitle=Representation+Learning%3A+A+Review+and+New+Perspectives&amp;rft.volume=35&amp;rft.pages=1798-1828&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1206.5538&amp;rft_id=info%3Adoi%2F10.1109%2Ftpami.2013.50&amp;rft.au=Y.+Bengio&amp;rft.au=A.+Courville&amp;rft.au=P.+Vincent&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation conference">Nathan Srebro; Jason D. M. Rennie; Tommi S. Jaakkola (2004). <i>Maximum-Margin Matrix Factorization</i>. <a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Maximum-Margin+Matrix+Factorization&amp;rft.date=2004&amp;rft.au=Nathan+Srebro&amp;rft.au=Jason+D.+M.+Rennie&amp;rft.au=Tommi+S.+Jaakkola&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-coates2011-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-coates2011_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-coates2011_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-coates2011_3-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-coates2011_3-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Coates, Adam; Lee, Honglak; Ng, Andrew Y. (2011). <a rel="nofollow" class="external text" href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf"><i>An analysis of single-layer networks in unsupervised feature learning</i></a> <span style="font-size:85%;">(PDF)</span>. Int'l Conf. on AI and Statistics (AISTATS).</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=An+analysis+of+single-layer+networks+in+unsupervised+feature+learning&amp;rft.date=2011&amp;rft.aulast=Coates&amp;rft.aufirst=Adam&amp;rft.au=Lee%2C+Honglak&amp;rft.au=Ng%2C+Andrew+Y.&amp;rft_id=http%3A%2F%2Fmachinelearning.wustl.edu%2Fmlpapers%2Fpaper_files%2FAISTATS2011_CoatesNL11.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation conference">Csurka, Gabriella; Dance, Christopher C.; Fan, Lixin; Willamowski, Jutta; Bray, Cédric (2004). <a rel="nofollow" class="external text" href="http://www.cs.cmu.edu/%7Eefros/courses/LBMV07/Papers/csurka-eccv-04.pdf"><i>Visual categorization with bags of keypoints</i></a> <span style="font-size:85%;">(PDF)</span>. ECCV Workshop on Statistical Learning in Computer Vision.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Visual+categorization+with+bags+of+keypoints&amp;rft.date=2004&amp;rft.aulast=Csurka&amp;rft.aufirst=Gabriella&amp;rft.au=Dance%2C+Christopher+C.&amp;rft.au=Fan%2C+Lixin&amp;rft.au=Willamowski%2C+Jutta&amp;rft.au=Bray%2C+C%C3%A9dric&amp;rft_id=http%3A%2F%2Fwww.cs.cmu.edu%2F~efros%2Fcourses%2FLBMV07%2FPapers%2Fcsurka-eccv-04.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-jurafsky-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-jurafsky_5-0">^</a></b></span> <span class="reference-text"><cite class="citation book">Daniel Jurafsky; James H. Martin (2009). <i>Speech and Language Processing</i>. Pearson Education International. pp.&nbsp;145–146.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Speech+and+Language+Processing&amp;rft.pages=145-146&amp;rft.pub=Pearson+Education+International&amp;rft.date=2009&amp;rft.au=Daniel+Jurafsky&amp;rft.au=James+H.+Martin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation journal">Mairal, Julien; Bach, Francis; Ponce, Jean; Sapiro, Guillermo; Zisserman, Andrew (2009). "Supervised Dictionary Learning". <i>Advances in Neural Information Processing Systems</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.atitle=Supervised+Dictionary+Learning&amp;rft.date=2009&amp;rft.aulast=Mairal&amp;rft.aufirst=Julien&amp;rft.au=Bach%2C+Francis&amp;rft.au=Ponce%2C+Jean&amp;rft.au=Sapiro%2C+Guillermo&amp;rft.au=Zisserman%2C+Andrew&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-liang-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-liang_7-0">^</a></b></span> <span class="reference-text"><cite class="citation thesis">Percy Liang (2005). <a rel="nofollow" class="external text" href="http://people.csail.mit.edu/pliang/papers/meng-thesis.pdf"><i>Semi-Supervised Learning for Natural Language</i></a> <span style="font-size:85%;">(PDF)</span> (M. Eng.). <a href="https://en.wikipedia.org/wiki/Massachusetts_Institute_of_Technology" title="Massachusetts Institute of Technology">MIT</a>. pp.&nbsp;44–52.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=Semi-Supervised+Learning+for+Natural+Language&amp;rft.inst=MIT&amp;rft.date=2005&amp;rft.au=Percy+Liang&amp;rft_id=http%3A%2F%2Fpeople.csail.mit.edu%2Fpliang%2Fpapers%2Fmeng-thesis.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-turian-8"><span class="mw-cite-backlink">^ <a href="#cite_ref-turian_8-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-turian_8-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Joseph Turian; Lev Ratinov; Yoshua Bengio (2010). <a rel="nofollow" class="external text" href="http://www.newdesign.aclweb.org/anthology/P/P10/P10-1040.pdf"><i>Word representations: a simple and general method for semi-supervised learning</i></a> <span style="font-size:85%;">(PDF)</span>. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Word+representations%3A+a+simple+and+general+method+for+semi-supervised+learning&amp;rft.date=2010&amp;rft.au=Joseph+Turian&amp;rft.au=Lev+Ratinov&amp;rft.au=Yoshua+Bengio&amp;rft_id=http%3A%2F%2Fwww.newdesign.aclweb.org%2Fanthology%2FP%2FP10%2FP10-1040.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-schwenker-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-schwenker_9-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Schwenker, Friedhelm; Kestler, Hans A.; Palm, Günther (2001). "Three learning phases for radial-basis-function networks". <i>Neural Networks</i>. <b>14</b>: 439–458. <a href="https://en.wikipedia.org/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&nbsp;<span class="plainlinks"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.109.312">10.1.1.109.312</a> <img alt="Freely accessible" src="9px-Lock-green.png" title="Freely accessible" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" width="9" height="14"></span>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016/s0893-6080%2801%2900027-2">10.1016/s0893-6080(01)00027-2</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Networks&amp;rft.atitle=Three+learning+phases+for+radial-basis-function+networks&amp;rft.volume=14&amp;rft.pages=439-458&amp;rft.date=2001&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.109.312&amp;rft_id=info%3Adoi%2F10.1016%2Fs0893-6080%2801%2900027-2&amp;rft.aulast=Schwenker&amp;rft.aufirst=Friedhelm&amp;rft.au=Kestler%2C+Hans+A.&amp;rft.au=Palm%2C+G%C3%BCnther&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-Coates2012-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-Coates2012_10-0">^</a></b></span> <span class="reference-text"><cite class="citation encyclopaedia">Coates,
 Adam; Ng, Andrew Y. (2012). "Learning feature representations with 
k-means".  In G. Montavon, G. B. Orr and K.-R. Müller. <i>Neural Networks: Tricks of the Trade</i>. Springer.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Learning+feature+representations+with+k-means&amp;rft.btitle=Neural+Networks%3A+Tricks+of+the+Trade&amp;rft.pub=Springer&amp;rft.date=2012&amp;rft.aulast=Coates&amp;rft.aufirst=Adam&amp;rft.au=Ng%2C+Andrew+Y.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation conference">Dekang Lin; Xiaoyun Wu (2009). <a rel="nofollow" class="external text" href="http://wmmks.csie.ncku.edu.tw/ACL-IJCNLP-2009/ACLIJCNLP/pdf/ACLIJCNLP116.pdf"><i>Phrase clustering for discriminative learning</i></a> <span style="font-size:85%;">(PDF)</span>. Proc. J. Conf. of the ACL and 4th Int'l J. Conf. on Natural Language Processing of the AFNLP. pp.&nbsp;1030–1038.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Phrase+clustering+for+discriminative+learning&amp;rft.pages=1030-1038&amp;rft.date=2009&amp;rft.au=Dekang+Lin&amp;rft.au=Xiaoyun+Wu&amp;rft_id=http%3A%2F%2Fwmmks.csie.ncku.edu.tw%2FACL-IJCNLP-2009%2FACLIJCNLP%2Fpdf%2FACLIJCNLP116.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-RowSau00-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-RowSau00_12-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Roweis, Sam T; Saul, Lawrence K (2000). "Nonlinear Dimensionality Reduction by Locally Linear Embedding". <i>Science, New Series</i>. <b>290</b> (5500): 2323–2326. <a href="https://en.wikipedia.org/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2000Sci...290.2323R">2000Sci...290.2323R</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1126/science.290.5500.2323">10.1126/science.290.5500.2323</a>. <a href="https://en.wikipedia.org/wiki/JSTOR" title="JSTOR">JSTOR</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.jstor.org/stable/3081722">3081722</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pubmed/11125150">11125150</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science%2C+New+Series&amp;rft.atitle=Nonlinear+Dimensionality+Reduction+by+Locally+Linear+Embedding&amp;rft.volume=290&amp;rft.issue=5500&amp;rft.pages=2323-2326&amp;rft.date=2000&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.290.5500.2323&amp;rft_id=info%3Apmid%2F11125150&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F3081722&amp;rft_id=info%3Abibcode%2F2000Sci...290.2323R&amp;rft.aulast=Roweis&amp;rft.aufirst=Sam+T&amp;rft.au=Saul%2C+Lawrence+K&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-SauRow00-13"><span class="mw-cite-backlink">^ <a href="#cite_ref-SauRow00_13-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-SauRow00_13-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Saul, Lawrence K; Roweis, Sam T (2000). <a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/%7Eroweis/lle/publications.html">"An Introduction to Locally Linear Embedding"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=An+Introduction+to+Locally+Linear+Embedding&amp;rft.date=2000&amp;rft.aulast=Saul&amp;rft.aufirst=Lawrence+K&amp;rft.au=Roweis%2C+Sam+T&amp;rft_id=http%3A%2F%2Fwww.cs.toronto.edu%2F~roweis%2Flle%2Fpublications.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hyvärinen, Aapo; Oja, Erkki (2000). "Independent Component Analysis: Algorithms and Applications". <i>Neural Networks</i>. <b>13</b> (4): 411–430. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016/s0893-6080%2800%2900026-5">10.1016/s0893-6080(00)00026-5</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pubmed/10946390">10946390</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Networks&amp;rft.atitle=Independent+Component+Analysis%3A+Algorithms+and+Applications&amp;rft.volume=13&amp;rft.issue=4&amp;rft.pages=411-430&amp;rft.date=2000&amp;rft_id=info%3Adoi%2F10.1016%2Fs0893-6080%2800%2900026-5&amp;rft_id=info%3Apmid%2F10946390&amp;rft.aulast=Hyv%C3%A4rinen&amp;rft.aufirst=Aapo&amp;rft.au=Oja%2C+Erkki&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation journal">Lee, Honglak; Battle, Alexis; Raina, Rajat; Ng, Andrew Y (2007). "Efficient sparse coding algorithms". <i>Advances in Neural Information Processing Systems</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.atitle=Efficient+sparse+coding+algorithms&amp;rft.date=2007&amp;rft.aulast=Lee&amp;rft.aufirst=Honglak&amp;rft.au=Battle%2C+Alexis&amp;rft.au=Raina%2C+Rajat&amp;rft.au=Ng%2C+Andrew+Y&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation journal">Aharon,
 Michal; Elad, Michael; Bruckstein, Alfred (2006). "K-SVD: An Algorithm 
for Designing Overcomplete Dictionaries for Sparse Representation". <i>IEEE Trans. Signal Process</i>. <b>54</b> (11): 4311–4322. <a href="https://en.wikipedia.org/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2006ITSP...54.4311A">2006ITSP...54.4311A</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1109/TSP.2006.881199">10.1109/TSP.2006.881199</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Trans.+Signal+Process.&amp;rft.atitle=K-SVD%3A+An+Algorithm+for+Designing+Overcomplete+Dictionaries+for+Sparse+Representation&amp;rft.volume=54&amp;rft.issue=11&amp;rft.pages=4311-4322&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1109%2FTSP.2006.881199&amp;rft_id=info%3Abibcode%2F2006ITSP...54.4311A&amp;rft.aulast=Aharon&amp;rft.aufirst=Michal&amp;rft.au=Elad%2C+Michael&amp;rft.au=Bruckstein%2C+Alfred&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bengio, Yoshua (2009). "Learning Deep Architectures for AI". <i>Foundations and Trends in Machine Learning</i>. <b>2</b> (1): 1–127. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1561/2200000006">10.1561/2200000006</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Foundations+and+Trends+in+Machine+Learning&amp;rft.atitle=Learning+Deep+Architectures+for+AI&amp;rft.volume=2&amp;rft.issue=1&amp;rft.pages=1-127&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.1561%2F2200000006&amp;rft.aulast=Bengio&amp;rft.aufirst=Yoshua&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-Hinton2006-18"><span class="mw-cite-backlink">^ <a href="#cite_ref-Hinton2006_18-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Hinton2006_18-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Hinton2006_18-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Hinton, G. E.; Salakhutdinov, R. R. (2006). <a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/%7Ehinton/science.pdf">"Reducing the Dimensionality of Data with Neural Networks"</a> <span style="font-size:85%;">(PDF)</span>. <i>Science</i>. <b>313</b> (5786): 504–507. <a href="https://en.wikipedia.org/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2006Sci...313..504H">2006Sci...313..504H</a>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1126/science.1127647">10.1126/science.1127647</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&nbsp;<a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pubmed/16873662">16873662</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Reducing+the+Dimensionality+of+Data+with+Neural+Networks&amp;rft.volume=313&amp;rft.issue=5786&amp;rft.pages=504-507&amp;rft.date=2006&amp;rft_id=info%3Apmid%2F16873662&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.1127647&amp;rft_id=info%3Abibcode%2F2006Sci...313..504H&amp;rft.aulast=Hinton&amp;rft.aufirst=G.+E.&amp;rft.au=Salakhutdinov%2C+R.+R.&amp;rft_id=http%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fscience.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-Lee2008-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-Lee2008_19-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Lee, Honglak; Ekanadham, Chaitanya; Andrew, Ng (2008). "Sparse deep belief net model for visual area V2". <i>Advances in Neural Information Processing Systems</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.atitle=Sparse+deep+belief+net+model+for+visual+area+V2&amp;rft.date=2008&amp;rft.aulast=Lee&amp;rft.aufirst=Honglak&amp;rft.au=Ekanadham%2C+Chaitanya&amp;rft.au=Andrew%2C+Ng&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFeature+learning" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
</ol></div>

<!-- 
NewPP limit report
Parsed by mw1261
Cached time: 20180802033907
Cache expiry: 1900800
Dynamic content: false
CPU time usage: 0.288 seconds
Real time usage: 0.356 seconds
Preprocessor visited node count: 1311/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 59654/2097152 bytes
Template argument size: 1110/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 1/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 24015/5000000 bytes
Number of Wikibase entities loaded: 0/400
Lua time usage: 0.132/10.000 seconds
Lua memory usage: 4.65 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  272.571      1 -total
 60.01%  163.569      1 Template:Reflist
 32.13%   87.564     11 Template:Cite_journal
 20.59%   56.134      1 Template:Machine_learning_bar
 19.28%   52.548      1 Template:Sidebar_with_collapsible_lists
 13.48%   36.733      1 Template:Clarify
 11.89%   32.408      1 Template:Fix-span
  9.08%   24.738      5 Template:Cite_conference
  7.90%   21.523      2 Template:Category_handler
  4.25%   11.580      1 Template:Portal-inline
-->
</div>
<!-- Saved in parser cache with key enwiki:pcache:idhash:38870173-0!canonical!math=5 and timestamp 20180802033906 and revision id 846656918
 -->
<noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;oldid=846656918">https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;oldid=846656918</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="https://en.wikipedia.org/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Machine_learning" title="Category:Machine learning">Machine learning</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_needing_clarification_from_June_2017" title="Category:Wikipedia articles needing clarification from June 2017">Wikipedia articles needing clarification from June 2017</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="https://en.wikipedia.org/wiki/Special:MyTalk" title="Discussion about edits from this IP address [Alt+Shift+n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="https://en.wikipedia.org/wiki/Special:MyContributions" title="A list of edits made from this IP address [Alt+Shift+y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&amp;returnto=Feature+learning&amp;returntoquery=oldid%3D846656918" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="https://en.wikipedia.org/w/index.php?title=Special:UserLogin&amp;returnto=Feature+learning&amp;returntoquery=oldid%3D846656918" title="You're encouraged to log in; however, it's not mandatory. [Alt+Shift+o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><span><a href="https://en.wikipedia.org/wiki/Feature_learning" title="View the content page [Alt+Shift+c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="https://en.wikipedia.org/wiki/Talk:Feature_learning" rel="discussion" title="Discussion about the content page [Alt+Shift+t]" accesskey="t">Talk</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input class="vectorMenuCheckbox" aria-labelledby="p-variants-label" type="checkbox">
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="https://en.wikipedia.org/wiki/Feature_learning">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=edit" title="Edit this page [Alt+Shift+e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=history" title="Past revisions of this page [Alt+Shift+h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<input class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" type="checkbox">
						<h3 id="p-cactions-label"><span>More</span></h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input name="search" placeholder="Search Wikipedia" title="Search Wikipedia [Alt+Shift+f]" accesskey="f" id="searchInput" type="search"><input value="Special:Search" name="title" type="hidden"><input name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton" type="submit"><input name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton" type="submit">							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page [Alt+Shift+z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="https://en.wikipedia.org/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="https://en.wikipedia.org/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="https://en.wikipedia.org/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="https://en.wikipedia.org/wiki/Special:Random" title="Load a random article [Alt+Shift+x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="https://shop.wikimedia.org/" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="https://en.wikipedia.org/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="https://en.wikipedia.org/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="https://en.wikipedia.org/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [Alt+Shift+r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="https://en.wikipedia.org/wiki/Special:WhatLinksHere/Feature_learning" title="List of all English Wikipedia pages containing links to this page [Alt+Shift+j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Feature_learning" rel="nofollow" title="Recent changes in pages linked from this page [Alt+Shift+k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [Alt+Shift+u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="https://en.wikipedia.org/wiki/Special:SpecialPages" title="A list of all special pages [Alt+Shift+q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;oldid=846656918" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q17013334" title="Link to connected data repository item [Alt+Shift+g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=Feature_learning&amp;id=846656918" title="Information on how to cite this page">Cite this page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Feature+learning">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="https://en.wikipedia.org/w/index.php?title=Special:ElectronPdf&amp;page=Feature+learning&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="https://en.wikipedia.org/w/index.php?title=Feature_learning&amp;printable=yes" title="Printable version of this page [Alt+Shift+p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label">
			<h3 id="p-lang-label">Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/Aprendizaje_de_caracter%C3%ADsticas" title="Aprendizaje de características – Spanish" hreflang="es" class="interlanguage-link-target" lang="es">Español</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/%D0%9D%D0%B0%D0%B2%D1%87%D0%B0%D0%BD%D0%BD%D1%8F_%D0%BE%D0%B7%D0%BD%D0%B0%D0%BA" title="Навчання ознак – Ukrainian" hreflang="uk" class="interlanguage-link-target" lang="uk">Українська</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0" title="表征学习 – Chinese" hreflang="zh" class="interlanguage-link-target" lang="zh">中文</a></li>				</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q17013334#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 20 June 2018, at 03:45<span class="anonymous-show">&nbsp;(UTC)</span>.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="https://wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="https://wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="https://www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="https://en.m.wikipedia.org/w/index.php?title=Feature_learning&amp;oldid=846656918&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							</ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://wikimediafoundation.org/"><img src="wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" alt="Wikimedia Foundation" width="88" height="31"></a>					</li>
										<li id="footer-poweredbyico">
						<a href="https://www.mediawiki.org/"><img src="poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.288","walltime":"0.356","ppvisitednodes":{"value":1311,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":59654,"limit":2097152},"templateargumentsize":{"value":1110,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":1,"limit":500},"unstrip-depth":{"value":0,"limit":20},"unstrip-size":{"value":24015,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  272.571      1 -total"," 60.01%  163.569      1 Template:Reflist"," 32.13%   87.564     11 Template:Cite_journal"," 20.59%   56.134      1 Template:Machine_learning_bar"," 19.28%   52.548      1 Template:Sidebar_with_collapsible_lists"," 13.48%   36.733      1 Template:Clarify"," 11.89%   32.408      1 Template:Fix-span","  9.08%   24.738      5 Template:Cite_conference","  7.90%   21.523      2 Template:Category_handler","  4.25%   11.580      1 Template:Portal-inline"]},"scribunto":{"limitreport-timeusage":{"value":"0.132","limit":"10.000"},"limitreport-memusage":{"value":4879208,"limit":52428800}},"cachereport":{"origin":"mw1261","timestamp":"20180802033907","ttl":1900800,"transientcontent":false}}});mw.config.set({"wgBackendResponseTime":142,"wgHostname":"mw1263"});});</script>
	

</body></html>