<!DOCTYPE html>
<html class="client-js ve-not-available" dir="ltr" lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<title>Nonlinear dimensionality reduction - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Nonlinear_dimensionality_reduction","wgTitle":"Nonlinear dimensionality reduction","wgCurRevisionId":848551806,"wgRevisionId":848551806,"wgArticleId":309261,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["Dimension reduction"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Nonlinear_dimensionality_reduction","wgRelevantArticleId":309261,"wgRequestId":"W2t83ApAMFUAAJhzdiMAAABF","wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikiEditorEnabledModules":[],"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFExpandAllSectionsUserOption":true,"wgMFEnableFontChanger":true,"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q7049464","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":true,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@1dqfd7l",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});mw.loader.load(["ext.cite.a11y","ext.math.scripts","site","mediawiki.page.startup","mediawiki.user","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.3d","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});</script>
<link rel="stylesheet" href="load_002.css">
<script async="" src="load_002.php"></script>
<style>
.cite-accessibility-label{ top:-99999px;clip:rect(1px 1px 1px 1px); clip:rect(1px,1px,1px,1px); position:absolute !important;padding:0 !important;border:0 !important;height:1px !important;width:1px !important; overflow:hidden}
@media screen {
	.tochidden,.toctoggle{-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}.toctoggle{font-size:94%}}
@media print {
	.toc.tochidden,.toctoggle{display:none}}
.suggestions{overflow:hidden;position:absolute;top:0;left:0;width:0;border:0;z-index:1099;padding:0;margin:-1px 0 0 0}.suggestions-special{position:relative;background-color:#fff;cursor:pointer;border:1px solid #a2a9b1;margin:0;margin-top:-2px;display:none;padding:0.25em 0.25em;line-height:1.25em}.suggestions-results{background-color:#fff;cursor:pointer;border:1px solid #a2a9b1;padding:0;margin:0}.suggestions-result{color:#000;margin:0;line-height:1.5em;padding:0.01em 0.25em;text-align:left; overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.suggestions-result-current{background-color:#2a4b8d;color:#fff}.suggestions-special .special-label{color:#72777d;text-align:left}.suggestions-special .special-query{color:#000;font-style:italic;text-align:left}.suggestions-special .special-hover{background-color:#c8ccd1}.suggestions-result-current .special-label,.suggestions-result-current .special-query{color:#fff}.highlight{font-weight:bold}
.wp-teahouse-question-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}#wp-th-question-ask{float:right}.wp-teahouse-ask a.external{background-image:none !important}.wp-teahouse-respond-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}.wp-th-respond{float:right}.wp-teahouse-respond a.external{background-image:none !important}
.referencetooltip{position:absolute;list-style:none;list-style-image:none;opacity:0;font-size:12px;margin:0;z-index:5;padding:0}.referencetooltip > li{background:#fff;border:1px solid #bbb;-webkit-box-shadow:0 0 10px rgba(0,0,0,0.2);-moz-box-shadow:0 0 10px rgba(0,0,0,0.2);box-shadow:0 0 10px rgba(0,0,0,0.2);margin:0;padding:8px 10px;line-height:18px;max-width:300px}.referencetooltip > li + li{box-sizing:border-box;margin-left:7px;margin-top:-1px;border:0;padding:0;height:3px;width:0;background-color:transparent;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none;border-top:12px #bbb solid;border-right:7px transparent solid;border-left:7px transparent solid}.referencetooltip > li + li::after{z-index:111;content:'';border:6px solid transparent;border-bottom:0;border-top:8px solid #fff;height:0;width:0;display:block;margin-left:-6px;margin-top:-12px}.RTflipped{padding-top:13px}.referencetooltip.RTflipped > li + li{position:absolute;top:0;border-top:0;border-bottom:12px #bbb solid}.referencetooltip.RTflipped > li + li::after{border-top:0;border-bottom:8px #fff solid;position:absolute;margin-top:7px}.RTsettings{ background-image:linear-gradient(transparent,transparent),url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22utf-8%22%3F%3E%0D%0A%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%2024%2024%22%3E%0D%0A%20%20%20%20%3Cpath%20fill%3D%22%23555%22%20d%3D%22M20%2014.5v-2.9l-1.8-.3c-.1-.4-.3-.8-.6-1.4l1.1-1.5-2.1-2.1-1.5%201.1c-.5-.3-1-.5-1.4-.6L13.5%205h-2.9l-.3%201.8c-.5.1-.9.3-1.4.6L7.4%206.3%205.3%208.4l1%201.5c-.3.5-.4.9-.6%201.4l-1.7.2v2.9l1.8.3c.1.5.3.9.6%201.4l-1%201.5%202.1%202.1%201.5-1c.4.2.9.4%201.4.6l.3%201.8h3l.3-1.8c.5-.1.9-.3%201.4-.6l1.5%201.1%202.1-2.1-1.1-1.5c.3-.5.5-1%20.6-1.4l1.5-.3zM12%2016c-1.7%200-3-1.3-3-3s1.3-3%203-3%203%201.3%203%203-1.3%203-3%203z%22%2F%3E%0D%0A%3C%2Fsvg%3E);  display:block;float:right;cursor:pointer;margin:0;margin-top:-4px;height:24px;width:24px;border-radius:2px;box-sizing:border-box;background-position:center center;background-repeat:no-repeat;background-size:24px 24px;margin-left:8px}.RTsettings:hover{background-color:#eee}.RTTarget{background-color:#def}
@-webkit-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-webkit-transform:translateY(-20px)}100%{opacity:1;-webkit-transform:translateY(0)}}@-moz-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-moz-transform:translateY(-20px)}100%{opacity:1;-moz-transform:translateY(0)}}@-o-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-o-transform:translateY(-20px)}100%{opacity:1;-o-transform:translateY(0)}}@keyframes centralAuthPPersonalAnimation{0%{opacity:0;transform:translateY(-20px)}100%{opacity:1;transform:translateY(0)}}.centralAuthPPersonalAnimation{-webkit-animation-duration:1s;-moz-animation-duration:1s;-o-animation-duration:1s;animation-duration:1s;-webkit-animation-fill-mode:both;-moz-animation-fill-mode:both;-o-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:centralAuthPPersonalAnimation;-moz-animation-name:centralAuthPPersonalAnimation;-o-animation-name:centralAuthPPersonalAnimation;animation-name:centralAuthPPersonalAnimation}
.mw-ui-button{font-family:inherit;font-size:1em;display:inline-block;min-width:4em;max-width:28.75em;padding:0.546875em 1em;line-height:1.286;margin:0;border-radius:2px;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;-webkit-appearance:none;*display:inline; zoom:1;vertical-align:middle;background-color:#f8f9fa;color:#222222;border:1px solid #a2a9b1;text-align:center;font-weight:bold;cursor:pointer}.mw-ui-button:visited{color:#222222}.mw-ui-button:hover{background-color:#ffffff;color:#444444;border-color:#a2a9b1}.mw-ui-button:focus{background-color:#ffffff;color:#222222;border-color:#3366cc;box-shadow:inset 0 0 0 1px #3366cc,inset 0 0 0 2px #ffffff}.mw-ui-button:active,.mw-ui-button.is-on,.mw-ui-button.mw-ui-checked{background-color:#d9d9d9;color:#000000;border-color:#72777d;box-shadow:none}.mw-ui-button:disabled{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1}.mw-ui-button:disabled:hover,.mw-ui-button:disabled:active{background-color:#c8ccd1;color:#fff;box-shadow:none;border-color:#c8ccd1}.mw-ui-button:focus{outline-width:0}.mw-ui-button:focus::-moz-focus-inner{border-color:transparent;padding:0}.mw-ui-button:not(:disabled){-webkit-transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms;-moz-transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms;transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms}.mw-ui-button:disabled{text-shadow:none;cursor:default}.mw-ui-button.mw-ui-big{font-size:1.3em}.mw-ui-button.mw-ui-block{display:block;width:100%;margin-left:auto;margin-right:auto}.mw-ui-button.mw-ui-progressive{background-color:#3366cc;color:#fff;border:1px solid #3366cc}.mw-ui-button.mw-ui-progressive:hover{background-color:#447ff5;border-color:#447ff5}.mw-ui-button.mw-ui-progressive:focus{box-shadow:inset 0 0 0 1px #3366cc,inset 0 0 0 2px #ffffff}.mw-ui-button.mw-ui-progressive:active,.mw-ui-button.mw-ui-progressive.is-on,.mw-ui-button.mw-ui-progressive.mw-ui-checked{background-color:#2a4b8d;border-color:#2a4b8d;box-shadow:none}.mw-ui-button.mw-ui-progressive:disabled{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1}.mw-ui-button.mw-ui-progressive:disabled:hover,.mw-ui-button.mw-ui-progressive:disabled:active,.mw-ui-button.mw-ui-progressive:disabled.mw-ui-checked{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1;box-shadow:none}.mw-ui-button.mw-ui-progressive.mw-ui-quiet{color:#222222}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:hover{background-color:transparent;color:#447ff5}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:active,.mw-ui-button.mw-ui-progressive.mw-ui-quiet.mw-ui-checked{color:#2a4b8d}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:focus{background-color:transparent;color:#3366cc}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:disabled{color:#c8ccd1}.mw-ui-button.mw-ui-destructive{background-color:#dd3333;color:#fff;border:1px solid #dd3333}.mw-ui-button.mw-ui-destructive:hover{background-color:#ff4242;border-color:#ff4242}.mw-ui-button.mw-ui-destructive:focus{box-shadow:inset 0 0 0 1px #dd3333,inset 0 0 0 2px #ffffff}.mw-ui-button.mw-ui-destructive:active,.mw-ui-button.mw-ui-destructive.is-on,.mw-ui-button.mw-ui-destructive.mw-ui-checked{background-color:#b32424;border-color:#b32424;box-shadow:none}.mw-ui-button.mw-ui-destructive:disabled{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1}.mw-ui-button.mw-ui-destructive:disabled:hover,.mw-ui-button.mw-ui-destructive:disabled:active,.mw-ui-button.mw-ui-destructive:disabled.mw-ui-checked{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1;box-shadow:none}.mw-ui-button.mw-ui-destructive.mw-ui-quiet{color:#222222}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:hover{background-color:transparent;color:#ff4242}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:active,.mw-ui-button.mw-ui-destructive.mw-ui-quiet.mw-ui-checked{color:#b32424}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:focus{background-color:transparent;color:#dd3333}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:disabled{color:#c8ccd1}.mw-ui-button.mw-ui-quiet{background:transparent;border:0;text-shadow:none;color:#222222}.mw-ui-button.mw-ui-quiet:hover{background-color:transparent;color:#444444}.mw-ui-button.mw-ui-quiet:active,.mw-ui-button.mw-ui-quiet.mw-ui-checked{color:#000000}.mw-ui-button.mw-ui-quiet:focus{background-color:transparent;color:#222222}.mw-ui-button.mw-ui-quiet:disabled{color:#c8ccd1}.mw-ui-button.mw-ui-quiet:hover,.mw-ui-button.mw-ui-quiet:focus{box-shadow:none}.mw-ui-button.mw-ui-quiet:active,.mw-ui-button.mw-ui-quiet:disabled{background:transparent}input.mw-ui-button::-moz-focus-inner,button.mw-ui-button::-moz-focus-inner{margin-top:-1px;margin-bottom:-1px}a.mw-ui-button{text-decoration:none}a.mw-ui-button:hover,a.mw-ui-button:focus{text-decoration:none}.mw-ui-button-group > *{min-width:48px;border-radius:0;float:left}.mw-ui-button-group > *:first-child{border-top-left-radius:2px;border-bottom-left-radius:2px}.mw-ui-button-group > *:not(:first-child){border-left:0}.mw-ui-button-group > *:last-child{border-top-right-radius:2px;border-bottom-right-radius:2px}.mw-ui-button-group .is-on .button{cursor:default}
.mw-ui-icon{position:relative;line-height:1.5em;min-height:1.5em;min-width:1.5em}span.mw-ui-icon{display:inline-block}.mw-ui-icon.mw-ui-icon-element{text-indent:-999px;overflow:hidden;width:3.5em;min-width:3.5em;max-width:3.5em}.mw-ui-icon.mw-ui-icon-element:before{left:0;right:0;position:absolute;margin:0 1em}.mw-ui-icon.mw-ui-icon-element.mw-ui-icon-large{width:4.625em;min-width:4.625em;max-width:4.625em;line-height:4.625em;min-height:4.625em}.mw-ui-icon.mw-ui-icon-element.mw-ui-icon-large:before{min-height:4.625em}.mw-ui-icon.mw-ui-icon-before:before,.mw-ui-icon.mw-ui-icon-element:before{background-position:50% 50%;background-repeat:no-repeat;background-size:100% auto;float:left;display:block;min-height:1.5em;content:''}.mw-ui-icon.mw-ui-icon-before:before{position:relative;width:1.5em;margin-right:1em}.mw-ui-icon.mw-ui-icon-small:before{background-size:66.67% auto}
.mw-editfont-monospace{font-family:monospace,monospace}.mw-editfont-sans-serif{font-family:sans-serif}.mw-editfont-serif{font-family:serif} .mw-editfont-monospace,.mw-editfont-sans-serif,.mw-editfont-serif{font-size:13px; }.mw-editfont-monospace.oo-ui-textInputWidget,.mw-editfont-sans-serif.oo-ui-textInputWidget,.mw-editfont-serif.oo-ui-textInputWidget{font-size:inherit}.mw-editfont-monospace > .oo-ui-inputWidget-input,.mw-editfont-sans-serif > .oo-ui-inputWidget-input,.mw-editfont-serif > .oo-ui-inputWidget-input{font-size:13px}
.uls-menu{border-radius:2px; font-size:medium}.uls-search,.uls-language-settings-close-block{border-top-right-radius:2px;border-top-left-radius:2px}.uls-language-list{border-bottom-right-radius:2px;border-bottom-left-radius:2px}.uls-menu.callout:before,.uls-menu.callout:after{border-top:10px solid transparent;border-bottom:10px solid transparent;display:inline-block; top:17px;position:absolute;content:''}.uls-menu.callout.selector-right:before{ border-left:10px solid #c8ccd1; right:-11px}.uls-menu.callout.selector-right:after{ border-left:10px solid #f8f9fa; right:-10px}.uls-menu.callout.selector-left:before{ border-right:10px solid #c8ccd1; left:-11px}.uls-menu.callout.selector-left:after{ border-right:10px solid #f8f9fa; left:-10px}.uls-ui-languages button{margin:5px 15px 5px 0;white-space:nowrap;overflow:hidden}.uls-search-wrapper-wrapper{position:relative;padding-left:40px;margin-top:5px;margin-bottom:5px}.uls-icon-back{background:transparent url(/w/extensions/UniversalLanguageSelector/resources/images/back-grey-ltr.png?90e9b) no-repeat scroll center center;background-image:-webkit-linear-gradient(transparent,transparent),url(/w/extensions/UniversalLanguageSelector/resources/images/back-grey-ltr.svg?e226b);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2224%22 height=%2224%22 viewBox=%220 0 24 24%22%3E %3Cpath fill=%22%2354595d%22 d=%22M7 13.1l8.9 8.9c.8-.8.8-2 0-2.8l-6.1-6.1 6-6.1c.8-.8.8-2 0-2.8L7 13.1z%22/%3E %3C/svg%3E");background-size:28px;background-position:center center;height:32px;width:40px;display:block;position:absolute;left:0;border-right:1px solid #c8ccd1;opacity:0.8}.uls-icon-back:hover{opacity:1;cursor:pointer}.uls-menu .uls-no-results-view .uls-no-found-more{background-color:#fff}.uls-menu .uls-no-results-view h3{padding:0 28px;margin:0;color:#54595d;font-size:1em;font-weight:normal}  .skin-vector .uls-menu{border-color:#c8ccd1;-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.25);box-shadow:0 2px 2px 0 rgba(0,0,0,0.25);font-size:0.875em}.skin-vector .uls-search{border-bottom-color:#c8ccd1}.skin-vector .uls-filtersuggestion{color:#72777d}.skin-vector .uls-lcd-region-title{color:#54595d}
.mw-spinner{background-color:transparent;background-position:center center;background-repeat:no-repeat}.mw-spinner-small{background-image:url(data:image/gif;base64,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);background-image:url(/w/resources/src/jquery.spinner/images/spinner.gif?ca65b)!ie;height:20px;width:20px; min-width:20px}.mw-spinner-large{background-image:url(data:image/gif;base64,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);background-image:url(/w/resources/src/jquery.spinner/images/spinner-large.gif?57f34)!ie;height:32px;width:32px; min-width:32px}.mw-spinner-block{display:block; width:100%}.mw-spinner-inline{display:inline-block;vertical-align:middle}
@media print{#centralNotice{display:none}}.cn-closeButton{display:inline-block;zoom:1;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUBAMAAAB/pwA+AAAAElBMVEUAAAAQEBDPz88AAABAQEDv7+9oe1vvAAAABnRSTlMA3rLe3rJS22KzAAAARElEQVQI12PAAUIUQCSTK5BwFgIxFU1AhKECUFAYKAAioXwwBeZChMGCEGGQIFQYJohgIhQgtCEMQ7ECYTHCOciOxA4AADgJTXIb9s8AAAAASUVORK5CYII=) no-repeat;background:url(/w/extensions/CentralNotice/resources/subscribing/close.png?8e3d8) no-repeat!ie;width:20px;height:20px;text-indent:20px;white-space:nowrap;overflow:hidden}</style><style>
.suggestions a.mw-searchSuggest-link,.suggestions a.mw-searchSuggest-link:hover,.suggestions a.mw-searchSuggest-link:active,.suggestions a.mw-searchSuggest-link:focus{color:#000;text-decoration:none}.suggestions-result-current a.mw-searchSuggest-link,.suggestions-result-current a.mw-searchSuggest-link:hover,.suggestions-result-current a.mw-searchSuggest-link:active,.suggestions-result-current a.mw-searchSuggest-link:focus{color:#fff}.suggestions a.mw-searchSuggest-link .special-query{ overflow:hidden;text-overflow:ellipsis;white-space:nowrap}
.mw-mmv-overlay{position:fixed;top:0;left:0;right:0;bottom:0;z-index:1000;background-color:#000}body.mw-mmv-lightbox-open{overflow-y:auto;  }body.mw-mmv-lightbox-open #mw-page-base,body.mw-mmv-lightbox-open #mw-head-base,body.mw-mmv-lightbox-open #mw-navigation,body.mw-mmv-lightbox-open #content,body.mw-mmv-lightbox-open #footer,body.mw-mmv-lightbox-open #globalWrapper{ display:none}body.mw-mmv-lightbox-open > *{ display:none}body.mw-mmv-lightbox-open > .mw-mmv-overlay,body.mw-mmv-lightbox-open > .mw-mmv-wrapper{display:block}.mw-mmv-filepage-buttons{margin-top:5px}.mw-mmv-filepage-buttons .mw-mmv-view-expanded,.mw-mmv-filepage-buttons .mw-mmv-view-config{display:block;line-height:inherit}.mw-mmv-filepage-buttons .mw-mmv-view-expanded.mw-ui-icon:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 1024 768%22%3E %3Cpath d=%22M851.2 71.6L690.7 232.1l-40.1-40.3-9.6 164.8 164.8-9.3-40.3-40.4L926 146.4l58.5 58.5L997.6 0 792.7 13.1%22/%3E %3Cpath d=%22M769.6 89.3H611.9l70.9 70.8 7.9 7.5m-47.1 234.6l-51.2 3 3-51.2 9.4-164.4 5.8-100.3H26.4V768h883.1V387l-100.9 5.8-165 9.4zM813.9 678H113.6l207.2-270.2 31.5-12.9L548 599.8l105.9-63.2 159.8 140.8.2.6zm95.6-291.9V228l-79.1 78.9 7.8 7.9%22/%3E %3C/svg%3E")}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 1024 768%22%3E %3Cpath d=%22M897 454.6V313.4L810.4 299c-6.4-23.3-16-45.7-27.3-65.8l50.5-71.4-99.4-100.2-71.4 50.5c-20.9-11.2-42.5-20.9-65.8-27.3L582.6-1H441.4L427 85.6c-23.3 6.4-45.7 16-65.8 27.3l-71.4-50.5-100.3 99.5 50.5 71.4c-11.2 20.9-20.9 42.5-27.3 66.6L127 313.4v141.2l85.8 14.4c6.4 23.3 16 45.7 27.3 66.6L189.6 607l99.5 99.5 71.4-50.5c20.9 11.2 42.5 20.9 66.6 27.3l14.4 85.8h141.2l14.4-86.6c23.3-6.4 45.7-16 65.8-27.3l71.4 50.5 99.5-99.5-50.5-71.4c11.2-20.9 20.9-42.5 27.3-66.6l86.4-13.6zm-385 77c-81.8 0-147.6-66.6-147.6-147.6 0-81.8 66.6-147.6 147.6-147.6S659.6 302.2 659.6 384 593.8 531.6 512 531.6z%22/%3E %3C/svg%3E");opacity:0.75}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before:hover{opacity:1}.mw-mmv-button{background-color:transparent;min-width:0;border:0;padding:0;overflow-x:hidden;text-indent:-9999em}
.ve-init-mw-tempWikitextEditorWidget{border:0;padding:0;color:inherit;line-height:1.5em; }.ve-init-mw-tempWikitextEditorWidget:focus{outline:0;padding:0}.ve-init-mw-tempWikitextEditorWidget::selection{background:rgba(109,169,247,0.5); }
#uls-settings-block{background-color:#f8f9fa;border-top:1px solid #c8ccd1;padding-left:10px;line-height:1.2em;border-radius:0 0 2px 2px}#uls-settings-block > button{background:left top transparent no-repeat;background-size:20px auto;color:#54595d;display:inline-block;margin:8px 15px;border:0;padding:0 0 0 26px;font-size:medium;cursor:pointer}#uls-settings-block > button:hover{color:#222}#uls-settings-block > button.display-settings-block{background-image:url(/w/extensions/UniversalLanguageSelector/resources/images/display.png?d25f1);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Cpath fill=%22%23222%22 d=%22M.002 2.275V15.22h8.405c.535 1.624-.975 1.786-1.902 2.505 0 0 2.293-.024 3.439-.024 1.144 0 3.432.024 3.432.024-.905-.688-2.355-.868-1.902-2.505h8.527V2.275h-20zm6.81 1.84h.797l3.313 8.466H9.879L8.836 9.943H5.462l-1.043 2.638h-.982zm.368 1.104c-.084.369-.211.785-.368 1.227L5.83 9.023h2.699l-.982-2.577c-.128-.33-.234-.747-.368-1.227zm7.117.982c.753 0 1.295.157 1.656.491.365.334.552.858.552 1.595v4.294h-.675l-.184-.859h-.062c-.315.396-.605.655-.92.798-.311.138-.758.184-1.227.184-.626 0-1.115-.168-1.472-.491-.353-.323-.491-.754-.491-1.35 0-1.275 1.028-1.963 3.068-2.025h1.043v-.429c0-.495-.091-.87-.307-1.104-.211-.238-.574-.307-1.043-.307-.526 0-1.115.107-1.779.429l-.307-.675a4.748 4.748 0 0 1 1.043-.429 4.334 4.334 0 0 1 1.104-.123zm.307 3.313c-.761.027-1.318.157-1.656.368-.334.207-.491.54-.491.982 0 .346.1.617.307.798.211.181.544.245.92.245.595 0 1.012-.164 1.35-.491.342-.326.552-.762.552-1.35v-.552z%22/%3E %3C/svg%3E")}#uls-settings-block > button.input-settings-block{background-image:url(/w/extensions/UniversalLanguageSelector/resources/images/input.png?aea9e);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Cpath fill=%22%23222%22 d=%22M9 1.281c-.124.259-.185.599-.5.688-.55.081-1.133.018-1.688 0-.866-.032-1.733-.148-2.594 0-.588.157-.953.727-1.188 1.25-.178.416-.271.836-.344 1.281H-.002V16h20V4.5H3.654c.109-.52.203-1.057.563-1.469.222-.231.587-.17.875-.188 1.212.003 2.415.179 3.625.063.463-.058.812-.455.969-.875l.188-.438-.875-.313zM1.875 7.125h1.563c.094 0 .188.093.188.188v1.531a.201.201 0 0 1-.188.188H1.875c-.094 0-.156-.093-.156-.188V7.313c0-.094.062-.188.156-.188zm2.844 0h1.563c.094 0 .156.093.156.188v1.531c0 .094-.062.188-.156.188H4.719c-.094 0-.156-.093-.156-.188V7.313c0-.094.062-.188.156-.188zm2.844 0h1.563c.094 0 .156.093.156.188v1.531c0 .094-.062.188-.156.188H7.563a.201.201 0 0 1-.188-.188V7.313c0-.094.093-.188.188-.188zm2.813 0h1.563c.094 0 .188.093.188.188v1.531a.201.201 0 0 1-.188.188h-1.563c-.094 0-.156-.093-.156-.188V7.313c0-.094.062-.188.156-.188zm2.844 0h1.563c.094 0 .156.093.156.188v1.531c0 .094-.062.188-.156.188H13.22c-.094 0-.156-.093-.156-.188V7.313c0-.094.062-.188.156-.188zm2.844 0h1.531c.094 0 .188.093.188.188v1.531a.201.201 0 0 1-.188.188h-1.531a.201.201 0 0 1-.188-.188V7.313c0-.094.093-.188.188-.188zm-12.844 3h1.563c.094 0 .156.093.156.188v1.563c0 .094-.062.156-.156.156H3.22c-.094 0-.156-.062-.156-.156v-1.563c0-.094.062-.188.156-.188zm2.906 0h1.563c.094 0 .188.093.188.188v1.563c0 .094-.093.156-.188.156H6.126c-.094 0-.156-.062-.156-.156v-1.563c0-.094.062-.188.156-.188zm2.938 0h1.531c.094 0 .188.093.188.188v1.563c0 .094-.093.156-.188.156H9.064c-.094 0-.188-.062-.188-.156v-1.563c0-.094.093-.188.188-.188zm2.906 0h1.563c.094 0 .156.093.156.188v1.563c0 .094-.062.156-.156.156H11.97c-.094 0-.188-.062-.188-.156v-1.563c0-.094.093-.188.188-.188zm2.906 0h1.563c.094 0 .156.093.156.188v1.563c0 .094-.062.156-.156.156h-1.563c-.094 0-.156-.062-.156-.156v-1.563c0-.094.062-.188.156-.188zM4.001 13.688h12c.088 0 .156.068.156.156v.844a.154.154 0 0 1-.156.156h-12a.154.154 0 0 1-.156-.156v-.844c0-.088.068-.156.156-.156z%22/%3E %3C/svg%3E")}
#p-lang .body ul .uls-trigger,#p-lang .pBody ul .uls-trigger{background-image:none;padding:0} .mw-interlanguage-selector,.mw-interlanguage-selector:active{cursor:pointer;padding:4px 6px 4px 25px;font-size:13px;font-weight:normal;background-image:url(/w/extensions/UniversalLanguageSelector/resources/images/compact-links-trigger.png?b0c8e);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2218%22 height=%2218%22 viewBox=%220 0 24 24%22%3E %3Cpath fill=%22%2372777d%22 d=%22M13 19l.8-3h5.3l.9 3h2.2L18 6h-3l-4.2 13H13zm3.5-11l2 6h-4l2-6zM5 4l.938 1.906H1V8h1.594C3.194 9.8 4 11.206 5 12.406c-1.1.7-4.313 1.781-4.313 1.781L2 16s3.487-1.387 4.688-2.188c1 .7 2.319 1.188 3.719 1.688l.594-2c-1-.3-1.988-.688-2.688-1.188 1.1-1.1 1.9-2.506 2.5-4.406h2.188l.5-2H7.938L7 4H5zm-.188 4h3.781c-.4 1.3-.906 2-1.906 3-1.1-1-1.475-1.7-1.875-3z%22/%3E %3C/svg%3E");background-size:18px;background-repeat:no-repeat;background-position:left 4px center;margin:4px 0;text-align:left}.mw-interlanguage-selector:active,.mw-interlanguage-selector.selector-open{background-color:#c8ccd1;color:#54595d}.interlanguage-uls-menu:before,.interlanguage-uls-menu:after{border-top:10px solid transparent;border-bottom:10px solid transparent;display:inline-block; top:17px;position:absolute;content:''}.interlanguage-uls-menu.selector-right:before{ border-left:10px solid #c8ccd1; right:-11px}.interlanguage-uls-menu.selector-right:after{ border-left:10px solid #f8f9fa; right:-10px}.interlanguage-uls-menu.selector-left:before{ border-right:10px solid #c8ccd1; left:-11px}.interlanguage-uls-menu.selector-left:after{ border-right:10px solid #f8f9fa; left:-10px}
.mw-3d-wrapper{display:inline-block;position:relative;overflow:hidden;vertical-align:top}.mw-3d-badge{position:absolute;top:11px;left:11px;color:#1e1f21;font-size:14px;line-height:19px;font-weight:bold;opacity:0.8;padding:2px 5px;background-color:#f8f9fa;border-radius:2px}.mw-3d-thumb-placeholder{display:inline-block;text-decoration:none;color:#222}</style><style>
.ve-activated .ve-init-mw-desktopArticleTarget-editableContent #toc,.ve-activated #siteNotice,.ve-activated .mw-indicators,.ve-activated #t-print,.ve-activated #t-permalink,.ve-activated #p-coll-print_export,.ve-activated #t-cite,.ve-deactivating .ve-ui-surface,.ve-active .ve-init-mw-desktopArticleTarget-editableContent,.ve-active .ve-init-mw-tempWikitextEditorWidget{display:none} .ve-activating .ve-ui-surface{height:0;padding:0 !important; overflow:hidden} .ve-loading #content > :not(.ve-init-mw-desktopArticleTarget-loading-overlay), .ve-activated .ve-init-mw-desktopArticleTarget-uneditableContent{pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;opacity:0.5}.ve-activated #catlinks{cursor:pointer}.ve-activated #catlinks a{opacity:1} .ve-activated #content{position:relative} .ve-init-mw-desktopArticleTarget-loading-overlay{position:absolute;top:1.25em;left:0;right:0;z-index:1;margin-top:-0.5em}.ve-init-mw-desktopArticleTarget-progress{height:1em;overflow:hidden;margin:0 25%}.ve-init-mw-desktopArticleTarget-progress-bar{height:1em;width:0}.ve-init-mw-desktopArticleTarget-toolbarPlaceholder{transition:height 250ms ease;height:0; } .oo-ui-element-hidden{display:none !important; } .mw-editsection{white-space:nowrap; unicode-bidi:-moz-isolate;unicode-bidi:-webkit-isolate;unicode-bidi:isolate}.mw-editsection-divider{color:#54595d} .ve-init-mw-desktopArticleTarget-progress{height:0.75em;border:1px solid #36c;background:#fff;border-radius:2px;box-shadow:0 0.1em 0 0 rgba(0,0,0,0.15)}.ve-init-mw-desktopArticleTarget-progress-bar{height:0.75em;background:#36c}.ve-init-mw-desktopArticleTarget-toolbarPlaceholder{border-bottom:1px solid #c8ccd1;box-shadow:0 1px 1px 0 rgba(0,0,0,0.1)}.ve-init-mw-desktopArticleTarget-toolbarPlaceholder-open{height:40px} .skin-vector .ve-init-mw-desktopArticleTarget-toolbar,.skin-vector .ve-init-mw-desktopArticleTarget-toolbarPlaceholder{font-size:0.875em; margin:-1.14em -1.14em 1.14em -1.14em; }@media screen and (min-width:982px){.skin-vector .ve-init-mw-desktopArticleTarget-toolbar,.skin-vector .ve-init-mw-desktopArticleTarget-toolbarPlaceholder{ margin:-1.43em -1.71em 1.43em -1.71em}}</style><style>
.mw-ui-icon-popups-settings:before{background-image:url(/w/load.php?modules=ext.popups.images&image=popups-settings&format=rasterized&lang=en&skin=vector&version=1t0nopp);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg width=%2220px%22 height=%2220px%22 viewbox=%220 0 20 20%22 xmlns=%22http://www.w3.org/2000/svg%22%3E %3Cg fill=%22%2354595D%22%3E %3Cpath d=%22M10.112 4.554a5.334 5.334 0 1 0 0 10.668 5.334 5.334 0 0 0 0-10.668zm0 7.823a2.49 2.49 0 1 1 0-4.978 2.49 2.49 0 0 1 0 4.978z%22/%3E %3Cpath d=%22M11.4 5.303L11.05 3h-2.1L8.6 5.303a4.9 4.9 0 0 1 2.8 0zm-2.8 9.394L8.95 17h2.1l.35-2.303a4.9 4.9 0 0 1-2.8 0zm5.712-7.028l1.4-1.876L14.2 4.309l-1.876 1.4a4.9 4.9 0 0 1 1.981 1.981l.007-.021zm-8.624 4.662L4.309 14.2 5.8 15.691l1.876-1.4a4.9 4.9 0 0 1-1.981-1.981l-.007.021zm9.009-.931L17 11.05v-2.1l-2.303-.35a4.9 4.9 0 0 1 0 2.8zM5.303 8.6L3 8.95v2.1l2.303.35a4.9 4.9 0 0 1 0-2.8zm7.028 5.712l1.876 1.4 1.484-1.512-1.4-1.876a4.9 4.9 0 0 1-1.981 1.981l.021.007zM7.669 5.688L5.8 4.309 4.309 5.8l1.4 1.876a4.9 4.9 0 0 1 1.96-1.988z%22/%3E %3C/g%3E %3C/svg%3E")}.mw-ui-icon-popups-close:before{background-image:url(/w/load.php?modules=ext.popups.images&image=popups-close&format=rasterized&lang=en&skin=vector&version=1t0nopp);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Cpath d=%22M3.636 2.222l14.142 14.142-1.414 1.414L2.222 3.636z%22/%3E %3Cpath d=%22M17.778 3.636L3.636 17.778l-1.414-1.414L16.364 2.222z%22/%3E %3C/svg%3E")}.mw-ui-icon-preview-generic:before{background-image:url(/w/load.php?modules=ext.popups.images&image=preview-generic&format=rasterized&lang=en&skin=vector&version=1t0nopp);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg width=%2237%22 height=%2227%22 xmlns=%22http://www.w3.org/2000/svg%22%3E %3Cg id=%22Page-1%22 fill=%22none%22 fill-rule=%22evenodd%22%3E %3Cg id=%22sad-face%22 fill=%22%23C8CCD1%22%3E %3Cpath d=%22M5.475.7v20.075L0 26.25h31.025c3.102 0 5.475-2.372 5.475-5.475V.7H5.475zm20.44 4.562c1.277 0 2.19 1.095 2.19 2.19 0 1.096-.913 2.373-2.19 2.373-1.278 0-2.19-1.095-2.19-2.19s1.095-2.373 2.19-2.373zm-9.855 0c1.277 0 2.19 1.095 2.19 2.19 0 1.096-1.095 2.373-2.19 2.373s-2.19-1.095-2.19-2.19.913-2.373 2.19-2.373zm4.928 8.213c-7.153 0-8.415 7.012-8.415 7.012s2.805-1.403 8.415-1.403c5.61 0 8.414 1.403 8.414 1.403S28 13.475 20.988 13.475z%22 id=%22Shape%22/%3E %3C/g%3E %3C/g%3E %3C/svg%3E")}.mw-ui-icon-footer:before{background-image:url(/w/load.php?modules=ext.popups.images&image=footer&format=rasterized&lang=en&skin=vector&version=1t0nopp);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%22230%22 height=%22179%22 xmlns:xlink=%22http://www.w3.org/1999/xlink%22 viewBox=%220 0 230 179%22%3E %3Cdefs%3E %3Crect id=%22a%22 width=%22201%22 height=%2213%22 rx=%222%22/%3E %3Crect id=%22b%22 width=%22201%22 height=%22169%22 y=%2210%22 rx=%222%22/%3E %3Crect id=%22c%22 width=%2230%22 height=%222%22 x=%22135%22 y=%22158%22 rx=%221%22/%3E %3C/defs%3E %3Cg fill=%22none%22 fill-rule=%22evenodd%22%3E %3Cg transform=%22matrix%281 0 0 -1 0 13%29%22%3E %3Cuse fill=%22%23f8f9fa%22 xlink:href=%22%23a%22/%3E %3Crect width=%22199%22 height=%2211%22 x=%221%22 y=%221%22 stroke=%22%23a2a9b1%22 stroke-width=%222%22 rx=%222%22/%3E %3C/g%3E %3Cuse fill=%22%23fff%22 xlink:href=%22%23b%22/%3E %3Crect width=%22199%22 height=%22167%22 x=%221%22 y=%2211%22 stroke=%22%23a2a9b1%22 stroke-width=%222%22 rx=%222%22/%3E %3Cg opacity=%22.4%22 transform=%22translate%2867 35%29%22%3E %3Crect width=%2273%22 height=%222%22 y=%227%22 fill=%22%23c8ccd1%22 rx=%221%22/%3E %3Crect width=%2281%22 height=%222%22 y=%2231%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2232%22 height=%222%22 y=%2285%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2273%22 height=%222%22 x=%2235%22 y=%2285%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2217%22 height=%222%22 y=%2245%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2217%22 height=%222%22 x=%2291%22 y=%2245%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2268%22 height=%222%22 x=%2220%22 y=%2245%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2217%22 height=%222%22 y=%2278%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2237%22 height=%222%22 x=%2272%22 y=%2278%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2249%22 height=%222%22 x=%2220%22 y=%2278%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2224%22 height=%222%22 x=%2284%22 y=%2231%22 fill=%22%2372777d%22 rx=%221%22 transform=%22matrix%28-1 0 0 1 192 0%29%22/%3E %3Crect width=%2281%22 height=%222%22 y=%2266%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2214%22 height=%222%22 x=%2254%22 y=%2224%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2237%22 height=%222%22 x=%2271%22 y=%2224%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2251%22 height=%222%22 y=%2224%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2259%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2252%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2292%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2238%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2251%22 height=%222%22 fill=%22%2372777d%22 rx=%221%22/%3E %3C/g%3E %3Crect width=%2230%22 height=%222%22 x=%2267%22 y=%22158%22 fill=%22%2372777d%22 opacity=%22.4%22 rx=%221%22/%3E %3Crect width=%2230%22 height=%222%22 x=%2299%22 y=%22158%22 fill=%22%2372777d%22 opacity=%22.4%22 rx=%221%22/%3E %3Cuse fill=%22%2336c%22 xlink:href=%22%23c%22/%3E %3Crect width=%2233%22 height=%225%22 x=%22133.5%22 y=%22156.5%22 stroke=%22%23ffc057%22 stroke-opacity=%22.447%22 stroke-width=%223%22 rx=%222.5%22/%3E %3Ccircle cx=%2234%22 cy=%2249%22 r=%2219%22 fill=%22%23eaecf0%22/%3E %3Cg fill=%22%23a2a9b1%22 transform=%22translate%285 5%29%22%3E %3Ccircle cx=%221.5%22 cy=%221.5%22 r=%221.5%22/%3E %3Ccircle cx=%226%22 cy=%221.5%22 r=%221.5%22/%3E %3Ccircle cx=%2210.5%22 cy=%221.5%22 r=%221.5%22/%3E %3C/g%3E %3Cpath stroke=%22%23ff00af%22 d=%22M174.5 159.5h54.01%22 stroke-linecap=%22square%22/%3E %3C/g%3E %3C/svg%3E")}.mw-ui-icon-preview-disambiguation:before{background-image:url(/w/load.php?modules=ext.popups.images&image=preview-disambiguation&format=rasterized&lang=en&skin=vector&version=1t0nopp);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%222 2 20 20%22%3E %3Cpath fill=%22%23C8CCD1%22 d=%22M11 12h4V7h-4v5zm-5 2h9v-1H6v1zm0 2h9v-1H6v1zm0 2h9v-1H6v1zm4-9H6v1h4V9zm0 2H6v1h4v-1zm0-4H6v1h4V7zM4 5h13v16H7c-1.7 0-3-1.3-3-3V5z%22/%3E %3Cpath fill-rule=%22evenodd%22 fill=%22%23C8CCD1%22 d=%22M18 4v14h2V2H7v2%22/%3E %3C/svg%3E")}</style><style>
@-webkit-keyframes mwe-popups-fade-in-up{0%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@-moz-keyframes mwe-popups-fade-in-up{0%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@keyframes mwe-popups-fade-in-up{0%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@-webkit-keyframes mwe-popups-fade-in-down{0%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@-moz-keyframes mwe-popups-fade-in-down{0%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@keyframes mwe-popups-fade-in-down{0%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}100%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}}@-webkit-keyframes mwe-popups-fade-out-down{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}}@-moz-keyframes mwe-popups-fade-out-down{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}}@keyframes mwe-popups-fade-out-down{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,20px);-moz-transform:translate(0,20px);-ms-transform:translate(0,20px);transform:translate(0,20px)}}@-webkit-keyframes mwe-popups-fade-out-up{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}}@-moz-keyframes mwe-popups-fade-out-up{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}}@keyframes mwe-popups-fade-out-up{0%{opacity:1;-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}100%{opacity:0;-webkit-transform:translate(0,-20px);-moz-transform:translate(0,-20px);-ms-transform:translate(0,-20px);transform:translate(0,-20px)}}.mwe-popups-fade-in-up{-webkit-animation:mwe-popups-fade-in-up 0.2s ease forwards;-moz-animation:mwe-popups-fade-in-up 0.2s ease forwards;animation:mwe-popups-fade-in-up 0.2s ease forwards}.mwe-popups-fade-in-down{-webkit-animation:mwe-popups-fade-in-down 0.2s ease forwards;-moz-animation:mwe-popups-fade-in-down 0.2s ease forwards;animation:mwe-popups-fade-in-down 0.2s ease forwards}.mwe-popups-fade-out-down{-webkit-animation:mwe-popups-fade-out-down 0.2s ease forwards;-moz-animation:mwe-popups-fade-out-down 0.2s ease forwards;animation:mwe-popups-fade-out-down 0.2s ease forwards}.mwe-popups-fade-out-up{-webkit-animation:mwe-popups-fade-out-up 0.2s ease forwards;-moz-animation:mwe-popups-fade-out-up 0.2s ease forwards;animation:mwe-popups-fade-out-up 0.2s ease forwards}   #mwe-popups-settings{z-index:1000;background:#fff;width:420px;border:1px solid #a2a9b1;box-shadow:0 2px 2px 0 rgba(0,0,0,0.25);border-radius:2px;font-size:14px}#mwe-popups-settings header{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;border-bottom:1px solid #c8ccd1;position:relative;display:table;width:100%;padding:5px 7px 5px 0}#mwe-popups-settings header > div{display:table-cell;width:3.5em;vertical-align:middle;cursor:pointer}#mwe-popups-settings header h1{margin-bottom:0.6em;padding-top:0.5em;border:0;width:100%;font-family:sans-serif;font-size:18px;font-weight:bold;text-align:center}#mwe-popups-settings .mwe-ui-icon-popups-close{opacity:0.87;-webkit-transition:opacity 100ms;-moz-transition:opacity 100ms;transition:opacity 100ms}#mwe-popups-settings .mwe-ui-icon-popups-close:hover{opacity:0.73}#mwe-popups-settings .mwe-ui-icon-popups-close:active{opacity:1}#mwe-popups-settings main{display:block;width:350px;padding:32px 0 24px;margin:0 auto}#mwe-popups-settings main p{color:#54595d;font-size:17px;margin:16px 0 0}#mwe-popups-settings main p:first-child{margin-top:0}#mwe-popups-settings main form img,#mwe-popups-settings main form input,#mwe-popups-settings main form label{vertical-align:top}#mwe-popups-settings main form img{margin-right:60px}#mwe-popups-settings main form input{display:inline-block;margin:0 10px 0 0;padding:0}#mwe-popups-settings main form label{font-size:13px;display:inline-block;line-height:16px;width:300px}#mwe-popups-settings main form label > span{color:#000;font-size:18px;font-weight:bold;display:block;margin-bottom:5px;line-height:18px}.mwe-popups-settings-help{font-size:13px;font-weight:800;margin:40px;position:relative}.mwe-popups-settings-help .mw-ui-icon:before,.mwe-popups-settings-help .mw-ui-icon{height:140px;width:180px;max-width:none;margin:0}.mwe-popups-settings-help p{left:180px;bottom:20px;position:absolute}.mwe-popups{cursor:pointer;background:#fff;position:absolute;z-index:110;-webkit-box-shadow:0 30px 90px -20px rgba(0,0,0,0.3),0 0 1px #a2a9b1;box-shadow:0 30px 90px -20px rgba(0,0,0,0.3),0 0 1px #a2a9b1;padding:0;display:none;font-size:14px;line-height:20px;min-width:300px;border-radius:2px; }.mwe-popups .mw-ui-icon{font-size:16px}.mwe-popups .mw-ui-icon-preview-disambiguation,.mwe-popups .mw-ui-icon-preview-generic{margin:21px 0 8px 0}.mwe-popups .mwe-popups-container{color:#222222;margin-top:-9px;padding-top:9px;text-decoration:none}.mwe-popups .mwe-popups-container footer{padding:16px;margin:0;font-size:10px;position:absolute;bottom:0;left:0}.mwe-popups .mwe-popups-extract{margin:16px;display:block;color:#222222;text-decoration:none;position:relative;   }.mwe-popups .mwe-popups-extract:hover{text-decoration:none}.mwe-popups .mwe-popups-extract:after{content:' ';position:absolute;bottom:0;width:25%;height:20px;background-color:transparent}.mwe-popups .mwe-popups-extract[dir='ltr']:after{ right:0; background-image:-webkit-linear-gradient(to right,rgba(255,255,255,0),#ffffff 50%); background-image:-moz-linear-gradient(to right,rgba(255,255,255,0),#ffffff 50%); background-image:-o-linear-gradient(to right,rgba(255,255,255,0),#ffffff 50%); background-image:linear-gradient(to right,rgba(255,255,255,0),#ffffff 50%)}.mwe-popups .mwe-popups-extract[dir='rtl']:after{ left:0; background-image:-webkit-linear-gradient(to left,rgba(255,255,255,0),#ffffff 50%); background-image:-moz-linear-gradient(to left,rgba(255,255,255,0),#ffffff 50%); background-image:-o-linear-gradient(to left,rgba(255,255,255,0),#ffffff 50%); background-image:linear-gradient(to left,rgba(255,255,255,0),#ffffff 50%)}.mwe-popups .mwe-popups-extract p{margin:0}.mwe-popups .mwe-popups-extract ul,.mwe-popups .mwe-popups-extract ol,.mwe-popups .mwe-popups-extract li,.mwe-popups .mwe-popups-extract dl,.mwe-popups .mwe-popups-extract dd,.mwe-popups .mwe-popups-extract dt{margin-top:0;margin-bottom:0}.mwe-popups svg{overflow:hidden}.mwe-popups.mwe-popups-is-tall{width:450px}.mwe-popups.mwe-popups-is-tall > div > a > svg{vertical-align:middle}.mwe-popups.mwe-popups-is-tall .mwe-popups-extract{width:215px;height:180px;overflow:hidden;float:left}.mwe-popups.mwe-popups-is-tall footer{width:215px;left:0}.mwe-popups.mwe-popups-is-not-tall{width:320px}.mwe-popups.mwe-popups-is-not-tall .mwe-popups-extract{min-height:40px;max-height:140px;overflow:hidden;margin-bottom:47px;padding-bottom:0}.mwe-popups.mwe-popups-is-not-tall footer{width:290px}.mwe-popups.mwe-popups-type-generic .mwe-popups-extract,.mwe-popups.mwe-popups-type-disambiguation .mwe-popups-extract{min-height:auto;padding-top:4px;margin-bottom:60px;margin-top:0}.mwe-popups.mwe-popups-type-generic .mwe-popups-read-link,.mwe-popups.mwe-popups-type-disambiguation .mwe-popups-read-link{font-weight:bold;font-size:12px}.mwe-popups.mwe-popups-type-generic .mwe-popups-extract:hover + footer .mwe-popups-read-link,.mwe-popups.mwe-popups-type-disambiguation .mwe-popups-extract:hover + footer .mwe-popups-read-link{text-decoration:underline}.mwe-popups.mwe-popups-no-image-pointer:before{content:'';position:absolute;border:8px solid transparent;border-top:0;border-bottom:8px solid #a2a9b1;top:-8px;left:10px}.mwe-popups.mwe-popups-no-image-pointer:after{content:'';position:absolute;border:11px solid transparent;border-top:0;border-bottom:11px solid #ffffff;top:-7px;left:7px}.mwe-popups.flipped-x.mwe-popups-no-image-pointer:before{left:auto;right:10px}.mwe-popups.flipped-x.mwe-popups-no-image-pointer:after{left:auto;right:7px}.mwe-popups.mwe-popups-image-pointer:before{content:'';position:absolute;border:9px solid transparent;border-top:0;border-bottom:9px solid #a2a9b1;top:-9px;left:9px;z-index:111}.mwe-popups.mwe-popups-image-pointer:after{content:'';position:absolute;border:12px solid transparent;border-top:0;border-bottom:12px solid #ffffff;top:-8px;left:6px;z-index:112}.mwe-popups.mwe-popups-image-pointer.flipped-x:before{content:'';position:absolute;border:9px solid transparent;border-top:0;border-bottom:9px solid #a2a9b1;top:-9px;left:273px}.mwe-popups.mwe-popups-image-pointer.flipped-x:after{content:'';position:absolute;border:12px solid transparent;border-top:0;border-bottom:12px solid #ffffff;top:-8px;left:269px}.mwe-popups.mwe-popups-image-pointer .mwe-popups-extract{padding-top:16px;margin-top:200px}.mwe-popups.mwe-popups-image-pointer > div > a > svg{margin-top:-8px;position:absolute;z-index:113;left:0}.mwe-popups.flipped-x.mwe-popups-is-tall{min-height:242px}.mwe-popups.flipped-x.mwe-popups-is-tall:before{content:'';position:absolute;border:9px solid transparent;border-top:0;border-bottom:9px solid #a2a9b1;top:-9px;left:420px;z-index:111}.mwe-popups.flipped-x.mwe-popups-is-tall > div > a > svg{margin:0;margin-top:-8px;margin-bottom:-7px;position:absolute;z-index:113;right:0}.mwe-popups.flipped-x-y:before{content:'';position:absolute;border:9px solid transparent;border-bottom:0;border-top:9px solid #a2a9b1;bottom:-9px;left:272px;z-index:111}.mwe-popups.flipped-x-y:after{content:'';position:absolute;border:12px solid transparent;border-bottom:0;border-top:12px solid #ffffff;bottom:-8px;left:269px;z-index:112}.mwe-popups.flipped-x-y.mwe-popups-is-tall{min-height:242px}.mwe-popups.flipped-x-y.mwe-popups-is-tall:before{content:'';position:absolute;border:9px solid transparent;border-bottom:0;border-top:9px solid #a2a9b1;bottom:-9px;left:420px}.mwe-popups.flipped-x-y.mwe-popups-is-tall:after{content:'';position:absolute;border:12px solid transparent;border-bottom:0;border-top:12px solid #ffffff;bottom:-8px;left:417px}.mwe-popups.flipped-x-y.mwe-popups-is-tall > div > a > svg{margin:0;margin-bottom:-9px;position:absolute;z-index:113;right:0}.mwe-popups.flipped-y:before{content:'';position:absolute;border:8px solid transparent;border-bottom:0;border-top:8px solid #a2a9b1;bottom:-8px;left:10px}.mwe-popups.flipped-y:after{content:'';position:absolute;border:11px solid transparent;border-bottom:0;border-top:11px solid #ffffff;bottom:-7px;left:7px}.mwe-popups-is-tall polyline{-webkit-transform:translate(0,0);-moz-transform:translate(0,0);-ms-transform:translate(0,0);transform:translate(0,0)}.mwe-popups-is-tall.flipped-x-y polyline{-webkit-transform:translate(0,-8px);-moz-transform:translate(0,-8px);-ms-transform:translate(0,-8px);transform:translate(0,-8px)}.mwe-popups-is-tall.flipped-x polyline{-webkit-transform:translate(0,8px);-moz-transform:translate(0,8px);-ms-transform:translate(0,8px);transform:translate(0,8px)}.rtl .mwe-popups-is-tall polyline{-webkit-transform:translate(-100%,0);-moz-transform:translate(-100%,0);-ms-transform:translate(-100%,0);transform:translate(-100%,0)}.rtl .mwe-popups-is-tall.flipped-x-y polyline{-webkit-transform:translate(-100%,-8px);-moz-transform:translate(-100%,-8px);-ms-transform:translate(-100%,-8px);transform:translate(-100%,-8px)}.rtl .mwe-popups-is-tall.flipped-x polyline{-webkit-transform:translate(-100%,8px);-moz-transform:translate(-100%,8px);-ms-transform:translate(-100%,8px);transform:translate(-100%,8px)}.mwe-popups-settings-icon{display:block;overflow:hidden;font-size:16px;width:1.5em;height:1.5em;padding:3px;float:right;margin:4px 4px 2px 4px;text-indent:-1em;border-radius:2px}.mwe-popups-settings-icon:hover{background-color:#eaecf0}.mwe-popups-settings-icon:active{background-color:#c8ccd1}.mwe-popups .mwe-popups-title{display:block;font-weight:bold;margin:0 16px}.mwe-popups-overlay{background-color:rgba(255,255,255,0.9);z-index:999;position:fixed;height:100%;width:100%;top:0;bottom:0;left:0;right:0;display:flex;justify-content:center;align-items:center}#mwe-popups-svg{position:absolute;top:-1000px}</style><meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="load.css">
<link rel="stylesheet" href="load_003.css">
<meta name="generator" content="MediaWiki 1.32.0-wmf.15">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-crossorigin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="noindex,nofollow">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/f/fd/Lle_hlle_swissroll.png">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit">
<link rel="edit" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit">
<link rel="apple-touch-icon" href="https://en.wikipedia.org/static/apple-touch/wikipedia.png">
<link rel="shortcut icon" href="https://en.wikipedia.org/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="https://en.wikipedia.org/w/opensearch_desc.php" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://en.wikipedia.org/w/api.php?action=rsd">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="https://en.wikipedia.org/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">
<link rel="dns-prefetch" href="https://login.wikimedia.org/">
<link rel="dns-prefetch" href="https://meta.wikimedia.org/">
<!--[if lt IE 9]><script src="/w/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
<script src="load.php"></script></head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-Nonlinear_dimensionality_reduction rootpage-Nonlinear_dimensionality_reduction skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>
			<div id="siteNotice" class="mw-body-content"><div id="centralNotice"></div><!-- CentralNotice --></div><div class="mw-indicators mw-body-content">
</div>
<h1 id="firstHeading" class="firstHeading" lang="en">Nonlinear dimensionality reduction</h1>			<div id="bodyContent" class="mw-body-content">
				<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>				<div id="contentSub"><div class="mw-revision"><div id="mw-revision-info-current"><table id="revision-info-current" class="plainlinks fmbox fmbox-system" role="presentation"><tbody><tr><td class="mbox-text"><b>This is the <a href="https://en.wikipedia.org/wiki/Help:Page_history" title="Help:Page history">current revision</a> of this page, as edited by <span id="mw-revision-name"><a href="https://en.wikipedia.org/wiki/Special:Contributions/18.90.1.52" class="mw-userlink mw-anonuserlink" title="Special:Contributions/18.90.1.52"><bdi>18.90.1.52</bdi></a> <span class="mw-usertoollinks">(<a href="https://en.wikipedia.org/w/index.php?title=User_talk:18.90.1.52&amp;action=edit&amp;redlink=1" class="new mw-usertoollinks-talk" title="User talk:18.90.1.52 (page does not exist)">talk</a>)</span></span> at <span id="mw-revision-date">17:20, 2 July 2018</span><span id="mw-revision-summary"> <span class="comment">(<a href="#Diffusion_maps">→</a>‎<span dir="auto"><span class="autocomment">Diffusion maps</span></span>)</span></span>. The present address (URL) is a <a href="https://en.wikipedia.org/wiki/Help:Permanent_link" title="Help:Permanent link">permanent link</a> to this version.</b></td></tr></tbody></table><div id="revision-info-current-plain" style="display: none;">Revision as of 17:20, 2 July 2018 by <a href="https://en.wikipedia.org/wiki/Special:Contributions/18.90.1.52" class="mw-userlink mw-anonuserlink" title="Special:Contributions/18.90.1.52"><bdi>18.90.1.52</bdi></a> <span class="mw-usertoollinks">(<a href="https://en.wikipedia.org/w/index.php?title=User_talk:18.90.1.52&amp;action=edit&amp;redlink=1" class="new mw-usertoollinks-talk" title="User talk:18.90.1.52 (page does not exist)">talk</a>)</span> <span class="comment">(<a href="#Diffusion_maps">→</a>‎<span dir="auto"><span class="autocomment">Diffusion maps</span></span>)</span></div>
</div><div id="mw-revision-nav">(<a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;diff=prev&amp;oldid=848551806" title="Nonlinear dimensionality reduction">diff</a>) <a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;direction=prev&amp;oldid=848551806" title="Nonlinear dimensionality reduction">← Previous revision</a>&nbsp;| Latest revision (diff)&nbsp;| Newer revision → (diff)</div></div></div>
				<div id="jump-to-nav"></div>				<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
				<a class="mw-jump-link" href="#p-search">Jump to search</a>
				<div id="mw-content-text" dir="ltr" class="mw-content-ltr" lang="en"><div class="mw-parser-output"><p><a href="https://en.wikipedia.org/wiki/High-dimensional" class="mw-redirect" title="High-dimensional">High-dimensional</a> data, meaning data that requires more than two or three dimensions to represent, can be  <a href="https://en.wikipedia.org/wiki/Curse_of_Dimensionality" class="mw-redirect" title="Curse of Dimensionality">difficult to interpret</a>. One approach to simplification is to assume that the data of interest lie on an <a href="https://en.wikipedia.org/wiki/Embedding" title="Embedding">embedded</a> non-linear <a href="https://en.wikipedia.org/wiki/Manifold" title="Manifold">manifold</a> within the <a href="https://en.wikipedia.org/wiki/Higher-dimensional_space" class="mw-redirect" title="Higher-dimensional space">higher-dimensional space</a>. If the manifold is of low enough dimension, the data can be visualised in the low-dimensional space.
</p>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="https://en.wikipedia.org/wiki/File:Lle_hlle_swissroll.png" class="image"><img alt="" src="300px-Lle_hlle_swissroll.png" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Lle_hlle_swissroll.png/450px-Lle_hlle_swissroll.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fd/Lle_hlle_swissroll.png/600px-Lle_hlle_swissroll.png 2x" data-file-width="906" data-file-height="708" width="300" height="234"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:Lle_hlle_swissroll.png" class="internal" title="Enlarge"></a></div>Top-left: a 3D dataset of 1000 points in a spiraling band (a.k.a. the <a href="https://en.wikipedia.org/wiki/Swiss_roll" title="Swiss roll">Swiss roll</a>)
 with a rectangular hole in the middle. Top-right: the original 2D 
manifold used to generate the 3D dataset. Bottom left and right: 2D 
recoveries of the manifold respectively using the <a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Locally-linear_embedding" title="Nonlinear dimensionality reduction">LLE</a> and <a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction#Hessian_Locally-Linear_Embedding_%28Hessian_LLE%29" title="Nonlinear dimensionality reduction">Hessian LLE</a> algorithms as implemented by the Modular Data Processing toolkit.</div></div></div>
<p>Below is a summary of some of the important algorithms from the history of <b>manifold learning</b> and <b>nonlinear dimensionality reduction</b> (NLDR).<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">[1]</a></sup><sup id="cite_ref-2" class="reference"><a href="#cite_note-2">[2]</a></sup> Many of these non-linear <a href="https://en.wikipedia.org/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a> methods are related to the <a href="#Related_Linear_Decomposition_Methods">linear methods listed below</a>.
 Non-linear methods can be broadly classified into two groups: those 
that provide a mapping (either from the high-dimensional space to the 
low-dimensional embedding or vice versa), and those that just give a 
visualisation.  In the context of <a href="https://en.wikipedia.org/wiki/Machine_learning" title="Machine learning">machine learning</a>, mapping methods may be viewed as a preliminary <a href="https://en.wikipedia.org/wiki/Feature_extraction" title="Feature extraction">feature extraction</a> step, after which <a href="https://en.wikipedia.org/wiki/Pattern_recognition#Algorithms" title="Pattern recognition">pattern recognition algorithms</a> are applied. Typically those that just give a visualisation are based on proximity data – that is, <a href="https://en.wikipedia.org/wiki/Distance" title="Distance">distance</a> measurements.
</p>
<div id="toc" class="toc"><input role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" type="checkbox"><div class="toctitle" dir="ltr" lang="en"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Related_Linear_Decomposition_Methods"><span class="tocnumber">1</span> <span class="toctext">Related Linear Decomposition Methods</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Applications_of_NLDR"><span class="tocnumber">2</span> <span class="toctext">Applications of NLDR</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Manifold_learning_algorithms"><span class="tocnumber">3</span> <span class="toctext">Manifold learning algorithms</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="#Sammon%27s_mapping"><span class="tocnumber">3.1</span> <span class="toctext">Sammon's mapping</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Self-organizing_map"><span class="tocnumber">3.2</span> <span class="toctext">Self-organizing map</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#Principal_curves_and_manifolds"><span class="tocnumber">3.3</span> <span class="toctext">Principal curves and manifolds</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Autoencoders"><span class="tocnumber">3.4</span> <span class="toctext">Autoencoders</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Gaussian_process_latent_variable_models"><span class="tocnumber">3.5</span> <span class="toctext">Gaussian process latent variable models</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Curvilinear_component_analysis"><span class="tocnumber">3.6</span> <span class="toctext">Curvilinear component analysis</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#Curvilinear_distance_analysis"><span class="tocnumber">3.7</span> <span class="toctext">Curvilinear distance analysis</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Diffeomorphic_dimensionality_reduction"><span class="tocnumber">3.8</span> <span class="toctext">Diffeomorphic dimensionality reduction</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Kernel_principal_component_analysis"><span class="tocnumber">3.9</span> <span class="toctext">Kernel principal component analysis</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="#Isomap"><span class="tocnumber">3.10</span> <span class="toctext">Isomap</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#Contagion_maps"><span class="tocnumber">3.11</span> <span class="toctext">Contagion maps</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Locally-linear_embedding"><span class="tocnumber">3.12</span> <span class="toctext">Locally-linear embedding</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Laplacian_eigenmaps"><span class="tocnumber">3.13</span> <span class="toctext">Laplacian eigenmaps</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="#Manifold_alignment"><span class="tocnumber">3.14</span> <span class="toctext">Manifold alignment</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#Diffusion_maps"><span class="tocnumber">3.15</span> <span class="toctext">Diffusion maps</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="#Hessian_Locally-Linear_Embedding_%28Hessian_LLE%29"><span class="tocnumber">3.16</span> <span class="toctext">Hessian Locally-Linear Embedding (Hessian LLE)</span></a></li>
<li class="toclevel-2 tocsection-20"><a href="#Modified_Locally-Linear_Embedding_%28MLLE%29"><span class="tocnumber">3.17</span> <span class="toctext">Modified Locally-Linear Embedding (MLLE)</span></a></li>
<li class="toclevel-2 tocsection-21"><a href="#Relational_perspective_map"><span class="tocnumber">3.18</span> <span class="toctext">Relational perspective map</span></a></li>
<li class="toclevel-2 tocsection-22"><a href="#Local_tangent_space_alignment"><span class="tocnumber">3.19</span> <span class="toctext">Local tangent space alignment</span></a></li>
<li class="toclevel-2 tocsection-23"><a href="#Local_multidimensional_scaling"><span class="tocnumber">3.20</span> <span class="toctext">Local multidimensional scaling</span></a></li>
<li class="toclevel-2 tocsection-24"><a href="#Maximum_variance_unfolding"><span class="tocnumber">3.21</span> <span class="toctext">Maximum variance unfolding</span></a></li>
<li class="toclevel-2 tocsection-25"><a href="#Nonlinear_PCA"><span class="tocnumber">3.22</span> <span class="toctext">Nonlinear PCA</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="#Data-driven_high-dimensional_scaling"><span class="tocnumber">3.23</span> <span class="toctext">Data-driven high-dimensional scaling</span></a></li>
<li class="toclevel-2 tocsection-27"><a href="#Manifold_sculpting"><span class="tocnumber">3.24</span> <span class="toctext">Manifold sculpting</span></a></li>
<li class="toclevel-2 tocsection-28"><a href="#t-distributed_stochastic_neighbor_embedding"><span class="tocnumber">3.25</span> <span class="toctext">t-distributed stochastic neighbor embedding</span></a></li>
<li class="toclevel-2 tocsection-29"><a href="#RankVisu"><span class="tocnumber">3.26</span> <span class="toctext">RankVisu</span></a></li>
<li class="toclevel-2 tocsection-30"><a href="#Topologically_constrained_isometric_embedding"><span class="tocnumber">3.27</span> <span class="toctext">Topologically constrained isometric embedding</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-31"><a href="#Methods_based_on_proximity_matrices"><span class="tocnumber">4</span> <span class="toctext">Methods based on proximity matrices</span></a></li>
<li class="toclevel-1 tocsection-32"><a href="#See_also"><span class="tocnumber">5</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-33"><a href="#References"><span class="tocnumber">6</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-34"><a href="#External_links"><span class="tocnumber">7</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Related_Linear_Decomposition_Methods">Related Linear Decomposition Methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=1" title="Edit section: Related Linear Decomposition Methods">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Independent_component_analysis" title="Independent component analysis">Independent component analysis</a> (ICA).</li>
<li><a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a> (PCA) (also called <a href="https://en.wikipedia.org/wiki/Karhunen%E2%80%93Lo%C3%A8ve_transform" class="mw-redirect" title="Karhunen–Loève transform">Karhunen–Loève transform</a> — KLT).</li>
<li><a href="https://en.wikipedia.org/wiki/Singular_value_decomposition" class="mw-redirect" title="Singular value decomposition">Singular value decomposition</a> (SVD).</li>
<li><a href="https://en.wikipedia.org/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a>.</li></ul>
<h2><span class="mw-headline" id="Applications_of_NLDR">Applications of NLDR</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=2" title="Edit section: Applications of NLDR">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Consider a dataset represented as a matrix (or a database table), 
such that each row represents a set of attributes (or features or 
dimensions) that describe a particular instance of something. If the 
number of attributes is large, then the space of unique possible rows is
 exponentially large. Thus, the larger the dimensionality, the more 
difficult it becomes to sample the space. This causes many problems. 
Algorithms that operate on high-dimensional data tend to have a very 
high time complexity. Many machine learning algorithms, for example, 
struggle with high-dimensional data. This has become known as the <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a>.
 Reducing data into fewer dimensions often makes analysis algorithms 
more efficient, and can help machine learning algorithms make more 
accurate predictions.
</p><p>Humans often have difficulty comprehending data in many 
dimensions. Thus, reducing data to a small number of dimensions is 
useful for visualization purposes.
</p>
<div class="thumb tright"><div class="thumbinner" style="width:502px;"><a href="https://en.wikipedia.org/wiki/File:Nldr.jpg" class="image"><img alt="" src="500px-Nldr.jpg" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/en/9/9c/Nldr.jpg 1.5x" data-file-width="584" data-file-height="265" width="500" height="227"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:Nldr.jpg" class="internal" title="Enlarge"></a></div>Plot
 of the two-dimensional points that results from using a NLDR algorithm.
 In this case, Manifold Sculpting used to reduce the data into just two 
dimensions (rotation and scale).</div></div></div>
<p>The reduced-dimensional representations of data are often referred to
 as "intrinsic variables". This description implies that these are the 
values from which the data was produced. For example, consider a dataset
 that contains images of a letter 'A', which has been scaled and rotated
 by varying amounts. Each image has 32x32 pixels. Each image can be 
represented as a vector of 1024 pixel values. Each row is a sample on a 
two-dimensional manifold in 1024-dimensional space (a <a href="https://en.wikipedia.org/wiki/Hamming_space" title="Hamming space">Hamming space</a>).
 The intrinsic dimensionality is two, because two variables (rotation 
and scale) were varied in order to produce the data. Information about 
the shape or look of a letter 'A' is not part of the intrinsic variables
 because it is the same in every instance. Nonlinear dimensionality 
reduction will discard the correlated information (the letter 'A') and 
recover only the varying information (rotation and scale). The image to 
the right shows sample images from this dataset (to save space, not all 
input images are shown), and a plot of the two-dimensional points that 
results from using a NLDR algorithm (in this case, Manifold Sculpting 
was used) to reduce the data into just two dimensions.
</p>
<div class="thumb tright"><div class="thumbinner" style="width:234px;"><a href="https://en.wikipedia.org/wiki/File:Letters_pca.png" class="image"><img alt="" src="Letters_pca.png" class="thumbimage" data-file-width="232" data-file-height="232" width="232" height="232"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:Letters_pca.png" class="internal" title="Enlarge"></a></div>PCA
 (a linear dimensionality reduction algorithm) is used to reduce this 
same dataset into two dimensions, the resulting values are not so well 
organized.</div></div></div>
<p>By comparison, if <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a>,
 which is a linear dimensionality reduction algorithm, is used to reduce
 this same dataset into two dimensions, the resulting values are not so 
well organized. This demonstrates that the high-dimensional vectors 
(each representing a letter 'A') that sample this manifold vary in a 
non-linear manner.
</p><p>It should be apparent, therefore, that NLDR has several 
applications in the field of computer-vision. For example, consider a 
robot that uses a camera to navigate in a closed static environment. The
 images obtained by that camera can be considered to be samples on a 
manifold in high-dimensional space, and the intrinsic variables of that 
manifold will represent the robot's position and orientation. This 
utility is not limited to robots. <a href="https://en.wikipedia.org/wiki/Dynamical_systems" class="mw-redirect" title="Dynamical systems">Dynamical systems</a>,
 a more general class of systems, which includes robots, are defined in 
terms of a manifold. Active research in NLDR seeks to unfold the 
observation manifolds associated with dynamical systems to develop 
techniques for modeling such systems and enable them to operate 
autonomously.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">[3]</a></sup>
</p>
<h2><span class="mw-headline" id="Manifold_learning_algorithms">Manifold learning algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=3" title="Edit section: Manifold learning algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Some of the more prominent manifold learning algorithms are listed 
below (in approximately chronological order). An algorithm may learn an <i>internal model</i>
 of the data, which can be used to map points unavailable at training 
time into the embedding in a process often called out-of-sample 
extension.
</p>
<h3><span id="Sammon.27s_mapping"></span><span class="mw-headline" id="Sammon's_mapping">Sammon's mapping</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=4" title="Edit section: Sammon's mapping">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/Sammon%27s_mapping" class="mw-redirect" title="Sammon's mapping">Sammon's mapping</a> is one of the first and most popular NLDR techniques.
</p>
<div class="thumb tleft"><div class="thumbinner" style="width:202px;"><a href="https://en.wikipedia.org/wiki/File:SOMsPCA.PNG" class="image"><img alt="" src="200px-SOMsPCA.PNG" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/bb/SOMsPCA.PNG/300px-SOMsPCA.PNG 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/bb/SOMsPCA.PNG/400px-SOMsPCA.PNG 2x" data-file-width="607" data-file-height="400" width="200" height="132"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:SOMsPCA.PNG" class="internal" title="Enlarge"></a></div>Approximation of a principal curve by one-dimensional <a href="https://en.wikipedia.org/wiki/Self-organizing_map" title="Self-organizing map">SOM</a> (a <a href="https://en.wikipedia.org/wiki/Broken_line" class="mw-redirect" title="Broken line">broken line</a> with red squares, 20 nodes). The first <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">principal component</a> is presented by a blue straight line. Data points are the small grey circles. For PCA, the <a href="https://en.wikipedia.org/wiki/Fraction_of_variance_unexplained" title="Fraction of variance unexplained">Fraction of variance unexplained</a> in this example is 23.23%, for SOM it is 6.86%.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">[4]</a></sup></div></div></div>
<h3><span class="mw-headline" id="Self-organizing_map">Self-organizing map</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=5" title="Edit section: Self-organizing map">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <a href="https://en.wikipedia.org/wiki/Self-organizing_map" title="Self-organizing map">self-organizing map</a> (SOM, also called <i>Kohonen map</i>) and its probabilistic variant <a href="https://en.wikipedia.org/wiki/Generative_topographic_mapping" class="mw-redirect" title="Generative topographic mapping">generative topographic mapping</a> (GTM) use a point representation in the embedded space to form a <a href="https://en.wikipedia.org/wiki/Latent_variable_model" title="Latent variable model">latent variable model</a> based on a non-linear mapping from the embedded space to the high-dimensional space.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">[5]</a></sup> These techniques are related to work on <a href="https://en.wikipedia.org/w/index.php?title=Density_networks&amp;action=edit&amp;redlink=1" class="new" title="Density networks (page does not exist)">density networks</a>, which also are based around the same probabilistic model.
</p>
<h3><span class="mw-headline" id="Principal_curves_and_manifolds">Principal curves and manifolds</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=6" title="Edit section: Principal curves and manifolds">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="https://en.wikipedia.org/wiki/File:SlideQualityLife.png" class="image"><img alt="" src="300px-SlideQualityLife.png" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/48/SlideQualityLife.png/450px-SlideQualityLife.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/48/SlideQualityLife.png/600px-SlideQualityLife.png 2x" data-file-width="1164" data-file-height="892" width="300" height="230"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:SlideQualityLife.png" class="internal" title="Enlarge"></a></div>Application of principal curves: Nonlinear quality of life index.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">[6]</a></sup> Points represent data of the <a href="https://en.wikipedia.org/wiki/United_Nations" title="United Nations">UN</a> 171 countries in 4-dimensional space formed by the values of 4 indicators: <a href="https://en.wikipedia.org/wiki/Gross_domestic_product" title="Gross domestic product">gross product per capita</a>, <a href="https://en.wikipedia.org/wiki/Life_expectancy" title="Life expectancy">life expectancy</a>, <a href="https://en.wikipedia.org/wiki/Infant_mortality" title="Infant mortality">infant mortality</a>, <a href="https://en.wikipedia.org/wiki/Tuberculosis" title="Tuberculosis">tuberculosis</a> incidence. Different forms and colors correspond to various geographical locations. Red bold line represents the <b>principal curve</b>, approximating the dataset. This principal curve was produced by the method of <a href="https://en.wikipedia.org/wiki/Elastic_map" title="Elastic map">elastic map</a>. Software is available for free non-commercial use.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">[7]</a></sup><sup id="cite_ref-8" class="reference"><a href="#cite_note-8">[8]</a></sup></div></div></div>
<p><b><a href="https://en.wikipedia.org/w/index.php?title=Principal_curve&amp;action=edit&amp;redlink=1" class="new" title="Principal curve (page does not exist)">Principal curves</a> and manifolds</b>
 give the natural geometric framework for nonlinear dimensionality 
reduction and extend the geometric interpretation of PCA by explicitly 
constructing an embedded manifold, and by encoding using standard 
geometric projection onto the manifold. This approach was proposed by <a href="https://en.wikipedia.org/wiki/Trevor_Hastie" title="Trevor Hastie">Trevor Hastie</a> in his thesis (1984)<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">[9]</a></sup> and developed further by many authors.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">[10]</a></sup>
How to define the "simplicity" of the manifold is problem-dependent, 
however, it is commonly measured by the intrinsic dimensionality and/or 
the smoothness of the manifold. Usually, the principal manifold is 
defined as a solution to an optimization problem. The objective function
 includes a quality of data approximation and some penalty terms for the
 bending of the manifold. The popular initial approximations are 
generated by linear PCA, Kohonen's SOM or autoencoders. The <a href="https://en.wikipedia.org/wiki/Elastic_map" title="Elastic map">elastic map</a> method provides the <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">expectation-maximization algorithm</a> for principal <a href="https://en.wikipedia.org/wiki/Manifold_learning" class="mw-redirect" title="Manifold learning">manifold learning</a> with minimization of quadratic energy functional at the "maximization" step.
</p>
<h3><span class="mw-headline" id="Autoencoders">Autoencoders</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=7" title="Edit section: Autoencoders">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>An <a href="https://en.wikipedia.org/wiki/Autoencoder" title="Autoencoder">autoencoder</a> is a feed-forward <a href="https://en.wikipedia.org/wiki/Neural_network" title="Neural network">neural network</a>
 which is trained to approximate the identity function. That is, it is 
trained to map from a vector of values to the same vector. When used for
 dimensionality reduction purposes, one of the hidden layers in the 
network is limited to contain only a small number of network units. 
Thus, the network must learn to encode the vector into a small number of
 dimensions and then decode it back into the original space. Thus, the 
first half of the network is a model which maps from high to 
low-dimensional space, and the second half maps from low to 
high-dimensional space. Although the idea of autoencoders is quite old, 
training of deep autoencoders has only recently become possible through 
the use of <a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machines</a> and stacked denoising autoencoders. Related to autoencoders is the <a href="https://en.wikipedia.org/w/index.php?title=NeuroScale&amp;action=edit&amp;redlink=1" class="new" title="NeuroScale (page does not exist)">NeuroScale</a> algorithm, which uses stress functions inspired by <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a> and <a href="https://en.wikipedia.org/wiki/Sammon_mapping" title="Sammon mapping">Sammon mappings</a>
 (see below) to learn a non-linear mapping from the high-dimensional to 
the embedded space. The mappings in NeuroScale are based on <a href="https://en.wikipedia.org/wiki/Radial_basis_function_network" title="Radial basis function network">radial basis function networks</a>.
</p>
<h3><span class="mw-headline" id="Gaussian_process_latent_variable_models">Gaussian process latent variable models</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=8" title="Edit section: Gaussian process latent variable models">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/w/index.php?title=Gaussian_process_latent_variable_model&amp;action=edit&amp;redlink=1" class="new" title="Gaussian process latent variable model (page does not exist)">Gaussian process latent variable models</a> (GPLVM)<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">[11]</a></sup>
 are probabilistic dimensionality reduction methods that use Gaussian 
Processes (GPs) to find a lower dimensional non-linear embedding of high
 dimensional data. They are an extension of the Probabilistic 
formulation of PCA. The model is defined probabilistically and the 
latent variables are then marginalized and parameters are obtained by 
maximizing the likelihood. Like kernel PCA they use a kernel function to
 form a non linear mapping (in the form of a <a href="https://en.wikipedia.org/wiki/Gaussian_process" title="Gaussian process">Gaussian process</a>).
 However, in the GPLVM the mapping is from the embedded(latent) space to
 the data space (like density networks and GTM) whereas in kernel PCA it
 is in the opposite direction. It was originally proposed for 
visualization of high dimensional data but has been extended to 
construct a shared manifold model between two observation spaces.
GPLVM and its many variants have been proposed specially for human 
motion modeling, e.g., back constrained GPLVM, GP dynamic model (GPDM), 
balanced GPDM (B-GPDM) and topologically constrained GPDM. To capture 
the coupling effect of the pose and gait manifolds in the gait analysis,
 a multi-layer joint gait-pose manifolds was proposed.<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">[12]</a></sup>
</p>
<h3><span class="mw-headline" id="Curvilinear_component_analysis">Curvilinear component analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=9" title="Edit section: Curvilinear component analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/w/index.php?title=Curvilinear_component_analysis&amp;action=edit&amp;redlink=1" class="new" title="Curvilinear component analysis (page does not exist)">Curvilinear component analysis</a> (CCA)<sup id="cite_ref-Demart_13-0" class="reference"><a href="#cite_note-Demart-13">[13]</a></sup>
 looks for the configuration of points in the output space that 
preserves original distances as much as possible while focusing on small
 distances in the output space (conversely to <a href="https://en.wikipedia.org/wiki/Sammon%27s_mapping" class="mw-redirect" title="Sammon's mapping">Sammon's mapping</a> which focus on small distances in original space).
</p><p>It should be noticed that CCA, as an iterative learning 
algorithm, actually starts with focus on large distances (like the 
Sammon algorithm), then gradually change focus to small distances. The 
small distance information will overwrite the large distance 
information, if compromises between the two have to be made.
</p><p>The stress function of CCA is related to a sum of right Bregman divergences<sup id="cite_ref-Jigang_14-0" class="reference"><a href="#cite_note-Jigang-14">[14]</a></sup>
</p>
<h3><span class="mw-headline" id="Curvilinear_distance_analysis">Curvilinear distance analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=10" title="Edit section: Curvilinear distance analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>CDA<sup id="cite_ref-Demart_13-1" class="reference"><a href="#cite_note-Demart-13">[13]</a></sup> trains a self-organizing neural network to fit the manifold and seeks to preserve <a href="https://en.wikipedia.org/wiki/Geodesic_distance" class="mw-redirect" title="Geodesic distance">geodesic distances</a>
 in its embedding. It is based on Curvilinear Component Analysis (which 
extended Sammon's mapping), but uses geodesic distances instead.
</p>
<h3><span class="mw-headline" id="Diffeomorphic_dimensionality_reduction">Diffeomorphic dimensionality reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=11" title="Edit section: Diffeomorphic dimensionality reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Diffeomorphic Dimensionality Reduction or <i>Diffeomap</i><sup id="cite_ref-15" class="reference"><a href="#cite_note-15">[15]</a></sup>
 learns a smooth diffeomorphic mapping which transports the data onto a 
lower-dimensional linear subspace. The methods solves for a smooth time 
indexed vector field such that flows along the field which start at the 
data points will end at a lower-dimensional linear subspace, thereby 
attempting to preserve pairwise differences under both the forward and 
inverse mapping.
</p>
<h3><span class="mw-headline" id="Kernel_principal_component_analysis">Kernel principal component analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=12" title="Edit section: Kernel principal component analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Perhaps the most widely used algorithm for manifold learning is <a href="https://en.wikipedia.org/wiki/Kernel_principal_component_analysis" title="Kernel principal component analysis">kernel PCA</a>.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">[16]</a></sup> It is a combination of <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">Principal component analysis</a> and the <a href="https://en.wikipedia.org/wiki/Kernel_trick" class="mw-redirect" title="Kernel trick">kernel trick</a>. PCA begins by computing the covariance matrix of the <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle m\times n}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>m</mi>
        <mo>×<!-- × --></mo>
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle m\times n}</annotation>
  </semantics>
</math></span><img src="12b23d207d23dd430b93320539abbb0bde84870d.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:6.276ex; height:1.676ex;" alt="m\times n"></span> matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {X} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">X</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {X} }</annotation>
  </semantics>
</math></span><img src="9f75966a2f9d5672136fa9401ee1e75008f95ffd.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.019ex; height:2.176ex;" alt="\mathbf {X} "></span>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle C={\frac {1}{m}}\sum _{i=1}^{m}{\mathbf {x} _{i}\mathbf {x} _{i}^{\mathsf {T}}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>C</mi>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mi>m</mi>
          </mfrac>
        </mrow>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <msubsup>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="sans-serif">T</mi>
              </mrow>
            </mrow>
          </msubsup>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C={\frac {1}{m}}\sum _{i=1}^{m}{\mathbf {x} _{i}\mathbf {x} _{i}^{\mathsf {T}}}.}</annotation>
  </semantics>
</math></span><img src="3606df7c7cb77f57490c8ca9c9380b34cbd9a8f5.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:17.49ex; height:6.843ex;" alt="C={\frac {1}{m}}\sum _{i=1}^{m}{\mathbf {x} _{i}\mathbf {x} _{i}^{\mathsf {T}}}."></span></dd></dl>
<p>It then projects the data onto the first <i>k</i> eigenvectors of 
that matrix. By comparison, KPCA begins by computing the covariance 
matrix of the data after being transformed into a higher-dimensional 
space,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle C={\frac {1}{m}}\sum _{i=1}^{m}{\Phi (\mathbf {x} _{i})\Phi (\mathbf {x} _{i})^{\mathsf {T}}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>C</mi>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mi>m</mi>
          </mfrac>
        </mrow>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>m</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">Φ<!-- Φ --></mi>
          <mo stretchy="false">(</mo>
          <msub>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mo stretchy="false">)</mo>
          <mi mathvariant="normal">Φ<!-- Φ --></mi>
          <mo stretchy="false">(</mo>
          <msub>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <msup>
            <mo stretchy="false">)</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="sans-serif">T</mi>
              </mrow>
            </mrow>
          </msup>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C={\frac {1}{m}}\sum _{i=1}^{m}{\Phi (\mathbf {x} _{i})\Phi (\mathbf {x} _{i})^{\mathsf {T}}}.}</annotation>
  </semantics>
</math></span><img src="ce442431fef9f2e1f868a550f94d1425da4503c9.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:25.265ex; height:6.843ex;" alt="C={\frac {1}{m}}\sum _{i=1}^{m}{\Phi (\mathbf {x} _{i})\Phi (\mathbf {x} _{i})^{\mathsf {T}}}."></span></dd></dl>
<p>It then projects the transformed data onto the first <i>k</i> 
eigenvectors of that matrix, just like PCA. It uses the kernel trick to 
factor away much of the computation, such that the entire process can be
 performed without actually computing <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \Phi (\mathbf {x} )}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">Φ<!-- Φ --></mi>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">x</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Phi (\mathbf {x} )}</annotation>
  </semantics>
</math></span><img src="24456514d4d4c7d8e48a78f19906cd67c3655f63.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:4.898ex; height:2.843ex;" alt="\Phi (\mathbf {x} )"></span>. Of course <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \Phi }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">Φ<!-- Φ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Phi }</annotation>
  </semantics>
</math></span><img src="aed80a2011a3912b028ba32a52dfa57165455f24.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.678ex; height:2.176ex;" alt="\Phi "></span>
 must be chosen such that it has a known corresponding kernel. 
Unfortunately, it is not trivial to find a good kernel for a given 
problem, so KPCA does not yield good results with some problems when 
using standard kernels. For example, it is known to perform poorly with 
these kernels on the <a href="https://en.wikipedia.org/wiki/Swiss_roll" title="Swiss roll">Swiss roll</a>
 manifold. However, one can view certain other methods that perform well
 in such settings (e.g., Laplacian Eigenmaps, LLE) as special cases of 
kernel PCA by constructing a data-dependent kernel matrix.<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">[17]</a></sup>
</p><p>KPCA has an internal model, so it can be used to map points onto its embedding that were not available at training time.
</p>
<h3><span class="mw-headline" id="Isomap">Isomap</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=13" title="Edit section: Isomap">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/Isomap" title="Isomap">Isomap</a><sup id="cite_ref-18" class="reference"><a href="#cite_note-18">[18]</a></sup> is a combination of the <a href="https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm" title="Floyd–Warshall algorithm">Floyd–Warshall algorithm</a> with classic <a href="https://en.wikipedia.org/wiki/Multidimensional_Scaling" class="mw-redirect" title="Multidimensional Scaling">Multidimensional Scaling</a>.
 Classic Multidimensional Scaling (MDS) takes a matrix of pair-wise 
distances between all points, and computes a position for each point.  
Isomap assumes that the pair-wise distances are only known between 
neighboring points, and uses the Floyd–Warshall algorithm to compute the
 pair-wise distances between all other points. This effectively 
estimates the full matrix of pair-wise <a href="https://en.wikipedia.org/wiki/Geodesic_distance" class="mw-redirect" title="Geodesic distance">geodesic distances</a> between all of the points. Isomap then uses classic MDS to compute the reduced-dimensional positions of all the points.
</p><p>Landmark-Isomap is a variant of this algorithm that uses landmarks to increase speed, at the cost of some accuracy.
</p>
<h3><span class="mw-headline" id="Contagion_maps">Contagion maps</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=14" title="Edit section: Contagion maps">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Contagion maps use multiple contagions on a network to map the nodes as a point cloud.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">[19]</a></sup> In the case of the <a href="https://en.wikipedia.org/wiki/Global_cascades_model" title="Global cascades model">Global cascades model</a> the speed of the spread can be adjusted with the threshold parameter <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle t\in [0,1]}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>t</mi>
        <mo>∈<!-- ∈ --></mo>
        <mo stretchy="false">[</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mn>1</mn>
        <mo stretchy="false">]</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle t\in [0,1]}</annotation>
  </semantics>
</math></span><img src="31a5c18739ff04858eecc8fec2f53912c348e0e5.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.333ex; height:2.843ex;" alt="{\displaystyle t\in [0,1]}"></span>. For <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle t=0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>t</mi>
        <mo>=</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle t=0}</annotation>
  </semantics>
</math></span><img src="43469ec032d858feae5aa87029e22eaaf0109e9c.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.101ex; height:2.176ex;" alt=" t=0 "></span> the contagion map is equivalent to the <a href="https://en.wikipedia.org/wiki/Isomap" title="Isomap">Isomap</a> algorithm.
</p>
<h3><span class="mw-headline" id="Locally-linear_embedding">Locally-linear embedding</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=15" title="Edit section: Locally-linear embedding">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/w/index.php?title=Locally-Linear_Embedding&amp;action=edit&amp;redlink=1" class="new" title="Locally-Linear Embedding (page does not exist)">Locally-Linear Embedding</a> (LLE)<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">[20]</a></sup>
 was presented at approximately the same time as Isomap. It has several 
advantages over Isomap, including faster optimization when implemented 
to take advantage of <a href="https://en.wikipedia.org/wiki/Sparse_matrix" title="Sparse matrix">sparse matrix</a>
 algorithms, and better results with many problems. LLE also begins by 
finding a set of the nearest neighbors of each point. It then computes a
 set of weights for each point that best describes the point as a linear
 combination of its neighbors. Finally, it uses an eigenvector-based 
optimization technique to find the low-dimensional embedding of points, 
such that each point is still described with the same linear combination
 of its neighbors. LLE tends to handle non-uniform sample densities 
poorly because there is no fixed unit to prevent the weights from 
drifting as various regions differ in sample densities. LLE has no 
internal model.
</p><p>LLE computes the barycentric coordinates of a point <i>X</i><sub><i>i</i></sub> based on its neighbors <i>X</i><sub><i>j</i></sub>. The original point is reconstructed by a linear combination, given by the weight matrix <i>W</i><sub><i>ij</i></sub>, of its neighbors. The reconstruction error is given by the cost function <i>E</i>(<i>W</i>).
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle E(W)=\sum _{i}|{\mathbf {X} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {X} _{j}}|}^{\mathsf {2}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>E</mi>
        <mo stretchy="false">(</mo>
        <mi>W</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold">X</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo>−<!-- − --></mo>
            <munder>
              <mo>∑<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
              </mrow>
            </munder>
            <mrow class="MJX-TeXAtom-ORD">
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">X</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mo stretchy="false">|</mo>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mn mathvariant="sans-serif">2</mn>
            </mrow>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle E(W)=\sum _{i}|{\mathbf {X} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {X} _{j}}|}^{\mathsf {2}}}</annotation>
  </semantics>
</math></span><img src="fc0bdb0b95b0a2d60d56e57add4d00cb1ed1da27.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:31.779ex; height:6.343ex;" alt="E(W)=\sum _{i}|{\mathbf {X} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {X} _{j}}|}^{\mathsf {2}}"></span></dd></dl>
<p>The weights <i>W</i><sub><i>ij</i></sub> refer to the amount of contribution the point <i>X</i><sub><i>j</i></sub> has while reconstructing the point <i>X</i><sub><i>i</i></sub>. The cost function is minimized under two constraints:
(a) Each data point <i>X</i><sub><i>i</i></sub> is reconstructed only from its neighbors, thus enforcing <i>W</i><sub><i>ij</i></sub> to be zero if point <i>X</i><sub><i>j</i></sub> is not a neighbor of the point <i>X</i><sub><i>i</i></sub> and 
(b) The sum of every row of the weight matrix equals 1.
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \sum _{j}{\mathbf {W} _{ij}}=1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">W</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
              <mi>j</mi>
            </mrow>
          </msub>
        </mrow>
        <mo>=</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{j}{\mathbf {W} _{ij}}=1}</annotation>
  </semantics>
</math></span><img src="281e3fb718c89cd647f0e97b944b2256aa03c9a2.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:12.243ex; height:5.843ex;" alt="\sum _{j}{\mathbf {W} _{ij}}=1"></span></dd></dl>
<p>The original data points are collected in a <i>D</i> dimensional space and the goal of the algorithm is to reduce the dimensionality to <i>d</i> such that <i>D</i> &gt;&gt; <i>d</i>. The same weights <i>W</i><sub><i>ij</i></sub> that reconstructs the <i>i</i>th data point in the <i>D</i> dimensional space will be used to reconstruct the same point in the lower <i>d</i> dimensional space. A neighborhood preserving map is created based on this idea. Each point X<sub>i</sub> in the <i>D</i> dimensional space is mapped onto a point Y<sub>i</sub> in the <i>d</i> dimensional space by minimizing the cost function
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle C(Y)=\sum _{i}|{\mathbf {Y} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {Y} _{j}}|}^{\mathsf {2}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>C</mi>
        <mo stretchy="false">(</mo>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold">Y</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo>−<!-- − --></mo>
            <munder>
              <mo>∑<!-- ∑ --></mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
              </mrow>
            </munder>
            <mrow class="MJX-TeXAtom-ORD">
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">W</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                  <mi>j</mi>
                </mrow>
              </msub>
              <msub>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="bold">Y</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mo stretchy="false">|</mo>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mn mathvariant="sans-serif">2</mn>
            </mrow>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C(Y)=\sum _{i}|{\mathbf {Y} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {Y} _{j}}|}^{\mathsf {2}}}</annotation>
  </semantics>
</math></span><img src="07e4d9614a270cf1d338095999935621139f8854.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:31.108ex; height:6.343ex;" alt="C(Y)=\sum _{i}|{\mathbf {Y} _{i}-\sum _{j}{\mathbf {W} _{ij}\mathbf {Y} _{j}}|}^{\mathsf {2}}"></span></dd></dl>
<p>In this cost function, unlike the previous one, the weights W<sub>ij</sub> are kept fixed and the minimization is done on the points Y<sub>i</sub> to optimize the coordinates. This minimization problem can be solved by solving a sparse <i>N</i> X <i>N</i> <a href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix" title="Eigendecomposition of a matrix">eigen value problem</a> (<i>N</i> being the number of data points), whose bottom <i>d</i> nonzero eigen vectors provide an orthogonal set of coordinates. Generally the data points are reconstructed from <i>K</i> nearest neighbors, as measured by <a href="https://en.wikipedia.org/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a>. For such an implementation the algorithm has only one free parameter <i>K,</i> which can be chosen by cross validation.
</p>
<h3><span class="mw-headline" id="Laplacian_eigenmaps">Laplacian eigenmaps</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=16" title="Edit section: Laplacian eigenmaps">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="https://en.wikipedia.org/wiki/Manifold_regularization" title="Manifold regularization">Manifold regularization</a></div>
<p>Laplacian Eigenmaps<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">[21]</a></sup>
 uses spectral techniques to perform dimensionality reduction. This 
technique relies on the basic assumption that the data lies in a 
low-dimensional manifold in a high-dimensional space.<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">[22]</a></sup>  This algorithm cannot embed out of sample points, but techniques based on <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space" title="Reproducing kernel Hilbert space">Reproducing kernel Hilbert space</a> regularization exist for adding this capability.<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">[23]</a></sup>  Such techniques can be applied to other nonlinear dimensionality reduction algorithms as well.
</p><p>Traditional techniques like principal component analysis do not 
consider the intrinsic geometry of the data. Laplacian eigenmaps builds a
 graph from neighborhood information of the data set. Each data point 
serves as a node on the graph and connectivity between nodes is governed
 by the proximity of neighboring points (using e.g. the <a href="https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm" class="mw-redirect" title="K-nearest neighbor algorithm">k-nearest neighbor algorithm</a>).
 The graph thus generated can be considered as a discrete approximation 
of the low-dimensional manifold in the high-dimensional space. 
Minimization of a cost function based on the graph ensures that points 
close to each other on the manifold are mapped close to each other in 
the low-dimensional space, preserving local distances. The 
eigenfunctions of the <a href="https://en.wikipedia.org/wiki/Laplace%E2%80%93Beltrami_operator" title="Laplace–Beltrami operator">Laplace–Beltrami operator</a>
 on the manifold serve as the embedding dimensions, since under mild 
conditions this operator has a countable spectrum that is a basis for 
square integrable functions on the manifold (compare to <a href="https://en.wikipedia.org/wiki/Fourier_series" title="Fourier series">Fourier series</a>
 on the unit circle manifold).  Attempts to place Laplacian eigenmaps on
 solid theoretical ground have met with some success, as under certain 
nonrestrictive assumptions, the graph Laplacian matrix has been shown to
 converge to the Laplace–Beltrami operator as the number of points goes 
to infinity.<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">[24]</a></sup> Matlab code for Laplacian Eigenmaps can be found in algorithms<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">[25]</a></sup> and the PhD thesis of Belkin can be found at the <a href="https://en.wikipedia.org/wiki/Ohio_State_University" title="Ohio State University">Ohio State University</a>.<sup id="cite_ref-26" class="reference"><a href="#cite_note-26">[26]</a></sup>
</p><p>In classification applications, low dimension manifolds can be 
used to model data classes which can be defined from sets of observed 
instances. Each observed instance can be described by two independent 
factors termed ’content’ and ’style’, where ’content’ is the invariant 
factor related to the essence of the class and ’style’ expresses 
variations in that class between instances.<sup id="cite_ref-27" class="reference"><a href="#cite_note-27">[27]</a></sup>
 Unfortunately, Laplacian Eigenmaps may fail to produce a coherent 
representation of a class of interest when training data consist of 
instances varying significantly in terms of style.<sup id="cite_ref-28" class="reference"><a href="#cite_note-28">[28]</a></sup>
 In the case of classes which are represented by multivariate sequences,
 Structural Laplacian Eigenmaps has been proposed to overcome this issue
 by adding additional constraints within the Laplacian Eigenmaps 
neighborhood information graph to better reflect the intrinsic structure
 of the class.<sup id="cite_ref-ReferenceB_29-0" class="reference"><a href="#cite_note-ReferenceB-29">[29]</a></sup>
 More specifically, the graph is used to encode both the sequential 
structure of the multivariate sequences and, to minimise stylistic 
variations, proximity between data points of different sequences or even
 within a sequence, if it contains repetitions. Using <a href="https://en.wikipedia.org/wiki/Dynamic_time_warping" title="Dynamic time warping">dynamic time warping</a>,
 proximity is detected by finding correspondences between and within 
sections of the multivariate sequences that exhibit high similarity. 
Experiments conducted on <a href="https://en.wikipedia.org/w/index.php?title=Vision-based_activity_recognition&amp;action=edit&amp;redlink=1" class="new" title="Vision-based activity recognition (page does not exist)">vision-based activity recognition</a>,
 object orientation classification and human 3D pose recovery 
applications have demonstrate the added value of Structural Laplacian 
Eigenmaps when dealing with multivariate sequence data.<sup id="cite_ref-ReferenceB_29-1" class="reference"><a href="#cite_note-ReferenceB-29">[29]</a></sup>
 An extension of Structural Laplacian Eigenmaps, Generalized Laplacian 
Eigenmaps led to the generation of manifolds where one of the dimensions
 specifically represents variations in style. This has proved 
particularly valuable in applications such as tracking of the human 
articulated body and silhouette extraction.<sup id="cite_ref-30" class="reference"><a href="#cite_note-30">[30]</a></sup>
</p>
<h3><span class="mw-headline" id="Manifold_alignment">Manifold alignment</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=17" title="Edit section: Manifold alignment">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/Manifold_alignment" title="Manifold alignment">Manifold alignment</a>
 takes advantage of the assumption that disparate data sets produced by 
similar generating processes will share a similar underlying manifold 
representation. By learning projections from each original space to the 
shared manifold, correspondences are recovered and knowledge from one 
domain can be transferred to another. Most manifold alignment techniques
 consider only two data sets, but the concept extends to arbitrarily 
many initial data sets.<sup id="cite_ref-31" class="reference"><a href="#cite_note-31">[31]</a></sup>
</p>
<h3><span class="mw-headline" id="Diffusion_maps">Diffusion maps</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=18" title="Edit section: Diffusion maps">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/Diffusion_map" title="Diffusion map">Diffusion maps</a> leverages the relationship between heat <a href="https://en.wikipedia.org/wiki/Diffusion" title="Diffusion">diffusion</a> and a <a href="https://en.wikipedia.org/wiki/Random_walk" title="Random walk">random walk</a> (<a href="https://en.wikipedia.org/wiki/Markov_Chain" class="mw-redirect" title="Markov Chain">Markov Chain</a>);
 an analogy is drawn between the diffusion operator on a manifold and a 
Markov transition matrix operating on functions defined on the graph 
whose nodes were sampled from the manifold.<sup id="cite_ref-32" class="reference"><a href="#cite_note-32">[32]</a></sup> In particular, let a data set be represented by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {X} =[x_{1},x_{2},\ldots ,x_{n}]\in \Omega \subset \mathbf {R^{D}} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">X</mi>
        </mrow>
        <mo>=</mo>
        <mo stretchy="false">[</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>…<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">]</mo>
        <mo>∈<!-- ∈ --></mo>
        <mi mathvariant="normal">Ω<!-- Ω --></mi>
        <mo>⊂<!-- ⊂ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <msup>
            <mi mathvariant="bold">R</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">D</mi>
            </mrow>
          </msup>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {X} =[x_{1},x_{2},\ldots ,x_{n}]\in \Omega \subset \mathbf {R^{D}} }</annotation>
  </semantics>
</math></span><img src="27767b00cc5714632b393ff3d9173e6eece42867.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:31.242ex; height:3.176ex;" alt="\mathbf {X} =[x_{1},x_{2},\ldots ,x_{n}]\in \Omega \subset \mathbf {R^{D}} "></span>. The underlying assumption of diffusion map is that the high-dimensional data lies on a low-dimensional manifold of dimension <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbf {d} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="bold">d</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbf {d} }</annotation>
  </semantics>
</math></span><img src="8ec3b626fc045b6ff579316e29978fccfed884c2.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.485ex; height:2.176ex;" alt="\mathbf {d} "></span>. Let <b>X</b> represent the data set and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mu }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>μ<!-- μ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mu }</annotation>
  </semantics>
</math></span><img src="9fd47b2a39f7a7856952afec1f1db72c67af6161.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:1.402ex; height:2.176ex;" alt="\mu "></span> represent the distribution of the data points on <b>X</b>. Further, define a <b>kernel</b> which represents some notion of affinity of the points in <b>X</b>. The kernel <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\mathit {k}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-mathit" mathvariant="italic">k</mi>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathit {k}}}</annotation>
  </semantics>
</math></span><img src="2acbed51a293a14971b3a890107329cc63bbc055.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; margin-right: -0.088ex; width:1.158ex; height:2.176ex;" alt="{\mathit {k}}"></span> has the following properties<sup id="cite_ref-ReferenceA_33-0" class="reference"><a href="#cite_note-ReferenceA-33">[33]</a></sup>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle k(x,y)=k(y,x),\,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>k</mi>
        <mo stretchy="false">(</mo>
        <mi>y</mi>
        <mo>,</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
        <mspace width="thinmathspace"></mspace>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k(x,y)=k(y,x),\,}</annotation>
  </semantics>
</math></span><img src="1304950773f768dc626e3e7daa68caec312d0f9b.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:17.212ex; height:2.843ex;" alt="k(x,y)=k(y,x),\,"></span></dd></dl>
<p><i>k</i> is symmetric
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle k(x,y)\geq 0\qquad \forall x,y,k}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
        <mo>≥<!-- ≥ --></mo>
        <mn>0</mn>
        <mspace width="2em"></mspace>
        <mi mathvariant="normal">∀<!-- ∀ --></mi>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo>,</mo>
        <mi>k</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k(x,y)\geq 0\qquad \forall x,y,k}</annotation>
  </semantics>
</math></span><img src="e5cbd0e2f6010f21c4f2d7395496e5bb4a6133f2.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:22.503ex; height:2.843ex;" alt="k(x,y)\geq 0\qquad \forall x,y,k"></span></dd></dl>
<p><i>k</i> is positivity preserving
</p><p>Thus one can think of the individual data points as the nodes of a graph and the kernel <i>k</i>
 as defining some sort of affinity on that graph. The graph is symmetric
 by construction since the kernel is symmetric. It is easy to see here 
that from the tuple (<b>X</b>,<b>k</b>) one can construct a reversible <a href="https://en.wikipedia.org/wiki/Markov_Chain" class="mw-redirect" title="Markov Chain">Markov Chain</a>. This technique is common to a variety of fields and is known as the graph Laplacian.
</p><p>For example, the graph <b>K</b> = (<i>X</i>,<i>E</i>) can be constructed using a Gaussian kernel.
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle K_{ij}={\begin{cases}e^{-\|x_{i}-x_{j}\|_{2}^{2}/\sigma ^{2}}&amp;{\text{if }}x_{i}\sim x_{j}\\0&amp;{\text{otherwise}}\end{cases}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>K</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow>
            <mo>{</mo>
            <mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false">
              <mtr>
                <mtd>
                  <msup>
                    <mi>e</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mo>−<!-- − --></mo>
                      <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
                      <msub>
                        <mi>x</mi>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mi>i</mi>
                        </mrow>
                      </msub>
                      <mo>−<!-- − --></mo>
                      <msub>
                        <mi>x</mi>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mi>j</mi>
                        </mrow>
                      </msub>
                      <msubsup>
                        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mn>2</mn>
                        </mrow>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mn>2</mn>
                        </mrow>
                      </msubsup>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mo>/</mo>
                      </mrow>
                      <msup>
                        <mi>σ<!-- σ --></mi>
                        <mrow class="MJX-TeXAtom-ORD">
                          <mn>2</mn>
                        </mrow>
                      </msup>
                    </mrow>
                  </msup>
                </mtd>
                <mtd>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>if&nbsp;</mtext>
                  </mrow>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>i</mi>
                    </mrow>
                  </msub>
                  <mo>∼<!-- ∼ --></mo>
                  <msub>
                    <mi>x</mi>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mi>j</mi>
                    </mrow>
                  </msub>
                </mtd>
              </mtr>
              <mtr>
                <mtd>
                  <mn>0</mn>
                </mtd>
                <mtd>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mtext>otherwise</mtext>
                  </mrow>
                </mtd>
              </mtr>
            </mtable>
            <mo fence="true" stretchy="true" symmetric="true"></mo>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 
K_{ij}={\begin{cases}e^{-\|x_{i}-x_{j}\|_{2}^{2}/\sigma 
^{2}}&amp;{\text{if }}x_{i}\sim 
x_{j}\\0&amp;{\text{otherwise}}\end{cases}}}</annotation>
  </semantics>
</math></span><img src="7cb0d2ac1637a4ea11c92a3d23d3c51e0f053f9b.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.671ex; width:33.105ex; height:6.509ex;" alt="K_{ij}={\begin{cases}e^{-\|x_{i}-x_{j}\|_{2}^{2}/\sigma ^{2}}&amp;{\text{if }}x_{i}\sim x_{j}\\0&amp;{\text{otherwise}}\end{cases}}"></span></dd></dl>
<p>In the above equation, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x_{i}\sim x_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>∼<!-- ∼ --></mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{i}\sim x_{j}}</annotation>
  </semantics>
</math></span><img src="683edd3f6501abdc531a4210ef6b67ab17d2f937.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:7.467ex; height:2.343ex;" alt="x_{i}\sim x_{j}"></span> denotes that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x_{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{i}}</annotation>
  </semantics>
</math></span><img src="e87000dd6142b81d041896a30fe58f0c3acb2158.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.129ex; height:2.009ex;" alt="x_{i}"></span> is a nearest neighbor of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{j}}</annotation>
  </semantics>
</math></span><img src="5db47cb3d2f9496205a17a6856c91c1d3d363ccd.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.239ex; height:2.343ex;" alt="x_{j}"></span>. Properly, <a href="https://en.wikipedia.org/wiki/Geodesic" title="Geodesic">Geodesic</a> distance should be used to actually measure distances on the <a href="https://en.wikipedia.org/wiki/Manifold" title="Manifold">manifold</a>.
 Since the exact structure of the manifold is not available, for the 
nearest neighbors the geodesic distance is approximated by euclidean 
distance. The choice <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \sigma }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>σ<!-- σ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sigma }</annotation>
  </semantics>
</math></span><img src="59f59b7c3e6fdb1d0365a494b81fb9a696138c36.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="\sigma "></span> modulates our notion of proximity in the sense that if <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \|x_{i}-x_{j}\|_{2}\gg \sigma }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>−<!-- − --></mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>≫<!-- ≫ --></mo>
        <mi>σ<!-- σ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \|x_{i}-x_{j}\|_{2}\gg \sigma }</annotation>
  </semantics>
</math></span><img src="7c94b612bf2705bc301b280d336dc7c2f0d0c117.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:15.532ex; height:3.009ex;" alt="\|x_{i}-x_{j}\|_{2}\gg \sigma "></span> then <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle K_{ij}=0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>K</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K_{ij}=0}</annotation>
  </semantics>
</math></span><img src="3cce5f477da31b516c64aa6cdb7911989271b644.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:7.711ex; height:2.843ex;" alt="K_{ij}=0"></span> and if <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \|x_{i}-x_{j}\|_{2}\ll \sigma }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>−<!-- − --></mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>≪<!-- ≪ --></mo>
        <mi>σ<!-- σ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \|x_{i}-x_{j}\|_{2}\ll \sigma }</annotation>
  </semantics>
</math></span><img src="17a6fb63a900ed8e4b08ab33994f4e5fac9a77ef.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:15.532ex; height:3.009ex;" alt="\|x_{i}-x_{j}\|_{2}\ll \sigma "></span> then <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle K_{ij}=1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>K</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K_{ij}=1}</annotation>
  </semantics>
</math></span><img src="220951965b0d01826f686ac18d2bf566455cea72.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:7.711ex; height:2.843ex;" alt="K_{ij}=1"></span>.
 The former means that very little diffusion has taken place while the 
latter implies that the diffusion process is nearly complete. Different 
strategies to choose <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \sigma }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>σ<!-- σ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sigma }</annotation>
  </semantics>
</math></span><img src="59f59b7c3e6fdb1d0365a494b81fb9a696138c36.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="\sigma "></span> can be found in.<sup id="cite_ref-34" class="reference"><a href="#cite_note-34">[34]</a></sup>
</p><p>In order to faithfully represent a Markov matrix, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle K}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K}</annotation>
  </semantics>
</math></span><img src="2b76fce82a62ed5461908f0dc8f037de4e3686b0.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;" alt="K"></span> must be normalized by the corresponding <a href="https://en.wikipedia.org/wiki/Degree_matrix" title="Degree matrix">degree matrix</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle D}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>D</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D}</annotation>
  </semantics>
</math></span><img src="f34a0c600395e5d4345287e21fb26efd386990e6.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.924ex; height:2.176ex;" alt="D"></span>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle P=D^{-1}K.\,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo>=</mo>
        <msup>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>−<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mi>K</mi>
        <mo>.</mo>
        <mspace width="thinmathspace"></mspace>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P=D^{-1}K.\,}</annotation>
  </semantics>
</math></span><img src="eb029075ae4592f7e78f75c0bee5d9ecbcfaf17e.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:12.201ex; height:2.676ex;" alt="P=D^{-1}K.\,"></span></dd></dl>
<p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle P}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P}</annotation>
  </semantics>
</math></span><img src="b4dc73bf40314945ff376bd363916a738548d40a.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.745ex; height:2.176ex;" alt="P"></span> now represents a Markov chain. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle P(x_{i},x_{j})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P(x_{i},x_{j})}</annotation>
  </semantics>
</math></span><img src="4ed08139d79724fd6a470d8d02bd3f92c95e98b4.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:8.957ex; height:3.009ex;" alt="P(x_{i},x_{j})"></span> is the probability of transitioning from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x_{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{i}}</annotation>
  </semantics>
</math></span><img src="e87000dd6142b81d041896a30fe58f0c3acb2158.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.129ex; height:2.009ex;" alt="x_{i}"></span> to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{j}}</annotation>
  </semantics>
</math></span><img src="5db47cb3d2f9496205a17a6856c91c1d3d363ccd.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.239ex; height:2.343ex;" alt="x_{j}"></span> in one time step. Similarly the probability of transitioning from <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x_{i}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{i}}</annotation>
  </semantics>
</math></span><img src="e87000dd6142b81d041896a30fe58f0c3acb2158.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.129ex; height:2.009ex;" alt="x_{i}"></span> to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x_{j}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{j}}</annotation>
  </semantics>
</math></span><img src="5db47cb3d2f9496205a17a6856c91c1d3d363ccd.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.239ex; height:2.343ex;" alt="x_{j}"></span> in <b>t</b> time steps is given by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle P^{t}(x_{i},x_{j})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P^{t}(x_{i},x_{j})}</annotation>
  </semantics>
</math></span><img src="71719d0f7f63c9128f27ce0e0d31d9d0baedde56.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:9.859ex; height:3.176ex;" alt="P^{t}(x_{i},x_{j})"></span>. Here <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle P^{t}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P^{t}}</annotation>
  </semantics>
</math></span><img src="3874f8df55758c9c93fc87e7d3540f24ab666faf.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.647ex; height:2.509ex;" alt="P^{t}"></span> is the matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle P}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P}</annotation>
  </semantics>
</math></span><img src="b4dc73bf40314945ff376bd363916a738548d40a.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.745ex; height:2.176ex;" alt="P"></span> multiplied by itself <b>t</b> times.
</p><p>The Markov matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle P}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P}</annotation>
  </semantics>
</math></span><img src="b4dc73bf40314945ff376bd363916a738548d40a.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.745ex; height:2.176ex;" alt="P"></span> constitutes some notion of local geometry of the data set <b>X</b>. The major difference between diffusion maps and <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a>
 is that only local features of the data are considered in diffusion 
maps as opposed to taking correlations of the entire data set.
</p><p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle K}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K}</annotation>
  </semantics>
</math></span><img src="2b76fce82a62ed5461908f0dc8f037de4e3686b0.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.066ex; height:2.176ex;" alt="K"></span>
 defines a random walk on the data set which means that the kernel 
captures some local geometry of data set. The Markov chain defines fast 
and slow directions of propagation through the kernel values. As the 
walk propagates forward in time, the local geometry information 
aggregates in the same way as local transitions (defined by differential
 equations) of the dynamical system.<sup id="cite_ref-ReferenceA_33-1" class="reference"><a href="#cite_note-ReferenceA-33">[33]</a></sup> The metaphor of diffusion arises from the definition of a family diffusion distance  {<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle D_{t}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}}</annotation>
  </semantics>
</math></span><img src="875ab92f7e53970140b3663bc81e5fdcd9528a63.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.75ex; height:2.509ex;" alt="D_{t}"></span>}<span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle _{t\in N}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
            <mo>∈<!-- ∈ --></mo>
            <mi>N</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle _{t\in N}}</annotation>
  </semantics>
</math></span><img src="122e751d6f6ce6cbce64f4f0838a379c315cb0c9.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:3.381ex; height:1.676ex;" alt="_{t\in N}"></span>
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle D_{t}^{2}(x,y)=||p_{t}(x,\cdot )-p_{t}(y,\cdot )||^{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msubsup>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mo>⋅<!-- ⋅ --></mo>
        <mo stretchy="false">)</mo>
        <mo>−<!-- − --></mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>y</mi>
        <mo>,</mo>
        <mo>⋅<!-- ⋅ --></mo>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">|</mo>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}^{2}(x,y)=||p_{t}(x,\cdot )-p_{t}(y,\cdot )||^{2}}</annotation>
  </semantics>
</math></span><img src="0a3d5eae037d1c6269e9b9319586317597405528.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:31.343ex; height:3.509ex;" alt="D_{t}^{2}(x,y)=||p_{t}(x,\cdot )-p_{t}(y,\cdot )||^{2}"></span></dd></dl>
<p>For fixed t, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle D_{t}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}}</annotation>
  </semantics>
</math></span><img src="875ab92f7e53970140b3663bc81e5fdcd9528a63.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.75ex; height:2.509ex;" alt="D_{t}"></span> defines a distance between any two points of the data set based on path connectivity: the value of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle D_{t}(x,y)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}(x,y)}</annotation>
  </semantics>
</math></span><img src="29152128bcc76b04fc7e91db93358e8c6b31c2df.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.079ex; height:2.843ex;" alt="D_{t}(x,y)"></span> will be smaller the more paths that connect <b>x</b> to <b>y</b> and vice versa. Because the quantity <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle D_{t}(x,y)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo>,</mo>
        <mi>y</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}(x,y)}</annotation>
  </semantics>
</math></span><img src="29152128bcc76b04fc7e91db93358e8c6b31c2df.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.079ex; height:2.843ex;" alt="D_{t}(x,y)"></span> involves a sum over of all paths of length t, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle D_{t}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}}</annotation>
  </semantics>
</math></span><img src="875ab92f7e53970140b3663bc81e5fdcd9528a63.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.75ex; height:2.509ex;" alt="D_{t}"></span> is much more robust to noise in the data than geodesic distance. <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle D_{t}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>t</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{t}}</annotation>
  </semantics>
</math></span><img src="875ab92f7e53970140b3663bc81e5fdcd9528a63.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.75ex; height:2.509ex;" alt="D_{t}"></span>
 takes into account all the relation between points x and y while 
calculating the distance and serves as a better notion of proximity than
 just <a href="https://en.wikipedia.org/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> or even geodesic distance.
</p>
<h3><span id="Hessian_Locally-Linear_Embedding_.28Hessian_LLE.29"></span><span class="mw-headline" id="Hessian_Locally-Linear_Embedding_(Hessian_LLE)">Hessian Locally-Linear Embedding (Hessian LLE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=19" title="Edit section: Hessian Locally-Linear Embedding (Hessian LLE)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Like LLE, <a href="https://en.wikipedia.org/w/index.php?title=Hessian_LLE&amp;action=edit&amp;redlink=1" class="new" title="Hessian LLE (page does not exist)">Hessian LLE</a><sup id="cite_ref-35" class="reference"><a href="#cite_note-35">[35]</a></sup>
 is also based on sparse matrix techniques. It tends to yield results of
 a much higher quality than LLE. Unfortunately, it has a very costly 
computational complexity, so it is not well-suited for heavily sampled 
manifolds. It has no internal model.
</p>
<h3><span id="Modified_Locally-Linear_Embedding_.28MLLE.29"></span><span class="mw-headline" id="Modified_Locally-Linear_Embedding_(MLLE)">Modified Locally-Linear Embedding (MLLE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=20" title="Edit section: Modified Locally-Linear Embedding (MLLE)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Modified LLE (MLLE)<sup id="cite_ref-36" class="reference"><a href="#cite_note-36">[36]</a></sup>
 is another LLE variant which uses multiple weights in each neighborhood
 to address the local weight matrix conditioning problem which leads to 
distortions in LLE maps.  MLLE produces robust projections similar to 
Hessian LLE, but without the significant additional computational cost.
</p>
<h3><span class="mw-headline" id="Relational_perspective_map">Relational perspective map</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=21" title="Edit section: Relational perspective map">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Relational perspective map is a <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a>
 algorithm. The algorithm finds a configuration of data points on a 
manifold by simulating a multi-particle dynamic system on a closed 
manifold, where data points are mapped to particles and distances (or 
dissimilarity) between data points represent a repulsive force. As the 
manifold gradually grows in size the multi-particle system cools down 
gradually and converges to a configuration that reflects the distance 
information of the data points.
</p><p>Relational perspective map was inspired by a physical model in 
which positively charged particles move freely on the surface of a ball.
  Guided by the <a href="https://en.wikipedia.org/wiki/Charles-Augustin_de_Coulomb" title="Charles-Augustin de Coulomb">Coulomb</a> <a href="https://en.wikipedia.org/wiki/Coulomb%27s_law" title="Coulomb's law">force</a>
 between particles, the minimal energy configuration of the particles 
will reflect the strength of repulsive forces between the particles.
</p><p>The Relational perspective map was introduced in.<sup id="cite_ref-37" class="reference"><a href="#cite_note-37">[37]</a></sup>
The algorithm firstly used the flat <a href="https://en.wikipedia.org/wiki/Torus" title="Torus">torus</a> as the image manifold, then it has been extended (in the software <a rel="nofollow" class="external text" href="http://www.visumap.com/">VisuMap</a> to use other types of closed manifolds, like the <a href="https://en.wikipedia.org/wiki/Sphere" title="Sphere">sphere</a>, <a href="https://en.wikipedia.org/wiki/Projective_space" title="Projective space">projective space</a>, and <a href="https://en.wikipedia.org/wiki/Klein_bottle" title="Klein bottle">Klein bottle</a>, as image manifolds.
</p>
<h3><span class="mw-headline" id="Local_tangent_space_alignment">Local tangent space alignment</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=22" title="Edit section: Local tangent space alignment">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Local_tangent_space_alignment" title="Local tangent space alignment">Local tangent space alignment</a></div>
<p><a href="https://en.wikipedia.org/wiki/Local_tangent_space_alignment" title="Local tangent space alignment">LTSA</a><sup id="cite_ref-38" class="reference"><a href="#cite_note-38">[38]</a></sup>
 is based on the intuition that when a manifold is correctly unfolded, 
all of the tangent hyperplanes to the manifold will become aligned.  It 
begins by computing the <i>k</i>-nearest neighbors of every point.  It computes the tangent space at every point by computing the <i>d</i>-first principal components in each local neighborhood.  It then optimizes to find an embedding that aligns the tangent spaces.
</p>
<h3><span class="mw-headline" id="Local_multidimensional_scaling">Local multidimensional scaling</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=23" title="Edit section: Local multidimensional scaling">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Local Multidimensional Scaling<sup id="cite_ref-39" class="reference"><a href="#cite_note-39">[39]</a></sup> performs <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling" title="Multidimensional scaling">multidimensional scaling</a> in local regions, and then uses convex optimization to fit all the pieces together.
</p>
<h3><span class="mw-headline" id="Maximum_variance_unfolding">Maximum variance unfolding</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=24" title="Edit section: Maximum variance unfolding">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/Maximum_Variance_Unfolding" class="mw-redirect" title="Maximum Variance Unfolding">Maximum Variance Unfolding</a>
 was formerly known as Semidefinite Embedding. The intuition for this 
algorithm is that when a manifold is properly unfolded, the variance 
over the points is maximized. This algorithm also begins by finding the <i>k</i>-nearest
 neighbors of every point. It then seeks to solve the problem of 
maximizing the distance between all non-neighboring points, constrained 
such that the distances between neighboring points are preserved. The 
primary contribution of this algorithm is a technique for casting this 
problem as a semidefinite programming problem. Unfortunately, 
semidefinite programming solvers have a high computational cost. The 
Landmark–MVU variant of this algorithm uses landmarks to increase speed 
with some cost to accuracy. It has no model.
</p>
<h3><span class="mw-headline" id="Nonlinear_PCA">Nonlinear PCA</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=25" title="Edit section: Nonlinear PCA">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Nonlinear PCA<sup id="cite_ref-40" class="reference"><a href="#cite_note-40">[40]</a></sup> (NLPCA) uses <a href="https://en.wikipedia.org/wiki/Backpropagation" title="Backpropagation">backpropagation</a>
 to train a multi-layer perceptron (MLP) to fit to a manifold. Unlike 
typical MLP training, which only updates the weights, NLPCA updates both
 the weights and the inputs. That is, both the weights and inputs are 
treated as latent values. After training, the latent inputs are a 
low-dimensional representation of the observed vectors, and the MLP maps
 from that low-dimensional representation to the high-dimensional 
observation space.
</p>
<h3><span class="mw-headline" id="Data-driven_high-dimensional_scaling">Data-driven high-dimensional scaling</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=26" title="Edit section: Data-driven high-dimensional scaling">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Data-Driven High Dimensional Scaling (DD-HDS)<sup id="cite_ref-41" class="reference"><a href="#cite_note-41">[41]</a></sup> is closely related to <a href="https://en.wikipedia.org/wiki/Sammon%27s_mapping" class="mw-redirect" title="Sammon's mapping">Sammon's mapping</a>
 and curvilinear component analysis except that (1) it simultaneously 
penalizes false neighborhoods and tears by focusing on small distances 
in both original and output space, and that (2) it accounts for <a href="https://en.wikipedia.org/wiki/Concentration_of_measure" title="Concentration of measure">concentration of measure</a> phenomenon by adapting the weighting function to the distance distribution.
</p>
<h3><span class="mw-headline" id="Manifold_sculpting">Manifold sculpting</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=27" title="Edit section: Manifold sculpting">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Manifold Sculpting<sup id="cite_ref-42" class="reference"><a href="#cite_note-42">[42]</a></sup> uses <a href="https://en.wikipedia.org/wiki/Graduated_optimization" title="Graduated optimization">graduated optimization</a> to find an embedding. Like other algorithms, it computes the <i>k</i>-nearest
 neighbors and tries to seek an embedding that preserves relationships 
in local neighborhoods. It slowly scales variance out of higher 
dimensions, while simultaneously adjusting points in lower dimensions to
 preserve those relationships. If the rate of scaling is small, it can 
find very precise embeddings. It boasts higher empirical accuracy than 
other algorithms with several problems. It can also be used to refine 
the results from other manifold learning algorithms. It struggles to 
unfold some manifolds, however, unless a very slow scaling rate is used.
 It has no model.
</p>
<h3><span class="mw-headline" id="t-distributed_stochastic_neighbor_embedding">t-distributed stochastic neighbor embedding</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=28" title="Edit section: t-distributed stochastic neighbor embedding">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-distributed stochastic neighbor embedding</a> (t-SNE) <sup id="cite_ref-43" class="reference"><a href="#cite_note-43">[43]</a></sup> is widely used. It is one of a family of stochastic neighbor embedding methods.
</p>
<h3><span class="mw-headline" id="RankVisu">RankVisu</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=29" title="Edit section: RankVisu">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>RankVisu<sup id="cite_ref-44" class="reference"><a href="#cite_note-44">[44]</a></sup>
 is designed to preserve rank of neighborhood rather than distance. 
RankVisu is especially useful on difficult tasks (when the preservation 
of distance cannot be achieved satisfyingly). Indeed, the rank of 
neighborhood is less informative than distance (ranks can be deduced 
from distances but distances cannot be deduced from ranks) and its 
preservation is thus easier.
</p>
<h3><span class="mw-headline" id="Topologically_constrained_isometric_embedding">Topologically constrained isometric embedding</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=30" title="Edit section: Topologically constrained isometric embedding">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><a href="https://en.wikipedia.org/w/index.php?title=Topologically_Constrained_Isometric_Embedding&amp;action=edit&amp;redlink=1" class="new" title="Topologically Constrained Isometric Embedding (page does not exist)">Topologically Constrained Isometric Embedding</a> (TCIE)<sup id="cite_ref-45" class="reference"><a href="#cite_note-45">[45]</a></sup>
 is an algorithm based approximating geodesic distances after filtering 
geodesics inconsistent with the Euclidean metric. Aimed at correcting 
the distortions caused when Isomap is used to map intrinsically 
non-convex data, TCIE uses weight least-squares MDS in order to obtain a
 more accurate mapping. The TCIE algorithm first detects possible 
boundary points in the data, and during computation of the geodesic 
length marks inconsistent geodesics, to be given a small weight in the 
weighted <a href="https://en.wikipedia.org/wiki/Stress_majorization" title="Stress majorization">Stress majorization</a> that follows.
</p>
<h2><span class="mw-headline" id="Methods_based_on_proximity_matrices">Methods based on proximity matrices</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=31" title="Edit section: Methods based on proximity matrices">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A method based on proximity matrices is one where the data is presented to the algorithm in the form of a <a href="https://en.wikipedia.org/wiki/Similarity_matrix" class="mw-redirect" title="Similarity matrix">similarity matrix</a> or a <a href="https://en.wikipedia.org/wiki/Distance_matrix" title="Distance matrix">distance matrix</a>. These methods all fall under the broader class of <a href="https://en.wikipedia.org/wiki/Multidimensional_scaling#Types" title="Multidimensional scaling">metric multidimensional scaling</a>. The variations tend to be differences in how the proximity data is computed; for example, <a href="https://en.wikipedia.org/wiki/Isomap" title="Isomap">Isomap</a>, <a href="https://en.wikipedia.org/wiki/Locally_linear_embeddings" class="mw-redirect" title="Locally linear embeddings">locally linear embeddings</a>, <a href="https://en.wikipedia.org/wiki/Maximum_variance_unfolding" class="mw-redirect" title="Maximum variance unfolding">maximum variance unfolding</a>, and <a href="https://en.wikipedia.org/wiki/Sammon%27s_projection" class="mw-redirect" title="Sammon's projection">Sammon mapping</a> (which is not in fact a mapping) are examples of metric multidimensional scaling methods.
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=32" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Discriminant_analysis" class="mw-redirect" title="Discriminant analysis">Discriminant analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Elastic_map" title="Elastic map">Elastic map</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Growing_self-organizing_map" title="Growing self-organizing map">Growing self-organizing map</a> (GSOM)</li>
<li><a href="https://en.wikipedia.org/wiki/Self-organizing_map" title="Self-organizing map">Self-organizing map</a> (SOM)</li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=33" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Lawrence, Neil D (2012). <a rel="nofollow" class="external text" href="http://www.jmlr.org/papers/v13/lawrence12a.html">"A unifying probabilistic perspective for spectral dimensionality reduction: insights and new models"</a>. <i><a href="https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">Journal of Machine Learning Research</a></i>. <b>13</b> (May): 1609–1638.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.atitle=A+unifying+probabilistic+perspective+for+spectral+dimensionality+reduction%3A+insights+and+new+models&amp;rft.volume=13&amp;rft.issue=May&amp;rft.pages=1609-1638&amp;rft.date=2012&amp;rft.aulast=Lawrence&amp;rft.aufirst=Neil+D&amp;rft_id=http%3A%2F%2Fwww.jmlr.org%2Fpapers%2Fv13%2Flawrence12a.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANonlinear+dimensionality+reduction" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">John A. Lee, Michel Verleysen, Nonlinear Dimensionality Reduction, Springer, 2007.</span>
</li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Gashler, M. and Martinez, T., <a rel="nofollow" class="external text" href="http://axon.cs.byu.edu/papers/gashler2011ijcnn2.pdf">Temporal Nonlinear Dimensionality Reduction</a>, In <i>Proceedings of the International Joint Conference on Neural Networks IJCNN'11</i>, pp. 1959–1966, 2011</span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">The illustration is prepared using free software: E.M. Mirkes, <a rel="nofollow" class="external text" href="http://www.math.le.ac.uk/people/ag153/homepage/PCA_SOM/PCA_SOM.html">Principal Component Analysis and Self-Organizing Maps: applet</a>. University of Leicester, 2011</span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Yin, Hujun; <a rel="nofollow" class="external text" href="http://pca.narod.ru/contentsgkwz.htm"><i>Learning Nonlinear Principal Manifolds by Self-Organising Maps</i></a>, in A.N. Gorban, B. Kégl, D.C. Wunsch, and A. Zinovyev (Eds.), <i>Principal Manifolds for Data Visualization and Dimension Reduction</i>, Lecture Notes in Computer Science and Engineering (LNCSE), vol. 58, Berlin, Germany: Springer, 2007, Ch. 3, pp. 68-95. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-3-540-73749-0" title="Special:BookSources/978-3-540-73749-0">978-3-540-73749-0</a></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">A. N. Gorban, A. Zinovyev, <a rel="nofollow" class="external text" href="https://arxiv.org/abs/1001.1122">Principal manifolds and graphs in practice: from molecular biology to dynamical systems</a>, <a href="https://en.wikipedia.org/wiki/International_Journal_of_Neural_Systems" title="International Journal of Neural Systems">International Journal of Neural Systems</a>, Vol. 20, No. 3 (2010) 219–232.</span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">A. Zinovyev, <a rel="nofollow" class="external text" href="http://bioinfo-out.curie.fr/projects/vidaexpert/">ViDaExpert</a> - Multidimensional Data Visualization Tool (free for non-commercial use). <a href="https://en.wikipedia.org/wiki/Curie_Institute_%28Paris%29" title="Curie Institute (Paris)">Institut Curie</a>, Paris.</span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">A. Zinovyev, <a rel="nofollow" class="external text" href="http://www.ihes.fr/%7Ezinovyev/vida/ViDaExpert/ViDaOverView.pdf">ViDaExpert overview</a>, <a rel="nofollow" class="external text" href="http://www.ihes.fr/">IHES</a> (<a href="https://en.wikipedia.org/wiki/Institut_des_Hautes_%C3%89tudes_Scientifiques" title="Institut des Hautes Études Scientifiques">Institut des Hautes Études Scientifiques</a>), Bures-Sur-Yvette, Île-de-France.</span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">T.
 Hastie, Principal Curves and Surfaces, Ph.D Dissertation, Stanford 
Linear Accelerator Center, Stanford University, Stanford, California, 
US, November 1984.</span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><a href="https://en.wikipedia.org/wiki/Alexander_Nikolaevich_Gorban" class="mw-redirect" title="Alexander Nikolaevich Gorban">A.N. Gorban</a>, B. Kégl, D.C. Wunsch, A. Zinovyev  (Eds.), <a rel="nofollow" class="external text" href="https://www.researchgate.net/publication/271642170_Principal_Manifolds_for_Data_Visualisation_and_Dimension_Reduction_LNCSE_58">Principal Manifolds for Data Visualisation and Dimension Reduction</a>, Lecture Notes in Computer Science and Engineering (LNCSE), Vol. 58, Springer, Berlin – Heidelberg – New York, 2007. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-3-540-73749-0" title="Special:BookSources/978-3-540-73749-0">978-3-540-73749-0</a></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">N. Lawrence, <a rel="nofollow" class="external text" href="http://jmlr.csail.mit.edu/papers/v6/lawrence05a.html">Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models</a>, Journal of Machine Learning Research 6(Nov):1783–1816, 2005.</span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">M. Ding, G. Fan, <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/abstract/document/6985586/">Multilayer Joint Gait-Pose Manifolds for Human Gait Motion Modeling</a>, IEEE Transactions on Cybernetics, Volume: 45, Issue: 11, Nov 2015.</span>
</li>
<li id="cite_note-Demart-13"><span class="mw-cite-backlink">^ <a href="#cite_ref-Demart_13-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Demart_13-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">P.
 Demartines and J. Hérault, Curvilinear Component Analysis: A 
Self-Organizing Neural Network for Nonlinear Mapping of Data Sets, IEEE 
Transactions on Neural Networks, Vol. 8(1), 1997, pp. 148–154</span>
</li>
<li id="cite_note-Jigang-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-Jigang_14-0"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Jigang Sun, Malcolm Crowe, and Colin Fyfe,  <a rel="nofollow" class="external text" href="http://www.dice.ucl.ac.be/Proceedings/esann/esannpdf/es2010-107.pdf">Curvilinear component analysis and Bregman divergences</a>, In European Symposium on Artificial Neural Networks (Esann), pages 81–86. d-side publications, 2010</span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Christian
 Walder and Bernhard Schölkopf, Diffeomorphic Dimensionality Reduction, 
Advances in Neural Information Processing Systems 22, 2009, pp. 
1713–1720, MIT Press</span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">B. Schölkopf, A. Smola, K.-R. Müller, Nonlinear Component Analysis as a Kernel Eigenvalue Problem. <i>Neural Computation </i>10(5):1299-1319, 1998, <a href="https://en.wikipedia.org/wiki/MIT_Press" title="MIT Press">MIT Press</a> Cambridge, MA, USA, <a href="https://doi.org/10.1162/089976698300017467" class="extiw" title="doi:10.1162/089976698300017467">doi:10.1162/089976698300017467</a></span>
</li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Jihun
 Ham, Daniel D. Lee, Sebastian Mika, Bernhard Schölkopf. A kernel view 
of the dimensionality reduction of manifolds. Proceedings of the 21st 
International Conference on Machine Learning, Banff, Canada, 2004. <a href="https://doi.org/10.1145/1015330.1015417" class="extiw" title="doi:10.1145/1015330.1015417">doi:10.1145/1015330.1015417</a></span>
</li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">J.
 B. Tenenbaum, V. de Silva, J. C. Langford, A Global Geometric Framework
 for Nonlinear Dimensionality Reduction, Science 290, (2000), 2319–2323.</span>
</li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Taylor,
 D., Klimm, F., Harrington, H. A., Kramár, M., Mischaikow, K., Porter, 
M. A., &amp; Mucha, P. J. (2015). Topological data analysis of contagion
 maps for examining spreading processes on networks. Nature 
Communications, 6, 7723.</span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">S.
 T. Roweis and L. K. Saul, Nonlinear Dimensionality Reduction by Locally
 Linear Embedding, Science Vol 290, 22 December 2000, 2323–2326.</span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Mikhail Belkin and <a href="https://en.wikipedia.org/wiki/Partha_Niyogi" title="Partha Niyogi">Partha Niyogi</a>,
 Laplacian Eigenmaps and Spectral Techniques for Embedding and 
Clustering, Advances in Neural Information Processing Systems 14, 2001, 
p. 586–691, MIT Press</span>
</li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Mikhail Belkin Problems of Learning on Manifolds, PhD Thesis, Department of Mathematics, The University Of Chicago, August 2003</span>
</li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Bengio
 et al. "Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and 
Spectral Clustering" in Advances in Neural Information Processing 
Systems (2004)</span>
</li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Mikhail Belkin Problems of Learning on Manifolds, PhD Thesis, Department of Mathematics, The <a href="https://en.wikipedia.org/wiki/University_Of_Chicago" class="mw-redirect" title="University Of Chicago">University Of Chicago</a>, August 2003</span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.cse.ohio-state.edu/%7Embelkin/algorithms/algorithms.html">Ohio-state.edu</a></span>
</li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.cse.ohio-state.edu/%7Embelkin/papers/papers.html#thesis">Ohio-state.edu</a></span>
</li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">J. Tenenbaum and W. Freeman, Separating style and content with bilinear models, Neural Computation, vol. 12, 2000.</span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">M.
 Lewandowski, J. Martinez-del Rincon, D. Makris, and J.-C. Nebel, 
Temporal extension of laplacian eigenmaps for unsupervised 
dimensionality reduction of time series, Proceedings of the 
International Conference on Pattern Recognition (ICPR), 2010</span>
</li>
<li id="cite_note-ReferenceB-29"><span class="mw-cite-backlink">^ <a href="#cite_ref-ReferenceB_29-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ReferenceB_29-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">M.
 Lewandowski, D. Makris, S.A. Velastin and J.-C. Nebel, Structural 
Laplacian Eigenmaps for Modeling Sets of Multivariate Sequences, IEEE 
Transactions on Cybernetics, 44(6): 936-949, 2014</span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">J.
 Martinez-del-Rincon, M. Lewandowski, J.-C. Nebel and D. Makris, 
Generalized Laplacian Eigenmaps for Modeling and Tracking Human Motions,
 IEEE Transactions on Cybernetics, 44(9), pp 1646-1660, 2014</span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation conference">Wang, Chang; Mahadevan, Sridhar (July 2008). <a rel="nofollow" class="external text" href="http://people.cs.umass.edu/%7Echwang/papers/ICML-2008.pdf"><i>Manifold Alignment using Procrustes Analysis</i></a> <span style="font-size:85%;">(PDF)</span>. The 25th International Conference on Machine Learning. pp.&nbsp;1120–1127.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Manifold+Alignment+using+Procrustes+Analysis&amp;rft.pages=1120-1127&amp;rft.date=2008-07&amp;rft.aulast=Wang&amp;rft.aufirst=Chang&amp;rft.au=Mahadevan%2C+Sridhar&amp;rft_id=http%3A%2F%2Fpeople.cs.umass.edu%2F~chwang%2Fpapers%2FICML-2008.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANonlinear+dimensionality+reduction" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Diffusion Maps and Geometric Harmonics, Stephane Lafon, PhD Thesis, <a href="https://en.wikipedia.org/wiki/Yale_University" title="Yale University">Yale University</a>, May 2004</span>
</li>
<li id="cite_note-ReferenceA-33"><span class="mw-cite-backlink">^ <a href="#cite_ref-ReferenceA_33-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-ReferenceA_33-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">Diffusion Maps, Ronald R. Coifman and Stephane Lafon,: Science, 19 June 2006</span>
</li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">B. Bah, "Diffusion Maps: Applications and Analysis", Masters Thesis, University of Oxford</span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">D.
 Donoho and C. Grimes, "Hessian eigenmaps: Locally linear embedding 
techniques for high-dimensional data" Proc Natl Acad Sci U S A. 2003 May
 13; 100(10): 5591–5596</span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Z. Zhang and J. Wang, "MLLE: Modified Locally Linear Embedding Using Multiple Weights" <a rel="nofollow" class="external free" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382</a></span>
</li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">James X. Li, <a rel="nofollow" class="external text" href="http://www.palgrave-journals.com/ivs/journal/v3/n1/pdf/9500051a.pdf">Visualizing high-dimensional data with relational perspective map</a>, Information Visualization (2004) 3, 49–59</span>
</li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Zhang, Zhenyue; Hongyuan Zha (2005). "Principal Manifolds and Nonlinear Dimension Reduction via Local Tangent Space Alignment". <i>SIAM Journal on Scientific Computing</i>. <b>26</b> (1): 313–338. <a href="https://en.wikipedia.org/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&nbsp;<span class="plainlinks"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.211.9957">10.1.1.211.9957</a> <img alt="Freely accessible" src="9px-Lock-green.png" title="Freely accessible" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" width="9" height="14"></span>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1137/s1064827502419154">10.1137/s1064827502419154</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=SIAM+Journal+on+Scientific+Computing&amp;rft.atitle=Principal+Manifolds+and+Nonlinear+Dimension+Reduction+via+Local+Tangent+Space+Alignment&amp;rft.volume=26&amp;rft.issue=1&amp;rft.pages=313-338&amp;rft.date=2005&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.211.9957&amp;rft_id=info%3Adoi%2F10.1137%2Fs1064827502419154&amp;rft.aulast=Zhang&amp;rft.aufirst=Zhenyue&amp;rft.au=Hongyuan+Zha&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANonlinear+dimensionality+reduction" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">J Venna and S Kaski, Local multidimensional scaling, Neural Networks, 2006</span>
</li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Scholz, M. Kaplan, F. Guy, C. L. Kopka, J. Selbig, J., Non-linear PCA: a missing data approach, In <i>Bioinformatics</i>, Vol. 21, Number 20, pp. 3887–3895, Oxford University Press, 2005</span>
</li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">S.
 Lespinats, M. Verleysen, A. Giron, B. Fertil, DD-HDS: a tool for 
visualization and exploration of high-dimensional data, IEEE 
Transactions on Neural Networks 18 (5) (2007) 1265–1279.</span>
</li>
<li id="cite_note-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-42"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Gashler, M. and Ventura, D. and Martinez, T., <i><a rel="nofollow" class="external text" href="http://axon.cs.byu.edu/papers/gashler2007nips.pdf">Iterative Non-linear Dimensionality Reduction with Manifold Sculpting</a></i>,
 In Platt, J.C. and Koller, D. and Singer, Y. and Roweis, S., editor, 
Advances in Neural Information Processing Systems 20, pp. 513–520, MIT 
Press, Cambridge, MA, 2008</span>
</li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">van der Maaten, L.J.P.; Hinton, G.E. (Nov 2008). <a rel="nofollow" class="external text" href="http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">"Visualizing High-Dimensional Data Using t-SNE"</a> <span style="font-size:85%;">(PDF)</span>. <i>Journal of Machine Learning Research 9</i>: 2579–2605.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Machine+Learning+Research+9&amp;rft.atitle=Visualizing+High-Dimensional+Data+Using+t-SNE&amp;rft.pages=2579-2605&amp;rft.date=2008-11&amp;rft.aulast=van+der+Maaten&amp;rft.aufirst=L.J.P.&amp;rft.au=Hinton%2C+G.E.&amp;rft_id=http%3A%2F%2Fjmlr.org%2Fpapers%2Fvolume9%2Fvandermaaten08a%2Fvandermaaten08a.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANonlinear+dimensionality+reduction" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Lespinats
 S., Fertil B., Villemain P. and Herault J., Rankvisu: Mapping from the 
neighbourhood network, Neurocomputing, vol. 72 (13–15), pp. 2964–2978, 
2009.</span>
</li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Rosman
 G., Bronstein M. M., Bronstein A. M. and Kimmel R., Nonlinear 
Dimensionality Reduction by Topologically Constrained Isometric 
Embedding, International Journal of Computer Vision, Volume 89, Number 
1, 56–68, 2010</span>
</li>
</ol></div></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit&amp;section=34" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20040411051530/http://isomap.stanford.edu/">Isomap</a></li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20090204021348/http://www.ncrg.aston.ac.uk/GTM/">Generative Topographic Mapping</a></li>
<li><a rel="nofollow" class="external text" href="http://www.miketipping.com/thesis.htm">Mike Tipping's Thesis</a></li>
<li><a rel="nofollow" class="external text" href="http://www.dcs.shef.ac.uk/%7Eneil/gplvm/">Gaussian Process Latent Variable Model</a></li>
<li><a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/%7Eroweis/lle/">Locally Linear Embedding</a></li>
<li><a rel="nofollow" class="external text" href="http://www.visumap.net/index.aspx?p=Resources/RpmOverview">Relational Perspective Map</a></li>
<li><a rel="nofollow" class="external text" href="http://waffles.sourceforge.net/">Waffles</a>
 is an open source C++ library containing implementations of LLE, 
Manifold Sculpting, and some other manifold learning algorithms.</li>
<li><a rel="nofollow" class="external text" href="http://sy.lespi.free.fr/DD-HDS-homepage.html">DD-HDS homepage</a></li>
<li><a rel="nofollow" class="external text" href="http://sy.lespi.free.fr/RankVisu-homepage.html">RankVisu homepage</a></li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20111002023651/http://tx.technion.ac.il/%7Erc/diffusion_maps.pdf">Short review of Diffusion Maps</a></li>
<li><a rel="nofollow" class="external text" href="http://www.nlpca.org/">Nonlinear PCA by autoencoder neural networks</a></li></ul>

<!-- 
NewPP limit report
Parsed by mw1262
Cached time: 20180801011444
Cache expiry: 1900800
Dynamic content: false
CPU time usage: 0.300 seconds
Real time usage: 0.456 seconds
Preprocessor visited node count: 1571/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 10378/2097152 bytes
Template argument size: 990/2097152 bytes
Highest expansion depth: 16/40
Expensive parser function count: 0/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 25742/5000000 bytes
Number of Wikibase entities loaded: 0/400
Lua time usage: 0.042/10.000 seconds
Lua memory usage: 2.54 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  156.818      1 -total
 70.41%  110.415      1 Template:Reflist
 26.75%   41.942      3 Template:Cite_journal
 19.84%   31.120      2 Template:ISBN
 16.40%   25.723      1 Template:See_also
  8.46%   13.265      2 Template:Catalog_lookup_link
  4.33%    6.787      1 Template:Cite_conference
  3.98%    6.240      2 Template:Error-small
  2.87%    4.493      6 Template:Yesno-no
  2.70%    4.241      1 Template:Main
-->
</div>
<!-- Saved in parser cache with key enwiki:pcache:idhash:309261-0!canonical!math=5 and timestamp 20180801011443 and revision id 848551806
 -->
<noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;oldid=848551806">https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;oldid=848551806</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="https://en.wikipedia.org/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Dimension_reduction" title="Category:Dimension reduction">Dimension reduction</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="https://en.wikipedia.org/wiki/Special:MyTalk" title="Discussion about edits from this IP address [Alt+Shift+n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="https://en.wikipedia.org/wiki/Special:MyContributions" title="A list of edits made from this IP address [Alt+Shift+y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&amp;returnto=Nonlinear+dimensionality+reduction&amp;returntoquery=oldid%3D848551806" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="https://en.wikipedia.org/w/index.php?title=Special:UserLogin&amp;returnto=Nonlinear+dimensionality+reduction&amp;returntoquery=oldid%3D848551806" title="You're encouraged to log in; however, it's not mandatory. [Alt+Shift+o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><span><a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction" title="View the content page [Alt+Shift+c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="https://en.wikipedia.org/wiki/Talk:Nonlinear_dimensionality_reduction" rel="discussion" title="Discussion about the content page [Alt+Shift+t]" accesskey="t">Talk</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input class="vectorMenuCheckbox" aria-labelledby="p-variants-label" type="checkbox">
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=edit" title="Edit this page [Alt+Shift+e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=history" title="Past revisions of this page [Alt+Shift+h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label" style="">
						<input class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" type="checkbox">
						<h3 id="p-cactions-label"><span>More</span></h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input name="search" placeholder="Search Wikipedia" title="Search Wikipedia [Alt+Shift+f]" accesskey="f" id="searchInput" tabindex="1" autocomplete="off" type="search"><input value="Special:Search" name="title" type="hidden"><input name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton" type="submit">							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page [Alt+Shift+z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="https://en.wikipedia.org/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="https://en.wikipedia.org/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="https://en.wikipedia.org/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="https://en.wikipedia.org/wiki/Special:Random" title="Load a random article [Alt+Shift+x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="https://shop.wikimedia.org/" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="https://en.wikipedia.org/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="https://en.wikipedia.org/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="https://en.wikipedia.org/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [Alt+Shift+r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="https://en.wikipedia.org/wiki/Special:WhatLinksHere/Nonlinear_dimensionality_reduction" title="List of all English Wikipedia pages containing links to this page [Alt+Shift+j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Nonlinear_dimensionality_reduction" rel="nofollow" title="Recent changes in pages linked from this page [Alt+Shift+k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [Alt+Shift+u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="https://en.wikipedia.org/wiki/Special:SpecialPages" title="A list of all special pages [Alt+Shift+q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;oldid=848551806" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q7049464" title="Link to connected data repository item [Alt+Shift+g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=Nonlinear_dimensionality_reduction&amp;id=848551806" title="Information on how to cite this page">Cite this page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Nonlinear+dimensionality+reduction">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="https://en.wikipedia.org/w/index.php?title=Special:ElectronPdf&amp;page=Nonlinear+dimensionality+reduction&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;printable=yes" title="Printable version of this page [Alt+Shift+p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label"><button class="uls-settings-trigger" title="Language settings"></button>
			<h3 id="p-lang-label">Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%DA%A9%D8%A7%D9%87%D8%B4_%D8%BA%DB%8C%D8%B1%D8%AE%D8%B7%DB%8C_%D8%A7%D8%A8%D8%B9%D8%A7%D8%AF" title="کاهش غیرخطی ابعاد – Persian" hreflang="fa" class="interlanguage-link-target" lang="fa">فارسی</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E9%9D%9E%E7%BA%BF%E6%80%A7%E9%99%8D%E7%BB%B4" title="非线性降维 – Chinese" hreflang="zh" class="interlanguage-link-target" lang="zh">中文</a></li>				</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q7049464#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 2 July 2018, at 17:20<span class="anonymous-show">&nbsp;(UTC)</span>.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="https://wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="https://wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="https://www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="https://en.m.wikipedia.org/w/index.php?title=Nonlinear_dimensionality_reduction&amp;oldid=848551806&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							<li style="display: none;"><a href="#">Enable previews</a></li></ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://wikimediafoundation.org/"><img src="wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" alt="Wikimedia Foundation" width="88" height="31"></a>					</li>
										<li id="footer-poweredbyico">
						<a href="https://www.mediawiki.org/"><img src="poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.300","walltime":"0.456","ppvisitednodes":{"value":1571,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":10378,"limit":2097152},"templateargumentsize":{"value":990,"limit":2097152},"expansiondepth":{"value":16,"limit":40},"expensivefunctioncount":{"value":0,"limit":500},"unstrip-depth":{"value":0,"limit":20},"unstrip-size":{"value":25742,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  156.818      1 -total"," 70.41%  110.415      1 Template:Reflist"," 26.75%   41.942      3 Template:Cite_journal"," 19.84%   31.120      2 Template:ISBN"," 16.40%   25.723      1 Template:See_also","  8.46%   13.265      2 Template:Catalog_lookup_link","  4.33%    6.787      1 Template:Cite_conference","  3.98%    6.240      2 Template:Error-small","  2.87%    4.493      6 Template:Yesno-no","  2.70%    4.241      1 Template:Main"]},"scribunto":{"limitreport-timeusage":{"value":"0.042","limit":"10.000"},"limitreport-memusage":{"value":2659682,"limit":52428800}},"cachereport":{"origin":"mw1262","timestamp":"20180801011444","ttl":1900800,"transientcontent":false}}});mw.config.set({"wgBackendResponseTime":116,"wgHostname":"mw1250"});});</script>
	

<div style="display: none; font-size: 13px;" class="suggestions"><div class="suggestions-results"></div><div class="suggestions-special"></div></div><a accesskey="v" href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction?action=edit" class="oo-ui-element-hidden"></a><div id="mwe-popups-svg"><svg xmlns="http://www.w3.org/2000/svg" width="0" height="0"><defs><clipPath id="mwe-popups-mask"><polygon points="0 8, 10 8, 18 0, 26 8, 1000 8, 1000 1000, 0 1000"></polygon></clipPath><clipPath id="mwe-popups-mask-flip"><polygon points="0 8, 274 8, 282 0, 290 8, 1000 8, 1000 1000, 0 1000"></polygon></clipPath><clipPath id="mwe-popups-landscape-mask"><polygon points="0 8, 174 8, 182 0, 190 8, 1000 8, 1000 1000, 0 1000"></polygon></clipPath><clipPath id="mwe-popups-landscape-mask-flip"><polygon points="0 0, 1000 0, 1000 242, 190 242, 182 250, 174 242, 0 242"></polygon></clipPath></defs></svg></div></body></html>