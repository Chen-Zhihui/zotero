<!DOCTYPE html>
<html class="client-js" dir="ltr" lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<meta charset="UTF-8">
<title>Entropy (information theory) - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Entropy_(information_theory)","wgTitle":"Entropy (information theory)","wgCurRevisionId":852254681,"wgRevisionId":852254681,"wgArticleId":15445,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 maint: Uses authors parameter","Wikipedia articles that are too technical from November 2017","All articles that are too technical","Articles needing expert attention from November 2017","All articles needing expert attention","Articles needing additional references from April 2012","All articles needing additional references","Articles with multiple maintenance issues","Use dmy dates from July 2013","All articles with unsourced statements","Articles with unsourced statements from May 2018","Articles with unsourced statements from April 2013","Articles with unsourced quotes","Wikipedia articles needing clarification from July 2014","Wikipedia articles incorporating text from PlanetMath","Wikipedia external links cleanup from June 2015","Wikipedia spam cleanup from June 2015","Wikipedia articles with BNE identifiers","Wikipedia articles with BNF identifiers","Wikipedia articles with GND identifiers","Wikipedia articles with LCCN identifiers","Wikipedia articles with NDL identifiers","Entropy and information","Information theory","Statistical randomness"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Entropy_(information_theory)","wgRelevantArticleId":15445,"wgRequestId":"W1-ydgpAIC0AABAalrgAAAAC","wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikiEditorEnabledModules":[],"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgMFExpandAllSectionsUserOption":true,"wgMFEnableFontChanger":true,"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q204570","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":true,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","jquery.makeCollapsible.styles":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"});mw.loader.implement("user.tokens@1dqfd7l",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});mw.loader.load(["ext.cite.a11y","ext.math.scripts","site","mediawiki.page.startup","mediawiki.user","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.compactlinks","ext.uls.interface","ext.3d","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});</script>
<link rel="stylesheet" href="load.css">
<script async="" src="load_002.php"></script>
<style>
.cite-accessibility-label{ top:-99999px;clip:rect(1px 1px 1px 1px); clip:rect(1px,1px,1px,1px); position:absolute !important;padding:0 !important;border:0 !important;height:1px !important;width:1px !important; overflow:hidden}
@media screen {
	.tochidden,.toctoggle{-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}.toctoggle{font-size:94%}}
@media print {
	.toc.tochidden,.toctoggle{display:none}}
.suggestions{overflow:hidden;position:absolute;top:0;left:0;width:0;border:0;z-index:1099;padding:0;margin:-1px 0 0 0}.suggestions-special{position:relative;background-color:#fff;cursor:pointer;border:1px solid #a2a9b1;margin:0;margin-top:-2px;display:none;padding:0.25em 0.25em;line-height:1.25em}.suggestions-results{background-color:#fff;cursor:pointer;border:1px solid #a2a9b1;padding:0;margin:0}.suggestions-result{color:#000;margin:0;line-height:1.5em;padding:0.01em 0.25em;text-align:left; overflow:hidden;text-overflow:ellipsis;white-space:nowrap}.suggestions-result-current{background-color:#2a4b8d;color:#fff}.suggestions-special .special-label{color:#72777d;text-align:left}.suggestions-special .special-query{color:#000;font-style:italic;text-align:left}.suggestions-special .special-hover{background-color:#c8ccd1}.suggestions-result-current .special-label,.suggestions-result-current .special-query{color:#fff}.highlight{font-weight:bold}
.wp-teahouse-question-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}#wp-th-question-ask{float:right}.wp-teahouse-ask a.external{background-image:none !important}.wp-teahouse-respond-form{position:absolute;margin-left:auto;margin-right:auto;background-color:#f4f3f0;border:1px solid #a7d7f9;padding:1em}.wp-th-respond{float:right}.wp-teahouse-respond a.external{background-image:none !important}
.referencetooltip{position:absolute;list-style:none;list-style-image:none;opacity:0;font-size:12px;margin:0;z-index:5;padding:0}.referencetooltip > li{background:#fff;border:1px solid #bbb;-webkit-box-shadow:0 0 10px rgba(0,0,0,0.2);-moz-box-shadow:0 0 10px rgba(0,0,0,0.2);box-shadow:0 0 10px rgba(0,0,0,0.2);margin:0;padding:8px 10px;line-height:18px;max-width:300px}.referencetooltip > li + li{box-sizing:border-box;margin-left:7px;margin-top:-1px;border:0;padding:0;height:3px;width:0;background-color:transparent;-webkit-box-shadow:none;-moz-box-shadow:none;box-shadow:none;border-top:12px #bbb solid;border-right:7px transparent solid;border-left:7px transparent solid}.referencetooltip > li + li::after{z-index:111;content:'';border:6px solid transparent;border-bottom:0;border-top:8px solid #fff;height:0;width:0;display:block;margin-left:-6px;margin-top:-12px}.RTflipped{padding-top:13px}.referencetooltip.RTflipped > li + li{position:absolute;top:0;border-top:0;border-bottom:12px #bbb solid}.referencetooltip.RTflipped > li + li::after{border-top:0;border-bottom:8px #fff solid;position:absolute;margin-top:7px}.RTsettings{ background-image:linear-gradient(transparent,transparent),url(data:image/svg+xml,%3C%3Fxml%20version%3D%221.0%22%20encoding%3D%22utf-8%22%3F%3E%0D%0A%3Csvg%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%20viewBox%3D%220%200%2024%2024%22%3E%0D%0A%20%20%20%20%3Cpath%20fill%3D%22%23555%22%20d%3D%22M20%2014.5v-2.9l-1.8-.3c-.1-.4-.3-.8-.6-1.4l1.1-1.5-2.1-2.1-1.5%201.1c-.5-.3-1-.5-1.4-.6L13.5%205h-2.9l-.3%201.8c-.5.1-.9.3-1.4.6L7.4%206.3%205.3%208.4l1%201.5c-.3.5-.4.9-.6%201.4l-1.7.2v2.9l1.8.3c.1.5.3.9.6%201.4l-1%201.5%202.1%202.1%201.5-1c.4.2.9.4%201.4.6l.3%201.8h3l.3-1.8c.5-.1.9-.3%201.4-.6l1.5%201.1%202.1-2.1-1.1-1.5c.3-.5.5-1%20.6-1.4l1.5-.3zM12%2016c-1.7%200-3-1.3-3-3s1.3-3%203-3%203%201.3%203%203-1.3%203-3%203z%22%2F%3E%0D%0A%3C%2Fsvg%3E);  display:block;float:right;cursor:pointer;margin:0;margin-top:-4px;height:24px;width:24px;border-radius:2px;box-sizing:border-box;background-position:center center;background-repeat:no-repeat;background-size:24px 24px;margin-left:8px}.RTsettings:hover{background-color:#eee}.RTTarget{background-color:#def}
@-webkit-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-webkit-transform:translateY(-20px)}100%{opacity:1;-webkit-transform:translateY(0)}}@-moz-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-moz-transform:translateY(-20px)}100%{opacity:1;-moz-transform:translateY(0)}}@-o-keyframes centralAuthPPersonalAnimation{0%{opacity:0;-o-transform:translateY(-20px)}100%{opacity:1;-o-transform:translateY(0)}}@keyframes centralAuthPPersonalAnimation{0%{opacity:0;transform:translateY(-20px)}100%{opacity:1;transform:translateY(0)}}.centralAuthPPersonalAnimation{-webkit-animation-duration:1s;-moz-animation-duration:1s;-o-animation-duration:1s;animation-duration:1s;-webkit-animation-fill-mode:both;-moz-animation-fill-mode:both;-o-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-name:centralAuthPPersonalAnimation;-moz-animation-name:centralAuthPPersonalAnimation;-o-animation-name:centralAuthPPersonalAnimation;animation-name:centralAuthPPersonalAnimation}
.mw-ui-button{font-family:inherit;font-size:1em;display:inline-block;min-width:4em;max-width:28.75em;padding:0.546875em 1em;line-height:1.286;margin:0;border-radius:2px;-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box;-webkit-appearance:none;*display:inline; zoom:1;vertical-align:middle;background-color:#f8f9fa;color:#222222;border:1px solid #a2a9b1;text-align:center;font-weight:bold;cursor:pointer}.mw-ui-button:visited{color:#222222}.mw-ui-button:hover{background-color:#ffffff;color:#444444;border-color:#a2a9b1}.mw-ui-button:focus{background-color:#ffffff;color:#222222;border-color:#3366cc;box-shadow:inset 0 0 0 1px #3366cc,inset 0 0 0 2px #ffffff}.mw-ui-button:active,.mw-ui-button.is-on,.mw-ui-button.mw-ui-checked{background-color:#d9d9d9;color:#000000;border-color:#72777d;box-shadow:none}.mw-ui-button:disabled{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1}.mw-ui-button:disabled:hover,.mw-ui-button:disabled:active{background-color:#c8ccd1;color:#fff;box-shadow:none;border-color:#c8ccd1}.mw-ui-button:focus{outline-width:0}.mw-ui-button:focus::-moz-focus-inner{border-color:transparent;padding:0}.mw-ui-button:not(:disabled){-webkit-transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms;-moz-transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms;transition:background-color 100ms,color 100ms,border-color 100ms,box-shadow 100ms}.mw-ui-button:disabled{text-shadow:none;cursor:default}.mw-ui-button.mw-ui-big{font-size:1.3em}.mw-ui-button.mw-ui-block{display:block;width:100%;margin-left:auto;margin-right:auto}.mw-ui-button.mw-ui-progressive{background-color:#3366cc;color:#fff;border:1px solid #3366cc}.mw-ui-button.mw-ui-progressive:hover{background-color:#447ff5;border-color:#447ff5}.mw-ui-button.mw-ui-progressive:focus{box-shadow:inset 0 0 0 1px #3366cc,inset 0 0 0 2px #ffffff}.mw-ui-button.mw-ui-progressive:active,.mw-ui-button.mw-ui-progressive.is-on,.mw-ui-button.mw-ui-progressive.mw-ui-checked{background-color:#2a4b8d;border-color:#2a4b8d;box-shadow:none}.mw-ui-button.mw-ui-progressive:disabled{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1}.mw-ui-button.mw-ui-progressive:disabled:hover,.mw-ui-button.mw-ui-progressive:disabled:active,.mw-ui-button.mw-ui-progressive:disabled.mw-ui-checked{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1;box-shadow:none}.mw-ui-button.mw-ui-progressive.mw-ui-quiet{color:#222222}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:hover{background-color:transparent;color:#447ff5}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:active,.mw-ui-button.mw-ui-progressive.mw-ui-quiet.mw-ui-checked{color:#2a4b8d}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:focus{background-color:transparent;color:#3366cc}.mw-ui-button.mw-ui-progressive.mw-ui-quiet:disabled{color:#c8ccd1}.mw-ui-button.mw-ui-destructive{background-color:#dd3333;color:#fff;border:1px solid #dd3333}.mw-ui-button.mw-ui-destructive:hover{background-color:#ff4242;border-color:#ff4242}.mw-ui-button.mw-ui-destructive:focus{box-shadow:inset 0 0 0 1px #dd3333,inset 0 0 0 2px #ffffff}.mw-ui-button.mw-ui-destructive:active,.mw-ui-button.mw-ui-destructive.is-on,.mw-ui-button.mw-ui-destructive.mw-ui-checked{background-color:#b32424;border-color:#b32424;box-shadow:none}.mw-ui-button.mw-ui-destructive:disabled{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1}.mw-ui-button.mw-ui-destructive:disabled:hover,.mw-ui-button.mw-ui-destructive:disabled:active,.mw-ui-button.mw-ui-destructive:disabled.mw-ui-checked{background-color:#c8ccd1;color:#fff;border-color:#c8ccd1;box-shadow:none}.mw-ui-button.mw-ui-destructive.mw-ui-quiet{color:#222222}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:hover{background-color:transparent;color:#ff4242}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:active,.mw-ui-button.mw-ui-destructive.mw-ui-quiet.mw-ui-checked{color:#b32424}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:focus{background-color:transparent;color:#dd3333}.mw-ui-button.mw-ui-destructive.mw-ui-quiet:disabled{color:#c8ccd1}.mw-ui-button.mw-ui-quiet{background:transparent;border:0;text-shadow:none;color:#222222}.mw-ui-button.mw-ui-quiet:hover{background-color:transparent;color:#444444}.mw-ui-button.mw-ui-quiet:active,.mw-ui-button.mw-ui-quiet.mw-ui-checked{color:#000000}.mw-ui-button.mw-ui-quiet:focus{background-color:transparent;color:#222222}.mw-ui-button.mw-ui-quiet:disabled{color:#c8ccd1}.mw-ui-button.mw-ui-quiet:hover,.mw-ui-button.mw-ui-quiet:focus{box-shadow:none}.mw-ui-button.mw-ui-quiet:active,.mw-ui-button.mw-ui-quiet:disabled{background:transparent}input.mw-ui-button::-moz-focus-inner,button.mw-ui-button::-moz-focus-inner{margin-top:-1px;margin-bottom:-1px}a.mw-ui-button{text-decoration:none}a.mw-ui-button:hover,a.mw-ui-button:focus{text-decoration:none}.mw-ui-button-group > *{min-width:48px;border-radius:0;float:left}.mw-ui-button-group > *:first-child{border-top-left-radius:2px;border-bottom-left-radius:2px}.mw-ui-button-group > *:not(:first-child){border-left:0}.mw-ui-button-group > *:last-child{border-top-right-radius:2px;border-bottom-right-radius:2px}.mw-ui-button-group .is-on .button{cursor:default}
.mw-ui-icon{position:relative;line-height:1.5em;min-height:1.5em;min-width:1.5em}span.mw-ui-icon{display:inline-block}.mw-ui-icon.mw-ui-icon-element{text-indent:-999px;overflow:hidden;width:3.5em;min-width:3.5em;max-width:3.5em}.mw-ui-icon.mw-ui-icon-element:before{left:0;right:0;position:absolute;margin:0 1em}.mw-ui-icon.mw-ui-icon-element.mw-ui-icon-large{width:4.625em;min-width:4.625em;max-width:4.625em;line-height:4.625em;min-height:4.625em}.mw-ui-icon.mw-ui-icon-element.mw-ui-icon-large:before{min-height:4.625em}.mw-ui-icon.mw-ui-icon-before:before,.mw-ui-icon.mw-ui-icon-element:before{background-position:50% 50%;background-repeat:no-repeat;background-size:100% auto;float:left;display:block;min-height:1.5em;content:''}.mw-ui-icon.mw-ui-icon-before:before{position:relative;width:1.5em;margin-right:1em}.mw-ui-icon.mw-ui-icon-small:before{background-size:66.67% auto}
.mw-editfont-monospace{font-family:monospace,monospace}.mw-editfont-sans-serif{font-family:sans-serif}.mw-editfont-serif{font-family:serif} .mw-editfont-monospace,.mw-editfont-sans-serif,.mw-editfont-serif{font-size:13px; }.mw-editfont-monospace.oo-ui-textInputWidget,.mw-editfont-sans-serif.oo-ui-textInputWidget,.mw-editfont-serif.oo-ui-textInputWidget{font-size:inherit}.mw-editfont-monospace > .oo-ui-inputWidget-input,.mw-editfont-sans-serif > .oo-ui-inputWidget-input,.mw-editfont-serif > .oo-ui-inputWidget-input{font-size:13px}
.uls-menu{border-radius:2px; font-size:medium}.uls-search,.uls-language-settings-close-block{border-top-right-radius:2px;border-top-left-radius:2px}.uls-language-list{border-bottom-right-radius:2px;border-bottom-left-radius:2px}.uls-menu.callout:before,.uls-menu.callout:after{border-top:10px solid transparent;border-bottom:10px solid transparent;display:inline-block; top:17px;position:absolute;content:''}.uls-menu.callout.selector-right:before{ border-left:10px solid #c8ccd1; right:-11px}.uls-menu.callout.selector-right:after{ border-left:10px solid #f8f9fa; right:-10px}.uls-menu.callout.selector-left:before{ border-right:10px solid #c8ccd1; left:-11px}.uls-menu.callout.selector-left:after{ border-right:10px solid #f8f9fa; left:-10px}.uls-ui-languages button{margin:5px 15px 5px 0;white-space:nowrap;overflow:hidden}.uls-search-wrapper-wrapper{position:relative;padding-left:40px;margin-top:5px;margin-bottom:5px}.uls-icon-back{background:transparent url(/w/extensions/UniversalLanguageSelector/resources/images/back-grey-ltr.png?90e9b) no-repeat scroll center center;background-image:-webkit-linear-gradient(transparent,transparent),url(/w/extensions/UniversalLanguageSelector/resources/images/back-grey-ltr.svg?e226b);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2224%22 height=%2224%22 viewBox=%220 0 24 24%22%3E %3Cpath fill=%22%2354595d%22 d=%22M7 13.1l8.9 8.9c.8-.8.8-2 0-2.8l-6.1-6.1 6-6.1c.8-.8.8-2 0-2.8L7 13.1z%22/%3E %3C/svg%3E");background-size:28px;background-position:center center;height:32px;width:40px;display:block;position:absolute;left:0;border-right:1px solid #c8ccd1;opacity:0.8}.uls-icon-back:hover{opacity:1;cursor:pointer}.uls-menu .uls-no-results-view .uls-no-found-more{background-color:#fff}.uls-menu .uls-no-results-view h3{padding:0 28px;margin:0;color:#54595d;font-size:1em;font-weight:normal}  .skin-vector .uls-menu{border-color:#c8ccd1;-webkit-box-shadow:0 2px 2px 0 rgba(0,0,0,0.25);box-shadow:0 2px 2px 0 rgba(0,0,0,0.25);font-size:0.875em}.skin-vector .uls-search{border-bottom-color:#c8ccd1}.skin-vector .uls-filtersuggestion{color:#72777d}.skin-vector .uls-lcd-region-title{color:#54595d}
.mw-spinner{background-color:transparent;background-position:center center;background-repeat:no-repeat}.mw-spinner-small{background-image:url(data:image/gif;base64,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);background-image:url(/w/resources/src/jquery.spinner/images/spinner.gif?ca65b)!ie;height:20px;width:20px; min-width:20px}.mw-spinner-large{background-image:url(data:image/gif;base64,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);background-image:url(/w/resources/src/jquery.spinner/images/spinner-large.gif?57f34)!ie;height:32px;width:32px; min-width:32px}.mw-spinner-block{display:block; width:100%}.mw-spinner-inline{display:inline-block;vertical-align:middle}
@media print{#centralNotice{display:none}}.cn-closeButton{display:inline-block;zoom:1;background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUBAMAAAB/pwA+AAAAElBMVEUAAAAQEBDPz88AAABAQEDv7+9oe1vvAAAABnRSTlMA3rLe3rJS22KzAAAARElEQVQI12PAAUIUQCSTK5BwFgIxFU1AhKECUFAYKAAioXwwBeZChMGCEGGQIFQYJohgIhQgtCEMQ7ECYTHCOciOxA4AADgJTXIb9s8AAAAASUVORK5CYII=) no-repeat;background:url(/w/extensions/CentralNotice/resources/subscribing/close.png?8e3d8) no-repeat!ie;width:20px;height:20px;text-indent:20px;white-space:nowrap;overflow:hidden}</style><style>
.suggestions a.mw-searchSuggest-link,.suggestions a.mw-searchSuggest-link:hover,.suggestions a.mw-searchSuggest-link:active,.suggestions a.mw-searchSuggest-link:focus{color:#000;text-decoration:none}.suggestions-result-current a.mw-searchSuggest-link,.suggestions-result-current a.mw-searchSuggest-link:hover,.suggestions-result-current a.mw-searchSuggest-link:active,.suggestions-result-current a.mw-searchSuggest-link:focus{color:#fff}.suggestions a.mw-searchSuggest-link .special-query{ overflow:hidden;text-overflow:ellipsis;white-space:nowrap}
.mw-mmv-overlay{position:fixed;top:0;left:0;right:0;bottom:0;z-index:1000;background-color:#000}body.mw-mmv-lightbox-open{overflow-y:auto;  }body.mw-mmv-lightbox-open #mw-page-base,body.mw-mmv-lightbox-open #mw-head-base,body.mw-mmv-lightbox-open #mw-navigation,body.mw-mmv-lightbox-open #content,body.mw-mmv-lightbox-open #footer,body.mw-mmv-lightbox-open #globalWrapper{ display:none}body.mw-mmv-lightbox-open > *{ display:none}body.mw-mmv-lightbox-open > .mw-mmv-overlay,body.mw-mmv-lightbox-open > .mw-mmv-wrapper{display:block}.mw-mmv-filepage-buttons{margin-top:5px}.mw-mmv-filepage-buttons .mw-mmv-view-expanded,.mw-mmv-filepage-buttons .mw-mmv-view-config{display:block;line-height:inherit}.mw-mmv-filepage-buttons .mw-mmv-view-expanded.mw-ui-icon:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 1024 768%22%3E %3Cpath d=%22M851.2 71.6L690.7 232.1l-40.1-40.3-9.6 164.8 164.8-9.3-40.3-40.4L926 146.4l58.5 58.5L997.6 0 792.7 13.1%22/%3E %3Cpath d=%22M769.6 89.3H611.9l70.9 70.8 7.9 7.5m-47.1 234.6l-51.2 3 3-51.2 9.4-164.4 5.8-100.3H26.4V768h883.1V387l-100.9 5.8-165 9.4zM813.9 678H113.6l207.2-270.2 31.5-12.9L548 599.8l105.9-63.2 159.8 140.8.2.6zm95.6-291.9V228l-79.1 78.9 7.8 7.9%22/%3E %3C/svg%3E")}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before{background-image:url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 1024 768%22%3E %3Cpath d=%22M897 454.6V313.4L810.4 299c-6.4-23.3-16-45.7-27.3-65.8l50.5-71.4-99.4-100.2-71.4 50.5c-20.9-11.2-42.5-20.9-65.8-27.3L582.6-1H441.4L427 85.6c-23.3 6.4-45.7 16-65.8 27.3l-71.4-50.5-100.3 99.5 50.5 71.4c-11.2 20.9-20.9 42.5-27.3 66.6L127 313.4v141.2l85.8 14.4c6.4 23.3 16 45.7 27.3 66.6L189.6 607l99.5 99.5 71.4-50.5c20.9 11.2 42.5 20.9 66.6 27.3l14.4 85.8h141.2l14.4-86.6c23.3-6.4 45.7-16 65.8-27.3l71.4 50.5 99.5-99.5-50.5-71.4c11.2-20.9 20.9-42.5 27.3-66.6l86.4-13.6zm-385 77c-81.8 0-147.6-66.6-147.6-147.6 0-81.8 66.6-147.6 147.6-147.6S659.6 302.2 659.6 384 593.8 531.6 512 531.6z%22/%3E %3C/svg%3E");opacity:0.75}.mw-mmv-filepage-buttons .mw-mmv-view-config.mw-ui-icon:before:hover{opacity:1}.mw-mmv-button{background-color:transparent;min-width:0;border:0;padding:0;overflow-x:hidden;text-indent:-9999em}
.ve-init-mw-tempWikitextEditorWidget{border:0;padding:0;color:inherit;line-height:1.5em; }.ve-init-mw-tempWikitextEditorWidget:focus{outline:0;padding:0}.ve-init-mw-tempWikitextEditorWidget::selection{background:rgba(109,169,247,0.5); }
#uls-settings-block{background-color:#f8f9fa;border-top:1px solid #c8ccd1;padding-left:10px;line-height:1.2em;border-radius:0 0 2px 2px}#uls-settings-block > button{background:left top transparent no-repeat;background-size:20px auto;color:#54595d;display:inline-block;margin:8px 15px;border:0;padding:0 0 0 26px;font-size:medium;cursor:pointer}#uls-settings-block > button:hover{color:#222}#uls-settings-block > button.display-settings-block{background-image:url(/w/extensions/UniversalLanguageSelector/resources/images/display.png?d25f1);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Cpath fill=%22%23222%22 d=%22M.002 2.275V15.22h8.405c.535 1.624-.975 1.786-1.902 2.505 0 0 2.293-.024 3.439-.024 1.144 0 3.432.024 3.432.024-.905-.688-2.355-.868-1.902-2.505h8.527V2.275h-20zm6.81 1.84h.797l3.313 8.466H9.879L8.836 9.943H5.462l-1.043 2.638h-.982zm.368 1.104c-.084.369-.211.785-.368 1.227L5.83 9.023h2.699l-.982-2.577c-.128-.33-.234-.747-.368-1.227zm7.117.982c.753 0 1.295.157 1.656.491.365.334.552.858.552 1.595v4.294h-.675l-.184-.859h-.062c-.315.396-.605.655-.92.798-.311.138-.758.184-1.227.184-.626 0-1.115-.168-1.472-.491-.353-.323-.491-.754-.491-1.35 0-1.275 1.028-1.963 3.068-2.025h1.043v-.429c0-.495-.091-.87-.307-1.104-.211-.238-.574-.307-1.043-.307-.526 0-1.115.107-1.779.429l-.307-.675a4.748 4.748 0 0 1 1.043-.429 4.334 4.334 0 0 1 1.104-.123zm.307 3.313c-.761.027-1.318.157-1.656.368-.334.207-.491.54-.491.982 0 .346.1.617.307.798.211.181.544.245.92.245.595 0 1.012-.164 1.35-.491.342-.326.552-.762.552-1.35v-.552z%22/%3E %3C/svg%3E")}#uls-settings-block > button.input-settings-block{background-image:url(/w/extensions/UniversalLanguageSelector/resources/images/input.png?aea9e);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Cpath fill=%22%23222%22 d=%22M9 1.281c-.124.259-.185.599-.5.688-.55.081-1.133.018-1.688 0-.866-.032-1.733-.148-2.594 0-.588.157-.953.727-1.188 1.25-.178.416-.271.836-.344 1.281H-.002V16h20V4.5H3.654c.109-.52.203-1.057.563-1.469.222-.231.587-.17.875-.188 1.212.003 2.415.179 3.625.063.463-.058.812-.455.969-.875l.188-.438-.875-.313zM1.875 7.125h1.563c.094 0 .188.093.188.188v1.531a.201.201 0 0 1-.188.188H1.875c-.094 0-.156-.093-.156-.188V7.313c0-.094.062-.188.156-.188zm2.844 0h1.563c.094 0 .156.093.156.188v1.531c0 .094-.062.188-.156.188H4.719c-.094 0-.156-.093-.156-.188V7.313c0-.094.062-.188.156-.188zm2.844 0h1.563c.094 0 .156.093.156.188v1.531c0 .094-.062.188-.156.188H7.563a.201.201 0 0 1-.188-.188V7.313c0-.094.093-.188.188-.188zm2.813 0h1.563c.094 0 .188.093.188.188v1.531a.201.201 0 0 1-.188.188h-1.563c-.094 0-.156-.093-.156-.188V7.313c0-.094.062-.188.156-.188zm2.844 0h1.563c.094 0 .156.093.156.188v1.531c0 .094-.062.188-.156.188H13.22c-.094 0-.156-.093-.156-.188V7.313c0-.094.062-.188.156-.188zm2.844 0h1.531c.094 0 .188.093.188.188v1.531a.201.201 0 0 1-.188.188h-1.531a.201.201 0 0 1-.188-.188V7.313c0-.094.093-.188.188-.188zm-12.844 3h1.563c.094 0 .156.093.156.188v1.563c0 .094-.062.156-.156.156H3.22c-.094 0-.156-.062-.156-.156v-1.563c0-.094.062-.188.156-.188zm2.906 0h1.563c.094 0 .188.093.188.188v1.563c0 .094-.093.156-.188.156H6.126c-.094 0-.156-.062-.156-.156v-1.563c0-.094.062-.188.156-.188zm2.938 0h1.531c.094 0 .188.093.188.188v1.563c0 .094-.093.156-.188.156H9.064c-.094 0-.188-.062-.188-.156v-1.563c0-.094.093-.188.188-.188zm2.906 0h1.563c.094 0 .156.093.156.188v1.563c0 .094-.062.156-.156.156H11.97c-.094 0-.188-.062-.188-.156v-1.563c0-.094.093-.188.188-.188zm2.906 0h1.563c.094 0 .156.093.156.188v1.563c0 .094-.062.156-.156.156h-1.563c-.094 0-.156-.062-.156-.156v-1.563c0-.094.062-.188.156-.188zM4.001 13.688h12c.088 0 .156.068.156.156v.844a.154.154 0 0 1-.156.156h-12a.154.154 0 0 1-.156-.156v-.844c0-.088.068-.156.156-.156z%22/%3E %3C/svg%3E")}
#p-lang .body ul .uls-trigger,#p-lang .pBody ul .uls-trigger{background-image:none;padding:0} .mw-interlanguage-selector,.mw-interlanguage-selector:active{cursor:pointer;padding:4px 6px 4px 25px;font-size:13px;font-weight:normal;background-image:url(/w/extensions/UniversalLanguageSelector/resources/images/compact-links-trigger.png?b0c8e);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2218%22 height=%2218%22 viewBox=%220 0 24 24%22%3E %3Cpath fill=%22%2372777d%22 d=%22M13 19l.8-3h5.3l.9 3h2.2L18 6h-3l-4.2 13H13zm3.5-11l2 6h-4l2-6zM5 4l.938 1.906H1V8h1.594C3.194 9.8 4 11.206 5 12.406c-1.1.7-4.313 1.781-4.313 1.781L2 16s3.487-1.387 4.688-2.188c1 .7 2.319 1.188 3.719 1.688l.594-2c-1-.3-1.988-.688-2.688-1.188 1.1-1.1 1.9-2.506 2.5-4.406h2.188l.5-2H7.938L7 4H5zm-.188 4h3.781c-.4 1.3-.906 2-1.906 3-1.1-1-1.475-1.7-1.875-3z%22/%3E %3C/svg%3E");background-size:18px;background-repeat:no-repeat;background-position:left 4px center;margin:4px 0;text-align:left}.mw-interlanguage-selector:active,.mw-interlanguage-selector.selector-open{background-color:#c8ccd1;color:#54595d}.interlanguage-uls-menu:before,.interlanguage-uls-menu:after{border-top:10px solid transparent;border-bottom:10px solid transparent;display:inline-block; top:17px;position:absolute;content:''}.interlanguage-uls-menu.selector-right:before{ border-left:10px solid #c8ccd1; right:-11px}.interlanguage-uls-menu.selector-right:after{ border-left:10px solid #f8f9fa; right:-10px}.interlanguage-uls-menu.selector-left:before{ border-right:10px solid #c8ccd1; left:-11px}.interlanguage-uls-menu.selector-left:after{ border-right:10px solid #f8f9fa; left:-10px}
.mw-3d-wrapper{display:inline-block;position:relative;overflow:hidden;vertical-align:top}.mw-3d-badge{position:absolute;top:11px;left:11px;color:#1e1f21;font-size:14px;line-height:19px;font-weight:bold;opacity:0.8;padding:2px 5px;background-color:#f8f9fa;border-radius:2px}.mw-3d-thumb-placeholder{display:inline-block;text-decoration:none;color:#222}
.mw-collapsible-toggle{float:right;-moz-user-select:none;-webkit-user-select:none;-ms-user-select:none;user-select:none}.mw-collapsible-toggle-default:before{content:'['}.mw-collapsible-toggle-default:after{content:']'}.mw-customtoggle,.mw-collapsible-toggle{cursor:pointer} caption .mw-collapsible-toggle,.mw-content-ltr caption .mw-collapsible-toggle,.mw-content-rtl caption .mw-collapsible-toggle,.mw-content-rtl .mw-content-ltr caption .mw-collapsible-toggle,.mw-content-ltr .mw-content-rtl caption .mw-collapsible-toggle{float:none}
.mw-ui-icon-popups-settings:before{background-image:url(/w/load.php?modules=ext.popups.images&image=popups-settings&format=rasterized&lang=en&skin=vector&version=07s8wyu);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg width=%2220px%22 height=%2220px%22 viewbox=%220 0 20 20%22 xmlns=%22http://www.w3.org/2000/svg%22%3E %3Cg fill=%22%2354595D%22%3E %3Cpath d=%22M10.112 4.554a5.334 5.334 0 1 0 0 10.668 5.334 5.334 0 0 0 0-10.668zm0 7.823a2.49 2.49 0 1 1 0-4.978 2.49 2.49 0 0 1 0 4.978z%22/%3E %3Cpath d=%22M11.4 5.303L11.05 3h-2.1L8.6 5.303a4.9 4.9 0 0 1 2.8 0zm-2.8 9.394L8.95 17h2.1l.35-2.303a4.9 4.9 0 0 1-2.8 0zm5.712-7.028l1.4-1.876L14.2 4.309l-1.876 1.4a4.9 4.9 0 0 1 1.981 1.981l.007-.021zm-8.624 4.662L4.309 14.2 5.8 15.691l1.876-1.4a4.9 4.9 0 0 1-1.981-1.981l-.007.021zm9.009-.931L17 11.05v-2.1l-2.303-.35a4.9 4.9 0 0 1 0 2.8zM5.303 8.6L3 8.95v2.1l2.303.35a4.9 4.9 0 0 1 0-2.8zm7.028 5.712l1.876 1.4 1.484-1.512-1.4-1.876a4.9 4.9 0 0 1-1.981 1.981l.021.007zM7.669 5.688L5.8 4.309 4.309 5.8l1.4 1.876a4.9 4.9 0 0 1 1.96-1.988z%22/%3E %3C/g%3E %3C/svg%3E")}.mw-ui-icon-popups-close:before{background-image:url(/w/load.php?modules=ext.popups.images&image=popups-close&format=rasterized&lang=en&skin=vector&version=07s8wyu);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%220 0 20 20%22%3E %3Cpath d=%22M3.636 2.222l14.142 14.142-1.414 1.414L2.222 3.636z%22/%3E %3Cpath d=%22M17.778 3.636L3.636 17.778l-1.414-1.414L16.364 2.222z%22/%3E %3C/svg%3E")}.mw-ui-icon-preview-generic:before{background-image:url(/w/load.php?modules=ext.popups.images&image=preview-generic&format=rasterized&lang=en&skin=vector&version=07s8wyu);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg width=%2237%22 height=%2227%22 xmlns=%22http://www.w3.org/2000/svg%22%3E %3Cg id=%22Page-1%22 fill=%22none%22 fill-rule=%22evenodd%22%3E %3Cg id=%22sad-face%22 fill=%22%23C8CCD1%22%3E %3Cpath d=%22M5.475.7v20.075L0 26.25h31.025c3.102 0 5.475-2.372 5.475-5.475V.7H5.475zm20.44 4.562c1.277 0 2.19 1.095 2.19 2.19 0 1.096-.913 2.373-2.19 2.373-1.278 0-2.19-1.095-2.19-2.19s1.095-2.373 2.19-2.373zm-9.855 0c1.277 0 2.19 1.095 2.19 2.19 0 1.096-1.095 2.373-2.19 2.373s-2.19-1.095-2.19-2.19.913-2.373 2.19-2.373zm4.928 8.213c-7.153 0-8.415 7.012-8.415 7.012s2.805-1.403 8.415-1.403c5.61 0 8.414 1.403 8.414 1.403S28 13.475 20.988 13.475z%22 id=%22Shape%22/%3E %3C/g%3E %3C/g%3E %3C/svg%3E")}.mw-ui-icon-footer:before{background-image:url(/w/load.php?modules=ext.popups.images&image=footer&format=rasterized&lang=en&skin=vector&version=07s8wyu);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%22230%22 height=%22179%22 xmlns:xlink=%22http://www.w3.org/1999/xlink%22 viewBox=%220 0 230 179%22%3E %3Cdefs%3E %3Crect id=%22a%22 width=%22201%22 height=%2213%22 rx=%222%22/%3E %3Crect id=%22b%22 width=%22201%22 height=%22169%22 y=%2210%22 rx=%222%22/%3E %3Crect id=%22c%22 width=%2230%22 height=%222%22 x=%22135%22 y=%22158%22 rx=%221%22/%3E %3C/defs%3E %3Cg fill=%22none%22 fill-rule=%22evenodd%22%3E %3Cg transform=%22matrix%281 0 0 -1 0 13%29%22%3E %3Cuse fill=%22%23f8f9fa%22 xlink:href=%22%23a%22/%3E %3Crect width=%22199%22 height=%2211%22 x=%221%22 y=%221%22 stroke=%22%23a2a9b1%22 stroke-width=%222%22 rx=%222%22/%3E %3C/g%3E %3Cuse fill=%22%23fff%22 xlink:href=%22%23b%22/%3E %3Crect width=%22199%22 height=%22167%22 x=%221%22 y=%2211%22 stroke=%22%23a2a9b1%22 stroke-width=%222%22 rx=%222%22/%3E %3Cg opacity=%22.4%22 transform=%22translate%2867 35%29%22%3E %3Crect width=%2273%22 height=%222%22 y=%227%22 fill=%22%23c8ccd1%22 rx=%221%22/%3E %3Crect width=%2281%22 height=%222%22 y=%2231%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2232%22 height=%222%22 y=%2285%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2273%22 height=%222%22 x=%2235%22 y=%2285%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2217%22 height=%222%22 y=%2245%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2217%22 height=%222%22 x=%2291%22 y=%2245%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2268%22 height=%222%22 x=%2220%22 y=%2245%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2217%22 height=%222%22 y=%2278%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2237%22 height=%222%22 x=%2272%22 y=%2278%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2249%22 height=%222%22 x=%2220%22 y=%2278%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2224%22 height=%222%22 x=%2284%22 y=%2231%22 fill=%22%2372777d%22 rx=%221%22 transform=%22matrix%28-1 0 0 1 192 0%29%22/%3E %3Crect width=%2281%22 height=%222%22 y=%2266%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2214%22 height=%222%22 x=%2254%22 y=%2224%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2237%22 height=%222%22 x=%2271%22 y=%2224%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2251%22 height=%222%22 y=%2224%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2259%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2252%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2292%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%22108%22 height=%222%22 y=%2238%22 fill=%22%2372777d%22 rx=%221%22/%3E %3Crect width=%2251%22 height=%222%22 fill=%22%2372777d%22 rx=%221%22/%3E %3C/g%3E %3Crect width=%2230%22 height=%222%22 x=%2267%22 y=%22158%22 fill=%22%2372777d%22 opacity=%22.4%22 rx=%221%22/%3E %3Crect width=%2230%22 height=%222%22 x=%2299%22 y=%22158%22 fill=%22%2372777d%22 opacity=%22.4%22 rx=%221%22/%3E %3Cuse fill=%22%2336c%22 xlink:href=%22%23c%22/%3E %3Crect width=%2233%22 height=%225%22 x=%22133.5%22 y=%22156.5%22 stroke=%22%23ffc057%22 stroke-opacity=%22.447%22 stroke-width=%223%22 rx=%222.5%22/%3E %3Ccircle cx=%2234%22 cy=%2249%22 r=%2219%22 fill=%22%23eaecf0%22/%3E %3Cg fill=%22%23a2a9b1%22 transform=%22translate%285 5%29%22%3E %3Ccircle cx=%221.5%22 cy=%221.5%22 r=%221.5%22/%3E %3Ccircle cx=%226%22 cy=%221.5%22 r=%221.5%22/%3E %3Ccircle cx=%2210.5%22 cy=%221.5%22 r=%221.5%22/%3E %3C/g%3E %3Cpath stroke=%22%23ff00af%22 d=%22M174.5 159.5h54.01%22 stroke-linecap=%22square%22/%3E %3C/g%3E %3C/svg%3E")}.mw-ui-icon-preview-disambiguation:before{background-image:url(/w/load.php?modules=ext.popups.images&image=preview-disambiguation&format=rasterized&lang=en&skin=vector&version=07s8wyu);background-image:linear-gradient(transparent,transparent),url("data:image/svg+xml,%3Csvg xmlns=%22http://www.w3.org/2000/svg%22 width=%2220%22 height=%2220%22 viewBox=%222 2 20 20%22%3E %3Cpath fill=%22%23C8CCD1%22 d=%22M11 12h4V7h-4v5zm-5 2h9v-1H6v1zm0 2h9v-1H6v1zm0 2h9v-1H6v1zm4-9H6v1h4V9zm0 2H6v1h4v-1zm0-4H6v1h4V7zM4 5h13v16H7c-1.7 0-3-1.3-3-3V5z%22/%3E %3Cpath fill-rule=%22evenodd%22 fill=%22%23C8CCD1%22 d=%22M18 4v14h2V2H7v2%22/%3E %3C/svg%3E")}</style><meta name="ResourceLoaderDynamicStyles" content="">
<link rel="stylesheet" href="load_002.css">
<link rel="stylesheet" href="load_003.css">
<meta name="generator" content="MediaWiki 1.32.0-wmf.14">
<meta name="referrer" content="origin">
<meta name="referrer" content="origin-when-crossorigin">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="robots" content="noindex,nofollow">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy_flip_2_coins.jpg/1200px-Entropy_flip_2_coins.jpg">
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit">
<link rel="edit" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit">
<link rel="apple-touch-icon" href="https://en.wikipedia.org/static/apple-touch/wikipedia.png">
<link rel="shortcut icon" href="https://en.wikipedia.org/static/favicon/wikipedia.ico">
<link rel="search" type="application/opensearchdescription+xml" href="https://en.wikipedia.org/w/opensearch_desc.php" title="Wikipedia (en)">
<link rel="EditURI" type="application/rsd+xml" href="https://en.wikipedia.org/w/api.php?action=rsd">
<link rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/">
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="https://en.wikipedia.org/w/index.php?title=Special:RecentChanges&amp;feed=atom">
<link rel="canonical" href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29">
<link rel="dns-prefetch" href="https://login.wikimedia.org/">
<link rel="dns-prefetch" href="https://meta.wikimedia.org/">
<!--[if lt IE 9]><script src="/w/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
<script src="load.php"></script></head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-Entropy_information_theory rootpage-Entropy_information_theory skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>
			<div id="siteNotice" class="mw-body-content"><div id="centralNotice"></div><!-- CentralNotice --></div><div class="mw-indicators mw-body-content">
</div>
<h1 id="firstHeading" class="firstHeading" lang="en">Entropy (information theory)</h1>			<div id="bodyContent" class="mw-body-content">
				<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>				<div id="contentSub"><div class="mw-revision"><div id="mw-revision-info-current"><table id="revision-info-current" class="plainlinks fmbox fmbox-system" role="presentation"><tbody><tr><td class="mbox-text"><b>This is the <a href="https://en.wikipedia.org/wiki/Help:Page_history" title="Help:Page history">current revision</a> of this page, as edited by <span id="mw-revision-name"><a href="https://en.wikipedia.org/wiki/Special:Contributions/204.98.118.114" class="mw-userlink mw-anonuserlink" title="Special:Contributions/204.98.118.114"><bdi>204.98.118.114</bdi></a> <span class="mw-usertoollinks">(<a href="https://en.wikipedia.org/w/index.php?title=User_talk:204.98.118.114&amp;action=edit&amp;redlink=1" class="new mw-usertoollinks-talk" title="User talk:204.98.118.114 (page does not exist)">talk</a>)</span></span> at <span id="mw-revision-date">16:57, 27 July 2018</span><span id="mw-revision-summary"> <span class="comment">(<a href="#Introduction">→</a>‎<span dir="auto"><span class="autocomment">Introduction</span></span>)</span></span>. The present address (URL) is a <a href="https://en.wikipedia.org/wiki/Help:Permanent_link" title="Help:Permanent link">permanent link</a> to this version.</b></td></tr></tbody></table><div id="revision-info-current-plain" style="display: none;">Revision as of 16:57, 27 July 2018 by <a href="https://en.wikipedia.org/wiki/Special:Contributions/204.98.118.114" class="mw-userlink mw-anonuserlink" title="Special:Contributions/204.98.118.114"><bdi>204.98.118.114</bdi></a> <span class="mw-usertoollinks">(<a href="https://en.wikipedia.org/w/index.php?title=User_talk:204.98.118.114&amp;action=edit&amp;redlink=1" class="new mw-usertoollinks-talk" title="User talk:204.98.118.114 (page does not exist)">talk</a>)</span> <span class="comment">(<a href="#Introduction">→</a>‎<span dir="auto"><span class="autocomment">Introduction</span></span>)</span></div>
</div><div id="mw-revision-nav">(<a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;diff=prev&amp;oldid=852254681" title="Entropy (information theory)">diff</a>) <a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;direction=prev&amp;oldid=852254681" title="Entropy (information theory)">← Previous revision</a>&nbsp;| Latest revision (diff)&nbsp;| Newer revision → (diff)</div></div></div>
				<div id="jump-to-nav"></div>				<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>
				<a class="mw-jump-link" href="#p-search">Jump to search</a>
				<div id="mw-content-text" dir="ltr" class="mw-content-ltr" lang="en"><div class="mw-parser-output"><div role="note" class="hatnote navigation-not-searchable">For other uses, see <a href="https://en.wikipedia.org/wiki/Entropy_%28disambiguation%29" class="mw-disambig" title="Entropy (disambiguation)">Entropy_(disambiguation)</a>.</div>
<table class="plainlinks metadata ambox ambox-content ambox-multiple_issues compact-ambox" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><img alt="" src="40px-Ambox_important.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/60px-Ambox_important.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/80px-Ambox_important.svg.png 2x" data-file-width="40" data-file-height="40" width="40" height="40"></div></td><td class="mbox-text"><div class="mbox-text-span"><div class="mw-collapsible mw-made-collapsible" style="width:95%; margin: 0.2em 0;"><span class="mw-collapsible-toggle mw-collapsible-toggle-default" role="button" tabindex="0"><a class="mw-collapsible-text">hide</a></span><b>This article has multiple issues.</b> Please help <b><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit">improve it</a></b> or discuss these issues on the <b><a href="https://en.wikipedia.org/wiki/Talk:Entropy_%28information_theory%29" title="Talk:Entropy (information theory)">talk page</a></b>. <small><i>(<a href="https://en.wikipedia.org/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove these template messages</a>)</i></small>
<div class="mw-collapsible-content" style="margin-top: 0.3em;">
      <table class="plainlinks metadata ambox ambox-style ambox-technical" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><img alt="" src="40px-Edit-clear.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/60px-Edit-clear.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/80px-Edit-clear.svg.png 2x" data-file-width="48" data-file-height="48" width="40" height="40"></div></td><td class="mbox-text"><div class="mbox-text-span">This article <b>may be too technical for most readers to understand</b>. Please <a class="external text" href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit">help improve it</a> to <a href="https://en.wikipedia.org/wiki/Wikipedia:Make_technical_articles_understandable" title="Wikipedia:Make technical articles understandable">make it understandable to non-experts</a>, without removing the technical details.  <small><i>(November 2017)</i></small><small class="hide-when-compact"><i> (<a href="https://en.wikipedia.org/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
<table class="plainlinks metadata ambox ambox-content ambox-Refimprove" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><a href="https://en.wikipedia.org/wiki/File:Question_book-new.svg" class="image"><img alt="" src="50px-Question_book-new.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x" data-file-width="512" data-file-height="399" width="50" height="39"></a></div></td><td class="mbox-text"><div class="mbox-text-span">This article <b>needs additional citations for <a href="https://en.wikipedia.org/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">verification</a></b>.<span class="hide-when-compact"> Please help <a class="external text" href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit">improve this article</a> by <a href="https://en.wikipedia.org/wiki/Help:Introduction_to_referencing_with_Wiki_Markup/1" title="Help:Introduction to referencing with Wiki Markup/1">adding citations to reliable sources</a>. Unsourced material may be challenged and removed.</span>  <small><i>(April 2012)</i></small><small class="hide-when-compact"><i> (<a href="https://en.wikipedia.org/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
    </div>
</div><small class="hide-when-compact"><i> (<a href="https://en.wikipedia.org/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
<p class="mw-empty-elt">
</p>
<div class="thumb tright"><div class="thumbinner" style="width:302px;"><a href="https://en.wikipedia.org/wiki/File:Entropy_flip_2_coins.jpg" class="image"><img alt="" src="300px-Entropy_flip_2_coins.jpg" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy_flip_2_coins.jpg/450px-Entropy_flip_2_coins.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Entropy_flip_2_coins.jpg/600px-Entropy_flip_2_coins.jpg 2x" data-file-width="1312" data-file-height="768" width="300" height="176"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:Entropy_flip_2_coins.jpg" class="internal" title="Enlarge"></a></div>Two
 bits of entropy: In the case of two fair coin tosses, the information 
entropy in bits is the base-2 logarithm of the number of possible 
outcomes; with two coins there are four possible outcomes, and two bits 
of entropy. Generally, information entropy is the average amount of 
information conveyed by an event, when considering all possible 
outcomes.</div></div></div>
<p><b>Information entropy</b> is the <a href="https://en.wikipedia.org/wiki/Expected_value" title="Expected value">average</a> rate at which <a href="https://en.wikipedia.org/wiki/Information" title="Information">information</a> is produced by a <a href="https://en.wikipedia.org/wiki/Stochastic" title="Stochastic">stochastic</a> source of data.
</p><p>The measure of information entropy associated with each possible data value is the negative <a href="https://en.wikipedia.org/wiki/Logarithm" title="Logarithm">logarithm</a> of the <a href="https://en.wikipedia.org/wiki/Probability_mass_function" title="Probability mass function">probability mass function</a>
 for the value. Thus, when the data source has a lower-probability value
 (i.e., when a low-probability event occurs), the event carries more 
"information" ("surprisal") than when the source data has a 
higher-probability value. The amount of information conveyed by each 
event defined in this way becomes a <a href="https://en.wikipedia.org/wiki/Random_variable" title="Random variable">random variable</a> whose expected value is the information entropy. Generally, <i>entropy</i> refers to disorder or uncertainty, and the definition of entropy used in information theory is directly analogous to the <a href="https://en.wikipedia.org/wiki/Entropy_%28statistical_thermodynamics%29" title="Entropy (statistical thermodynamics)">definition used</a> in <a href="https://en.wikipedia.org/wiki/Statistical_thermodynamics" class="mw-redirect" title="Statistical thermodynamics">statistical thermodynamics</a>. The concept of information entropy was introduced by <a href="https://en.wikipedia.org/wiki/Claude_Shannon" title="Claude Shannon">Claude Shannon</a> in his 1948 paper "<a href="https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication" title="A Mathematical Theory of Communication">A Mathematical Theory of Communication</a>".<sup id="cite_ref-shannonPaper_1-0" class="reference"><a href="#cite_note-shannonPaper-1">[1]</a></sup>
</p><p>The basic model of a <a href="https://en.wikipedia.org/wiki/Data_communication" class="mw-redirect" title="Data communication">data communication</a> system is composed of three elements, a source of data, a <a href="https://en.wikipedia.org/wiki/Communication_channel" title="Communication channel">communication channel</a>,
 and a receiver, and – as expressed by Shannon – the "fundamental 
problem of communication" is for the receiver to be able to identify 
what data was generated by the source, based on the signal it receives 
through the channel.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">[2]</a></sup> The entropy provides an absolute limit on the shortest possible average length of a <a href="https://en.wikipedia.org/wiki/Lossless" class="mw-redirect" title="Lossless">lossless</a> <a href="https://en.wikipedia.org/wiki/Data_compression" title="Data compression">compression</a> encoding of the data produced by a source, and if the entropy of the source is less than the <a href="https://en.wikipedia.org/wiki/Channel_capacity" title="Channel capacity">channel capacity</a>
 of the communication channel, the data generated by the source can be 
reliably communicated to the receiver (at least in theory, possibly 
neglecting some practical considerations such as the complexity of the 
system needed to convey the data and the amount of time it may take for 
the data to be conveyed).
</p><p>Information entropy is typically measured in <a href="https://en.wikipedia.org/wiki/Bit" title="Bit">bits</a> (alternatively called "<a href="https://en.wikipedia.org/wiki/Shannon_%28unit%29" title="Shannon (unit)">shannons</a>") or sometimes in "natural units" (<a href="https://en.wikipedia.org/wiki/Nat_%28unit%29" title="Nat (unit)">nats</a>)
 or decimal digits (called "dits", "bans", or "hartleys"). The unit of 
the measurement depends on the base of the logarithm that is used to 
define the entropy.
</p><p>The logarithm of the probability distribution is useful as a 
measure of entropy because it is additive for independent sources. For 
instance, the entropy of a fair coin toss is 1 bit, and the entropy of <span class="texhtml"><i>m</i></span> tosses is <span class="texhtml"><i>m</i></span> bits. In a straightforward representation, <span class="texhtml">log<sub>2</sub>(<i>n</i>)</span> bits are needed to represent a variable that can take one of <span class="texhtml"><i>n</i></span> values if <span class="texhtml"><i>n</i></span>
 is a power of 2. If these values are equally probable, the entropy (in 
bits) is equal to this number. If one of the values is more probable to 
occur than the others, an observation that this value occurs is less 
informative than if some less common outcome had occurred. Conversely, 
rarer events provide more information when observed. Since observation 
of less probable events occurs more rarely, the net effect is that the 
entropy (thought of as average information) received from non-uniformly 
distributed data is always less than or equal to <span class="texhtml">log<sub>2</sub>(<i>n</i>)</span>.
 Entropy is zero when one outcome is certain to occur. The entropy 
quantifies these considerations when a probability distribution of the 
source data is known. The <i>meaning</i> of the events observed (the meaning of <i>messages</i>)
 does not matter in the definition of entropy. Entropy only takes into 
account the probability of observing a specific event, so the 
information it encapsulates is information about the underlying 
probability distribution, not the meaning of the events themselves.
</p>
<div id="toc" class="toc"><input role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" type="checkbox"><div class="toctitle" dir="ltr" lang="en"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Introduction"><span class="tocnumber">1</span> <span class="toctext">Introduction</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Definition"><span class="tocnumber">2</span> <span class="toctext">Definition</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Example"><span class="tocnumber">3</span> <span class="toctext">Example</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Rationale"><span class="tocnumber">4</span> <span class="toctext">Rationale</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Aspects"><span class="tocnumber">5</span> <span class="toctext">Aspects</span></a>
<ul>
<li class="toclevel-2 tocsection-6"><a href="#Relationship_to_thermodynamic_entropy"><span class="tocnumber">5.1</span> <span class="toctext">Relationship to thermodynamic entropy</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Entropy_as_information_content"><span class="tocnumber">5.2</span> <span class="toctext">Entropy as information content</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="#Entropy_as_a_measure_of_diversity"><span class="tocnumber">5.3</span> <span class="toctext">Entropy as a measure of diversity</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Data_compression"><span class="tocnumber">5.4</span> <span class="toctext">Data compression</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#World%27s_technological_capacity_to_store_and_communicate_information"><span class="tocnumber">5.5</span> <span class="toctext">World's technological capacity to store and communicate information</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Limitations_of_entropy_as_information_content"><span class="tocnumber">5.6</span> <span class="toctext">Limitations of entropy as information content</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Limitations_of_entropy_in_cryptography"><span class="tocnumber">5.7</span> <span class="toctext">Limitations of entropy in cryptography</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="#Data_as_a_Markov_process"><span class="tocnumber">5.8</span> <span class="toctext">Data as a Markov process</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#b-ary_entropy"><span class="tocnumber">5.9</span> <span class="toctext"><span><i>b</i></span>-ary entropy</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-15"><a href="#Efficiency"><span class="tocnumber">6</span> <span class="toctext">Efficiency</span></a></li>
<li class="toclevel-1 tocsection-16"><a href="#Characterization"><span class="tocnumber">7</span> <span class="toctext">Characterization</span></a>
<ul>
<li class="toclevel-2 tocsection-17"><a href="#Continuity"><span class="tocnumber">7.1</span> <span class="toctext">Continuity</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#Symmetry"><span class="tocnumber">7.2</span> <span class="toctext">Symmetry</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="#Maximum"><span class="tocnumber">7.3</span> <span class="toctext">Maximum</span></a></li>
<li class="toclevel-2 tocsection-20"><a href="#Additivity"><span class="tocnumber">7.4</span> <span class="toctext">Additivity</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-21"><a href="#Further_properties"><span class="tocnumber">8</span> <span class="toctext">Further properties</span></a></li>
<li class="toclevel-1 tocsection-22"><a href="#Extending_discrete_entropy_to_the_continuous_case"><span class="tocnumber">9</span> <span class="toctext">Extending discrete entropy to the continuous case</span></a>
<ul>
<li class="toclevel-2 tocsection-23"><a href="#Differential_entropy"><span class="tocnumber">9.1</span> <span class="toctext">Differential entropy</span></a></li>
<li class="toclevel-2 tocsection-24"><a href="#Limiting_density_of_discrete_points"><span class="tocnumber">9.2</span> <span class="toctext">Limiting density of discrete points</span></a></li>
<li class="toclevel-2 tocsection-25"><a href="#Relative_entropy"><span class="tocnumber">9.3</span> <span class="toctext">Relative entropy</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-26"><a href="#Use_in_combinatorics"><span class="tocnumber">10</span> <span class="toctext">Use in combinatorics</span></a>
<ul>
<li class="toclevel-2 tocsection-27"><a href="#Loomis%E2%80%93Whitney_inequality"><span class="tocnumber">10.1</span> <span class="toctext">Loomis–Whitney inequality</span></a></li>
<li class="toclevel-2 tocsection-28"><a href="#Approximation_to_binomial_coefficient"><span class="tocnumber">10.2</span> <span class="toctext">Approximation to binomial coefficient</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-29"><a href="#See_also"><span class="tocnumber">11</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-30"><a href="#References"><span class="tocnumber">12</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-31"><a href="#Further_reading"><span class="tocnumber">13</span> <span class="toctext">Further reading</span></a>
<ul>
<li class="toclevel-2 tocsection-32"><a href="#Textbooks_on_information_theory"><span class="tocnumber">13.1</span> <span class="toctext">Textbooks on information theory</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-33"><a href="#External_links"><span class="tocnumber">14</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Introduction">Introduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=1" title="Edit section: Introduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The basic idea of information theory is the more one knows about a 
topic, the less new information one is apt to get about it. If an event 
is very probable, it is no surprise when it happens and thus provides 
little new information. Inversely, if the event was improbable, it is 
much more informative that the event happened. Therefore, the <a href="https://en.wikipedia.org/wiki/Self-information" title="Self-information">information content</a>
 is an increasing function of the inverse of the probability of the 
event (1/p). Now, if more events may happen, entropy measures the 
average  information content you can expect to get if one of the events 
actually happens. This implies that casting a die has more entropy than 
tossing a coin because each outcome of the die has smaller probability 
than each outcome of the coin.
</p><p>Thus, entropy is a measure of <i>unpredictability</i> of the state, or equivalently, of its <i>average information content</i>.
 To get an intuitive understanding of these terms, consider the example 
of a political poll. Usually, such polls happen because the outcome of 
the poll is not already known. In other words, the outcome of the poll 
is relatively <i>unpredictable</i>, and actually performing the poll and learning the results gives some new <i>information</i>; these are just different ways of saying that the <i>a priori</i>
 entropy of the poll results is large. Now, consider the case that the 
same poll is performed a second time shortly after the first poll. Since
 the result of the first poll is already known, the outcome of the 
second poll can be predicted well and the results should not contain 
much new information; in this case the <i>a priori</i> entropy of the second poll result is small relative to that of the first.
</p><p>Now consider the example of a coin toss. Assuming the probability
 of heads is the same as the probability of tails, then the entropy of 
the coin toss is as high as it could be. This is because there is no way
 to predict the outcome of the coin toss ahead of time: if we have to 
choose, the best we can do is predict that the coin will come up heads, 
and this prediction will be correct with probability 1/2. Such a coin 
toss has one bit of entropy since there are two possible outcomes that 
occur with equal probability, and learning the actual outcome contains 
one bit of information. In contrast, a coin toss using a coin that has 
two heads and no tails has zero entropy since the coin will always come 
up heads, and the outcome can be predicted perfectly. Analogously, one 
binary-outcome with equiprobable values has a Shannon entropy of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \log _{2}2=1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mn>2</mn>
        <mo>=</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \log _{2}2=1}</annotation>
  </semantics>
</math></span><img src="3a63d03d23e9d47761a4f1cfa1d0097919c395f1.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:9.836ex; height:2.676ex;" alt="\log _{2}2=1"></span> bit. Similarly, one <a href="https://en.wikipedia.org/wiki/Ternary_numeral_system" title="Ternary numeral system">trit</a> with equiprobable values contains <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \log _{2}3}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mn>3</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \log _{2}3}</annotation>
  </semantics>
</math></span><img src="f889b70067a012339b8baa7f0f9d17a0e6889c8f.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.576ex; height:2.676ex;" alt="\log _{2}3"></span> (about 1.58496) bits of information because it can have one of three values.
</p><p>English text, treated as a string of characters, has fairly low 
entropy, i.e., is fairly predictable. Even if we do not know exactly 
what is going to come next, we can be fairly certain that, for example, 
'e' will be far more common than 'z', that the combination 'qu' will be 
much more common than any other combination with a 'q' in it, and that 
the combination 'th' will be more common than 'z', 'q', or 'qu'. After 
the first few letters one can often guess the rest of the word. English 
text has between 0.6 and 1.3 bits of entropy per character of the 
message.<sup id="cite_ref-Schneier,_B_page_234_3-0" class="reference"><a href="#cite_note-Schneier,_B_page_234-3">[3]</a></sup>
</p><p>If a <a href="https://en.wikipedia.org/wiki/Data_compression" title="Data compression">compression</a>
 scheme is lossless—that is, you can always recover the entire original 
message by decompressing—then a compressed message has the same quantity
 of information as the original, but communicated in fewer characters. 
That is, it has more information, or a higher entropy, per character. 
This means a compressed message has less redundancy. Roughly speaking, <a href="https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem" title="Shannon's source coding theorem">Shannon's source coding theorem</a> says that a lossless compression scheme cannot compress messages, on average, to have <i>more</i> than one bit of information per bit of message, but that any value <i>less</i>
 than one bit of information per bit of message can be attained by 
employing a suitable coding scheme. The entropy of a message per bit 
multiplied by the length of that message is a measure of how much total 
information the message contains.
</p><p>Intuitively, imagine that we wish to transmit sequences 
comprising the 4 characters 'A', 'B', 'C', and 'D'. Thus, a message to 
be transmitted might be 'ABADDCAB'. Information theory gives a way of 
calculating the smallest possible amount of information that will convey
 this. If all 4 letters are equally likely (25%), we can do no better 
(over a binary channel) than to have 2 bits encode (in binary) each 
letter: 'A' might code as '00', 'B' as '01', 'C' as '10', and 'D' as 
'11'. Now suppose 'A' occurs with 70% probability, 'B' with 26%, and 'C'
 and 'D' with 2% each. We could assign variable length codes, so that 
receiving a '1' tells us to look at another bit unless we have already 
received 2 bits of sequential 1s. In this case, 'A' would be coded as 
'0' (one bit), 'B' as '10', and 'C' and 'D' as '110' and '111'. It is 
easy to see that 70% of the time only one bit needs to be sent, 26% of 
the time two bits, and only 4% of the time 3 bits. On average, then, 
fewer than 2 bits are required since the entropy is lower (owing to the 
high prevalence of 'A' followed by 'B' – together 96% of characters). 
The calculation of the sum of probability-weighted log probabilities 
measures and captures this effect.
</p><p>Shannon's theorem also implies that no lossless compression scheme can shorten <i>all</i> messages. If some messages come out shorter, at least one must come out longer due to the <a href="https://en.wikipedia.org/wiki/Pigeonhole_principle" title="Pigeonhole principle">pigeonhole principle</a>.
 In practical use, this is generally not a problem, because we are 
usually only interested in compressing certain types of messages, for 
example English documents as opposed to gibberish text, or digital 
photographs rather than noise, and it is unimportant if a compression 
algorithm makes some unlikely or uninteresting sequences larger. 
However, the problem can still arise even in everyday use when applying a
 compression algorithm to already compressed data: for example, making a
 ZIP file of music, pictures or videos that are already in a compressed 
format such as <a href="https://en.wikipedia.org/wiki/FLAC" title="FLAC">FLAC</a>, <a href="https://en.wikipedia.org/wiki/MP3" title="MP3">MP3</a>, <a href="https://en.wikipedia.org/wiki/WebM" title="WebM">WebM</a>, <a href="https://en.wikipedia.org/wiki/Advanced_Audio_Coding" title="Advanced Audio Coding">AAC</a>, <a href="https://en.wikipedia.org/wiki/Portable_Network_Graphics" title="Portable Network Graphics">PNG</a> or <a href="https://en.wikipedia.org/wiki/JPEG" title="JPEG">JPEG</a> will generally result in a ZIP file that is slightly <i>larger</i> than the source file(s).<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (May 2018)">citation needed</span></a></i>]</sup>
</p>
<h2><span class="mw-headline" id="Definition">Definition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=2" title="Edit section: Definition">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Named after <a href="https://en.wikipedia.org/wiki/H-theorem" title="H-theorem">Boltzmann's Η-theorem</a>, Shannon defined the entropy <span class="texhtml">Η</span> (Greek capital letter <a href="https://en.wikipedia.org/wiki/Eta" title="Eta">eta</a>) of a <a href="https://en.wikipedia.org/wiki/Discrete_random_variable" class="mw-redirect" title="Discrete random variable">discrete random variable</a> <span class="texhtml"><i>X</i></span> with possible values <span class="texhtml">{<i>x</i><sub>1</sub>, ..., <i>x</i><sub><i>n</i></sub></span>} and <a href="https://en.wikipedia.org/wiki/Probability_mass_function" title="Probability mass function">probability mass function</a> <span class="texhtml">P(<i>X</i>)</span> as:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (X)=\mathbb {E} [\mathrm {I} (X)]=\mathbb {E} [-\ln(\mathrm {P} (X))].}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="double-struck">E</mi>
        </mrow>
        <mo stretchy="false">[</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">I</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">]</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="double-struck">E</mi>
        </mrow>
        <mo stretchy="false">[</mo>
        <mo>−<!-- − --></mo>
        <mi>ln</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">P</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">]</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} (X)=\mathbb {E} [\mathrm {I} (X)]=\mathbb {E} [-\ln(\mathrm {P} (X))].}</annotation>
  </semantics>
</math></span><img src="f97536b2e0706d0b3d01ca1175252e86bfbd3372.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:34.009ex; height:2.843ex;" alt="{\displaystyle \mathrm {H} (X)=\mathbb {E} [\mathrm {I} (X)]=\mathbb {E} [-\ln(\mathrm {P} (X))].}"></span></dd></dl>
<p>Here <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbb {E} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="double-struck">E</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbb {E} }</annotation>
  </semantics>
</math></span><img src="ad9faf1fd4a61d36d7f8a2f3204f3805a43c0d4a.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.55ex; height:2.176ex;" alt="\mathbb {E} "></span> is the <a href="https://en.wikipedia.org/wiki/Expected_value" title="Expected value">expected value operator</a>, and <span class="texhtml">I</span> is the <a href="https://en.wikipedia.org/wiki/Self-information" title="Self-information">information content</a> of <span class="texhtml"><i>X</i></span>.<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">[4]</a></sup><sup id="cite_ref-5" class="reference"><a href="#cite_note-5">[5]</a></sup>
<span class="texhtml">I(<i>X</i>)</span> is itself a random variable.
</p><p>The entropy can explicitly be written as
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (X)=\sum _{i=1}^{n}{\mathrm {P} (x_{i})\,\mathrm {I} (x_{i})}=-\sum _{i=1}^{n}{\mathrm {P} (x_{i})\log _{b}\mathrm {P} (x_{i})},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">P</mi>
          </mrow>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mo stretchy="false">)</mo>
          <mspace width="thinmathspace"></mspace>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">I</mi>
          </mrow>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mo stretchy="false">)</mo>
        </mrow>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">P</mi>
          </mrow>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mo stretchy="false">)</mo>
          <msub>
            <mi>log</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>b</mi>
            </mrow>
          </msub>
          <mo>⁡<!-- ⁡ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">P</mi>
          </mrow>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mo stretchy="false">)</mo>
        </mrow>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} 
(X)=\sum _{i=1}^{n}{\mathrm {P} (x_{i})\,\mathrm {I} (x_{i})}=-\sum 
_{i=1}^{n}{\mathrm {P} (x_{i})\log _{b}\mathrm {P} (x_{i})},}</annotation>
  </semantics>
</math></span><img src="9017e7f5171f1770f8bd702487f297b77df8a907.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:48.469ex; height:6.843ex;" alt="{\displaystyle \mathrm {H} (X)=\sum _{i=1}^{n}{\mathrm {P} (x_{i})\,\mathrm {I} (x_{i})}=-\sum _{i=1}^{n}{\mathrm {P} (x_{i})\log _{b}\mathrm {P} (x_{i})},}"></span></dd></dl>
<p>where <span class="texhtml"><i>b</i></span> is the <a href="https://en.wikipedia.org/wiki/Base_%28exponentiation%29" title="Base (exponentiation)">base</a> of the <a href="https://en.wikipedia.org/wiki/Logarithm" title="Logarithm">logarithm</a> used. Common values of <span class="texhtml"><i>b</i></span> are 2, <a href="https://en.wikipedia.org/wiki/E_%28mathematical_constant%29" title="E (mathematical constant)">Euler's number <span class="texhtml"><i>e</i></span></a>, and 10, and the corresponding units of entropy are the <a href="https://en.wikipedia.org/wiki/Bit_%28unit%29" class="mw-redirect" title="Bit (unit)">bits</a> for <span class="texhtml"><i>b</i> = 2</span>, <a href="https://en.wikipedia.org/wiki/Nat_%28unit%29" title="Nat (unit)">nats</a> for <span class="texhtml"><i>b</i> = <i>e</i></span>, and <a href="https://en.wikipedia.org/wiki/Ban_%28unit%29" class="mw-redirect" title="Ban (unit)">bans</a> for <span class="texhtml"><i>b</i> = 10</span>.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">[6]</a></sup>
</p><p>In the case of <span class="texhtml">P(<i>x</i><sub><i>i</i></sub>) = 0</span> for some <span class="texhtml"><i>i</i></span>, the value of the corresponding summand <span class="texhtml">0 log<sub><i>b</i></sub>(0)</span> is taken to be <span class="texhtml">0</span>, which is consistent with the <a href="https://en.wikipedia.org/wiki/Limit_of_a_function" title="Limit of a function">limit</a>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \lim _{p\to 0+}p\log(p)=0.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo movablelimits="true" form="prefix">lim</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>p</mi>
            <mo stretchy="false">→<!-- → --></mo>
            <mn>0</mn>
            <mo>+</mo>
          </mrow>
        </munder>
        <mi>p</mi>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>p</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mn>0.</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \lim _{p\to 0+}p\log(p)=0.}</annotation>
  </semantics>
</math></span><img src="6080376128ce302230364b5dc6991c55c2fe4ca0.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.338ex; margin-left: -0.063ex; width:17.436ex; height:4.343ex;" alt="\lim _{p\to 0+}p\log(p)=0."></span></dd></dl>
<p>One may also define the <a href="https://en.wikipedia.org/wiki/Conditional_entropy" title="Conditional entropy">conditional entropy</a> of two events <span class="texhtml"><i>X</i></span> and <span class="texhtml"><i>Y</i></span> taking values <span class="texhtml"><i>x</i><sub><i>i</i></sub></span> and <span class="texhtml"><i>y</i><sub><i>j</i></sub></span> respectively, as
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (X|Y)=-\sum _{i,j}p(x_{i},y_{j})\log {\frac {p(x_{i},y_{j})}{p(y_{j})}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>,</mo>
            <mi>j</mi>
          </mrow>
        </munder>
        <mi>p</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi>p</mi>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
              <mo>,</mo>
              <msub>
                <mi>y</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
            </mrow>
            <mrow>
              <mi>p</mi>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>y</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>j</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} (X|Y)=-\sum _{i,j}p(x_{i},y_{j})\log {\frac {p(x_{i},y_{j})}{p(y_{j})}}}</annotation>
  </semantics>
</math></span><img src="a44d96821415e6aa4ac63b6c8f6af9dfdfa9a3f2.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:37.952ex; height:7.176ex;" alt="{\displaystyle \mathrm {H} (X|Y)=-\sum _{i,j}p(x_{i},y_{j})\log {\frac {p(x_{i},y_{j})}{p(y_{j})}}}"></span></dd></dl>
<p>where <span class="texhtml"><i>p</i>(<i>x</i><sub><i>i</i></sub>, <i>y</i><sub><i>j</i></sub>)</span> is the probability that <span class="texhtml"><i>X</i> = <i>x</i><sub><i>i</i></sub></span> and <span class="texhtml"><i>Y</i> = <i>y</i><sub><i>j</i></sub></span>. This quantity should be understood as the amount of randomness in the random variable <span class="texhtml"><i>X</i></span> given the event <span class="texhtml"><i>Y</i></span>.
</p>
<h2><span class="mw-headline" id="Example">Example</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=3" title="Edit section: Example">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright"><div class="thumbinner" style="width:202px;"><a href="https://en.wikipedia.org/wiki/File:Binary_entropy_plot.svg" class="image"><img alt="" src="200px-Binary_entropy_plot.png" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/300px-Binary_entropy_plot.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/2/22/Binary_entropy_plot.svg/400px-Binary_entropy_plot.svg.png 2x" data-file-width="300" data-file-height="300" width="200" height="200"></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:Binary_entropy_plot.svg" class="internal" title="Enlarge"></a></div>Entropy <span class="texhtml">Η(<i>X</i>)</span> (i.e. the <a href="https://en.wikipedia.org/wiki/Expected_value" title="Expected value">expected</a> <a href="https://en.wikipedia.org/wiki/Self-information" title="Self-information">surprisal</a>) of a coin flip, measured in bits, graphed versus the bias of the coin <span class="texhtml">Pr(<i>X</i> = 1)</span>, where <span class="texhtml"><i>X</i> = 1</span> represents a result of heads.<br><br>Here,
 the entropy is at most 1 bit, and to communicate the outcome of a coin 
flip (2 possible values) will require an average of at most 1 bit 
(exactly 1 bit for a fair coin). The result of a fair die (6 possible 
values) would require on average log<sub>2</sub>6 bits.</div></div></div>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Binary_entropy_function" title="Binary entropy function">Binary entropy function</a></div>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Bernoulli_process" title="Bernoulli process">Bernoulli process</a></div>
<p>Consider tossing a coin with known, not necessarily fair, probabilities of coming up heads or tails; this can be modelled as a <a href="https://en.wikipedia.org/wiki/Bernoulli_process" title="Bernoulli process">Bernoulli process</a>.
</p><p>The entropy of the unknown result of the next toss of the coin is
 maximized if the coin is fair (that is, if heads and tails both have 
equal probability 1/2). This is the situation of maximum uncertainty as 
it is most difficult to predict the outcome of the next toss; the result
 of each toss of the coin delivers one full <a href="https://en.wikipedia.org/wiki/Bit" title="Bit">bit</a> of information. This is because
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (X)=-\sum _{i=1}^{n}{\mathrm {P} (x_{i})\log _{b}\mathrm {P} (x_{i})}=-\sum _{i=1}^{2}{(1/2)\log _{2}(1/2)}=-\sum _{i=1}^{2}{(1/2)\times (-1)}=1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">P</mi>
          </mrow>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mo stretchy="false">)</mo>
          <msub>
            <mi>log</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>b</mi>
            </mrow>
          </msub>
          <mo>⁡<!-- ⁡ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">P</mi>
          </mrow>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>x</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mo stretchy="false">)</mo>
        </mrow>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">(</mo>
          <mn>1</mn>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>/</mo>
          </mrow>
          <mn>2</mn>
          <mo stretchy="false">)</mo>
          <msub>
            <mi>log</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>2</mn>
            </mrow>
          </msub>
          <mo>⁡<!-- ⁡ --></mo>
          <mo stretchy="false">(</mo>
          <mn>1</mn>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>/</mo>
          </mrow>
          <mn>2</mn>
          <mo stretchy="false">)</mo>
        </mrow>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">(</mo>
          <mn>1</mn>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>/</mo>
          </mrow>
          <mn>2</mn>
          <mo stretchy="false">)</mo>
          <mo>×<!-- × --></mo>
          <mo stretchy="false">(</mo>
          <mo>−<!-- − --></mo>
          <mn>1</mn>
          <mo stretchy="false">)</mo>
        </mrow>
        <mo>=</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} 
(X)=-\sum _{i=1}^{n}{\mathrm {P} (x_{i})\log _{b}\mathrm {P} 
(x_{i})}=-\sum _{i=1}^{2}{(1/2)\log _{2}(1/2)}=-\sum 
_{i=1}^{2}{(1/2)\times (-1)}=1}</annotation>
  </semantics>
</math></span><img src="0d5eab2ab8165e547d69a5fb49db1c18cb1518ef.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:80.551ex; height:7.343ex;" alt="{\displaystyle \mathrm {H} (X)=-\sum _{i=1}^{n}{\mathrm {P} (x_{i})\log _{b}\mathrm {P} (x_{i})}=-\sum _{i=1}^{2}{(1/2)\log _{2}(1/2)}=-\sum _{i=1}^{2}{(1/2)\times (-1)}=1}"></span></dd></dl>
<p>However, if we know the coin is not fair, but comes up heads or tails with probabilities <span class="texhtml"><i>p</i></span> and <span class="texhtml"><i>q</i></span>, where <span class="texhtml"><i>p</i> ≠ <i>q</i></span>,
 then there is less uncertainty. Every time it is tossed, one side is 
more likely to come up than the other. The reduced uncertainty is 
quantified in a lower entropy: on average each toss of the coin delivers
 less than one full bit of information. For example, if <span class="texhtml"><i>p</i></span>=0.7, then
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (X)=-p\log _{2}(p)-q\log _{2}(q)=-0.7\log _{2}(0.7)-0.3\log _{2}(0.3)\approx -0.7\times (-0.515)-0.3\times (-1.737)=0.8816&lt;1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <mi>p</mi>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>p</mi>
        <mo stretchy="false">)</mo>
        <mo>−<!-- − --></mo>
        <mi>q</mi>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>q</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <mn>0.7</mn>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mn>0.7</mn>
        <mo stretchy="false">)</mo>
        <mo>−<!-- − --></mo>
        <mn>0.3</mn>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mn>0.3</mn>
        <mo stretchy="false">)</mo>
        <mo>≈<!-- ≈ --></mo>
        <mo>−<!-- − --></mo>
        <mn>0.7</mn>
        <mo>×<!-- × --></mo>
        <mo stretchy="false">(</mo>
        <mo>−<!-- − --></mo>
        <mn>0.515</mn>
        <mo stretchy="false">)</mo>
        <mo>−<!-- − --></mo>
        <mn>0.3</mn>
        <mo>×<!-- × --></mo>
        <mo stretchy="false">(</mo>
        <mo>−<!-- − --></mo>
        <mn>1.737</mn>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mn>0.8816</mn>
        <mo>&lt;</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} 
(X)=-p\log _{2}(p)-q\log _{2}(q)=-0.7\log _{2}(0.7)-0.3\log 
_{2}(0.3)\approx -0.7\times (-0.515)-0.3\times (-1.737)=0.8816&lt;1}</annotation>
  </semantics>
</math></span><img src="21b8907b9f201f594eb1f8b7d660bdb134e2b67b.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:113.299ex; height:2.843ex;" alt="{\displaystyle \mathrm {H} (X)=-p\log _{2}(p)-q\log _{2}(q)=-0.7\log _{2}(0.7)-0.3\log _{2}(0.3)\approx -0.7\times (-0.515)-0.3\times (-1.737)=0.8816&lt;1}"></span></dd></dl>
<p>The extreme case is that of a double-headed coin that never comes up 
tails, or a double-tailed coin that never results in a head. Then there 
is no uncertainty. The entropy is zero: each toss of the coin delivers 
no new information as the outcome of each coin toss is always certain.
</p><p>Entropy can be normalized by dividing it by information length. This ratio is called <a href="https://en.wikipedia.org/wiki/Metric_entropy" class="mw-redirect" title="Metric entropy">metric entropy</a> and is a measure of the randomness of the information.
</p>
<h2><span class="mw-headline" id="Rationale">Rationale</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=4" title="Edit section: Rationale">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>To understand the meaning of <span class="texhtml">-∑ <i>p</i><sub><i>i</i></sub> log(<i>p</i><sub><i>i</i></sub>)</span>, at first, try to define an information function, <span class="texhtml">I</span>, in terms of an event <span class="texhtml"><i>i</i></span> with probability <span class="texhtml"><i>p</i><sub><i>i</i></sub></span>. How much information is acquired due to the observation of event <span class="texhtml"><i>i</i></span>? Shannon's solution follows from the fundamental <a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Characterization" title="Entropy (information theory)">properties</a> of information:<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">[7]</a></sup>
</p>
<ol><li><span class="texhtml">I(<i>p</i>)</span> is monotonically decreasing in <span class="texhtml"><i>p</i></span> – an increase in the probability of an event decreases the information from an observed event, and vice versa</li>
<li><span class="texhtml">I(<i>p</i>) ≥ 0</span> – information is a non-negative quantity</li>
<li><span class="texhtml">I(1) = 0</span> – events that always occur do not communicate information</li>
<li><span class="texhtml">I(<i>p</i><sub>1</sub> <i>p</i><sub>2</sub>) = I(<i>p</i><sub>1</sub>) + I(<i>p</i><sub>2</sub>)</span> – information due to independent events is additive</li></ol>
<p>The last is a crucial property. It states that joint probability of 
independent sources of information communicates as much information as 
the two individual events separately. Particularly, if the first event 
can yield one of <span class="texhtml"><i>n</i></span> equiprobable outcomes and another has one of <span class="texhtml"><i>m</i></span> equiprobable outcomes then there are <span class="texhtml"><i>mn</i></span> possible outcomes of the joint event. This means that if <span class="texhtml">log<sub>2</sub>(<i>n</i>)</span> bits are needed to encode the first value and <span class="texhtml">log<sub>2</sub>(<i>m</i>)</span> to encode the second, one needs <span class="texhtml">log<sub>2</sub>(<i>mn</i>) = log<sub>2</sub>(<i>m</i>) + log<sub>2</sub>(<i>n</i>)</span>
 to encode both. Shannon discovered that the proper choice of function 
to quantify information, preserving this additivity, is logarithmic, 
i.e.,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {I} (p)=\log(1/p)=-\log(p):}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">I</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>p</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi>p</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>p</mi>
        <mo stretchy="false">)</mo>
        <mo>:</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {I} (p)=\log(1/p)=-\log(p):}</annotation>
  </semantics>
</math></span><img src="2d284a6428564b9b44cb89d7fe90898e8613eb93.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:27.728ex; height:2.843ex;" alt="{\displaystyle \mathrm {I} (p)=\log(1/p)=-\log(p):}"></span></dd></dl>
<p>let <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\textstyle I}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="false" scriptlevel="0">
        <mi>I</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\textstyle I}</annotation>
  </semantics>
</math></span><img src="abfab7b61b17518d4537eba5d4491e0ecec5773c.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.172ex; height:2.176ex;" alt="{\textstyle I}"></span> be the information function which one assumes to be twice continuously differentiable, one has:
</p><p><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\begin{array}{lcl}I(p_{1}p_{2})&amp;=&amp;I(p_{1})+I(p_{2})\\p_{2}I'(p_{1}p_{2})&amp;=&amp;I'(p_{1})\\I'(p_{1}p_{2})+p_{1}p_{2}I''(p_{1}p_{2})&amp;=&amp;0\\I'(u)+uI''(u)&amp;=&amp;0\\(u\mapsto uI'(u))'&amp;=&amp;0\end{array}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="left center left" rowspacing="4pt" columnspacing="1em">
            <mtr>
              <mtd>
                <mi>I</mi>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                </msub>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mi>I</mi>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
                <mo>+</mo>
                <mi>I</mi>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msub>
                <msup>
                  <mi>I</mi>
                  <mo>′</mo>
                </msup>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                </msub>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <msup>
                  <mi>I</mi>
                  <mo>′</mo>
                </msup>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msup>
                  <mi>I</mi>
                  <mo>′</mo>
                </msup>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                </msub>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
                <mo>+</mo>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                </msub>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msub>
                <msup>
                  <mi>I</mi>
                  <mo>″</mo>
                </msup>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                </msub>
                <msub>
                  <mi>p</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mn>0</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msup>
                  <mi>I</mi>
                  <mo>′</mo>
                </msup>
                <mo stretchy="false">(</mo>
                <mi>u</mi>
                <mo stretchy="false">)</mo>
                <mo>+</mo>
                <mi>u</mi>
                <msup>
                  <mi>I</mi>
                  <mo>″</mo>
                </msup>
                <mo stretchy="false">(</mo>
                <mi>u</mi>
                <mo stretchy="false">)</mo>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mn>0</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mo stretchy="false">(</mo>
                <mi>u</mi>
                <mo stretchy="false">↦<!-- ↦ --></mo>
                <mi>u</mi>
                <msup>
                  <mi>I</mi>
                  <mo>′</mo>
                </msup>
                <mo stretchy="false">(</mo>
                <mi>u</mi>
                <mo stretchy="false">)</mo>
                <msup>
                  <mo stretchy="false">)</mo>
                  <mo>′</mo>
                </msup>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mn>0</mn>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 
{\begin{array}{lcl}I(p_{1}p_{2})&amp;=&amp;I(p_{1})+I(p_{2})\\p_{2}I'(p_{1}p_{2})&amp;=&amp;I'(p_{1})\\I'(p_{1}p_{2})+p_{1}p_{2}I''(p_{1}p_{2})&amp;=&amp;0\\I'(u)+uI''(u)&amp;=&amp;0\\(u\mapsto
 uI'(u))'&amp;=&amp;0\end{array}}}</annotation>
  </semantics>
</math></span><img src="68b9b960c9b69877a3425b4264de612f1df52bb8.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -7.671ex; width:44.51ex; height:16.509ex;" alt="{\displaystyle {\begin{array}{lcl}I(p_{1}p_{2})&amp;=&amp;I(p_{1})+I(p_{2})\\p_{2}I'(p_{1}p_{2})&amp;=&amp;I'(p_{1})\\I'(p_{1}p_{2})+p_{1}p_{2}I''(p_{1}p_{2})&amp;=&amp;0\\I'(u)+uI''(u)&amp;=&amp;0\\(u\mapsto uI'(u))'&amp;=&amp;0\end{array}}}"></span>
</p><p>This differential equation leads to the solution <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle I(u)=k\log u}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>I</mi>
        <mo stretchy="false">(</mo>
        <mi>u</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>k</mi>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mi>u</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle I(u)=k\log u}</annotation>
  </semantics>
</math></span><img src="0e64c0dff53717685c61f40209be0a5bef4a2074.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:13.696ex; height:2.843ex;" alt="{\displaystyle I(u)=k\log u}"></span> for any <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle k\in \mathbb {R} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>∈<!-- ∈ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="double-struck">R</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k\in \mathbb {R} }</annotation>
  </semantics>
</math></span><img src="177754a3bb6c26dbd54cbc866337b20bafa64e7c.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.73ex; height:2.176ex;" alt="{\displaystyle k\in \mathbb {R} }"></span>. Condition 2. leads to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle k&lt;0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>&lt;</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k&lt;0}</annotation>
  </semantics>
</math></span><img src="d59e54fad8568e90715f2b10521d3e39bc45fca9.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.472ex; height:2.176ex;" alt="k&lt;0"></span> and especially, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle k}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k}</annotation>
  </semantics>
</math></span><img src="c3c9a2c7b599b37105512c5d570edc034056dd40.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.211ex; height:2.176ex;" alt="k "></span> can be chosen on the form <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle k=-1/\log x}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <mn>1</mn>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k=-1/\log x}</annotation>
  </semantics>
</math></span><img src="54c55db025308891b67cb8f13e721d387dca459e.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:13.518ex; height:2.843ex;" alt="{\displaystyle k=-1/\log x}"></span> with <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle x&gt;1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
        <mo>&gt;</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x&gt;1}</annotation>
  </semantics>
</math></span><img src="0549e1fb7ee2023519833093c6e3b60236e7d09f.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.591ex; height:2.176ex;" alt="x&gt;1"></span>, which is equivalent to choosing a specific base for the logarithm. The different units of information (bits for <span class="texhtml">log<sub>2</sub></span>, nats for the natural logarithm <span class="texhtml">ln</span>, bans for <span class="texhtml">log<sub>10</sub></span> and so on) are just constant multiples of each other. For instance, in case of a fair coin toss, heads provides <span class="texhtml">log<sub>2</sub>(2) = 1</span> bit of information, which is approximately 0.693&nbsp;nats or 0.301&nbsp;decimal digits. Because of additivity, <span class="texhtml"><i>n</i></span> tosses provide <span class="texhtml"><i>n</i></span> bits of information, which is approximately <span class="texhtml">0.693<i>n</i></span> nats or <span class="texhtml">0.301<i>n</i></span> decimal digits.
</p><p>Now, suppose we have a distribution where event <span class="texhtml"><i>i</i></span> can happen with probability <span class="texhtml"><i>p</i><sub><i>i</i></sub></span>. Suppose we have sampled it <span class="texhtml"><i>N</i></span> times and outcome <span class="texhtml"><i>i</i></span> was, accordingly, seen <span class="texhtml"><i>n</i><sub><i>i</i></sub> = <i>N</i> <i>p</i><sub><i>i</i></sub></span> times. The total amount of information we have received is 
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \sum _{i}{n_{i}\mathrm {I} (p_{i})}=-\sum _{i}{Np_{i}\log {p_{i}}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi>n</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">I</mi>
          </mrow>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>p</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mo stretchy="false">)</mo>
        </mrow>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>N</mi>
          <msub>
            <mi>p</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mi>log</mi>
          <mo>⁡<!-- ⁡ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>p</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{i}{n_{i}\mathrm {I} (p_{i})}=-\sum _{i}{Np_{i}\log {p_{i}}}}</annotation>
  </semantics>
</math></span><img src="8f3c2812c9f408e099e96a88260e0b35869d8327.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:29.338ex; height:5.509ex;" alt="{\displaystyle \sum _{i}{n_{i}\mathrm {I} (p_{i})}=-\sum _{i}{Np_{i}\log {p_{i}}}}"></span>.</dd></dl>
<p>The <i>average</i> amount of information that we receive per event is therefore
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle -\sum _{i}{p_{i}\log {p_{i}}}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo>−<!-- − --></mo>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi>p</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>i</mi>
            </mrow>
          </msub>
          <mi>log</mi>
          <mo>⁡<!-- ⁡ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>p</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle -\sum _{i}{p_{i}\log {p_{i}}}.}</annotation>
  </semantics>
</math></span><img src="c4afa1ac7b0697e03c07be092c0a978b9c5fb0f3.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:14.268ex; height:5.509ex;" alt="{\displaystyle -\sum _{i}{p_{i}\log {p_{i}}}.}"></span></dd></dl>
<h2><span class="mw-headline" id="Aspects">Aspects</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=5" title="Edit section: Aspects">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Relationship_to_thermodynamic_entropy">Relationship to thermodynamic entropy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=6" title="Edit section: Relationship to thermodynamic entropy">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Entropy_in_thermodynamics_and_information_theory" title="Entropy in thermodynamics and information theory">Entropy in thermodynamics and information theory</a></div>
<p>The inspiration for adopting the word <i>entropy</i> in information theory came from the close resemblance between Shannon's formula and very similar known formulae from <a href="https://en.wikipedia.org/wiki/Statistical_mechanics" title="Statistical mechanics">statistical mechanics</a>.
</p><p>In <a href="https://en.wikipedia.org/wiki/Statistical_thermodynamics" class="mw-redirect" title="Statistical thermodynamics">statistical thermodynamics</a> the most general formula for the thermodynamic <a href="https://en.wikipedia.org/wiki/Entropy" title="Entropy">entropy</a> <span class="texhtml"><i>S</i></span> of a <a href="https://en.wikipedia.org/wiki/Thermodynamic_system" title="Thermodynamic system">thermodynamic system</a> is the <a href="https://en.wikipedia.org/wiki/Gibbs_entropy" class="mw-redirect" title="Gibbs entropy">Gibbs entropy</a>,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle S=-k_{\text{B}}\sum p_{i}\ln p_{i}\,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>S</mi>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <msub>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>B</mtext>
          </mrow>
        </msub>
        <mo>∑<!-- ∑ --></mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mi>ln</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mspace width="thinmathspace"></mspace>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S=-k_{\text{B}}\sum p_{i}\ln p_{i}\,}</annotation>
  </semantics>
</math></span><img src="364eb4940717302ff9703b739fdf989aff1741c0.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.338ex; width:20.181ex; height:3.843ex;" alt="{\displaystyle S=-k_{\text{B}}\sum p_{i}\ln p_{i}\,}"></span></dd></dl>
<p>where <span class="texhtml"><i>k</i><sub>B</sub></span> is the <a href="https://en.wikipedia.org/wiki/Boltzmann_constant" title="Boltzmann constant">Boltzmann constant</a>, and <span class="texhtml"><i>p</i><sub><i>i</i></sub></span> is the probability of a <a href="https://en.wikipedia.org/wiki/Microstate_%28statistical_mechanics%29" title="Microstate (statistical mechanics)">microstate</a>. The Gibbs entropy was defined by <a href="https://en.wikipedia.org/wiki/J._Willard_Gibbs" class="mw-redirect" title="J. Willard Gibbs">J. Willard Gibbs</a> in 1878 after earlier work by <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann" title="Ludwig Boltzmann">Boltzmann</a> (1872).<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">[8]</a></sup>
</p><p>The Gibbs entropy translates over almost unchanged into the world of <a href="https://en.wikipedia.org/wiki/Quantum_physics" class="mw-redirect" title="Quantum physics">quantum physics</a> to give the <a href="https://en.wikipedia.org/wiki/Von_Neumann_entropy" title="Von Neumann entropy">von Neumann entropy</a>, introduced by <a href="https://en.wikipedia.org/wiki/John_von_Neumann" title="John von Neumann">John von Neumann</a> in 1927,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle S=-k_{\text{B}}\,{\rm {Tr}}(\rho \ln \rho )\,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>S</mi>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <msub>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>B</mtext>
          </mrow>
        </msub>
        <mspace width="thinmathspace"></mspace>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">T</mi>
            <mi mathvariant="normal">r</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>ρ<!-- ρ --></mi>
        <mi>ln</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mi>ρ<!-- ρ --></mi>
        <mo stretchy="false">)</mo>
        <mspace width="thinmathspace"></mspace>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S=-k_{\text{B}}\,{\rm {Tr}}(\rho \ln \rho )\,}</annotation>
  </semantics>
</math></span><img src="ac7e7b1e146fc9cc32c9a1de3f4ebc509e9af072.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:19.304ex; height:2.843ex;" alt="{\displaystyle S=-k_{\text{B}}\,{\rm {Tr}}(\rho \ln \rho )\,}"></span></dd></dl>
<p>where ρ is the <a href="https://en.wikipedia.org/wiki/Density_matrix" title="Density matrix">density matrix</a> of the quantum mechanical system and Tr is the <a href="https://en.wikipedia.org/wiki/Trace_%28linear_algebra%29" title="Trace (linear algebra)">trace</a>.
</p><p>At an everyday practical level the links between information 
entropy and thermodynamic entropy are not evident. Physicists and 
chemists are apt to be more interested in <i>changes</i> in entropy as a system spontaneously evolves away from its initial conditions, in accordance with the <a href="https://en.wikipedia.org/wiki/Second_law_of_thermodynamics" title="Second law of thermodynamics">second law of thermodynamics</a>, rather than an unchanging probability distribution. And, as the minuteness of <a href="https://en.wikipedia.org/wiki/Boltzmann%27s_constant" class="mw-redirect" title="Boltzmann's constant">Boltzmann's constant</a> <span class="texhtml"><i>k</i><sub>B</sub></span> indicates, the changes in <span class="texhtml"><i>S</i> / <i>k</i><sub>B</sub></span>
 for even tiny amounts of substances in chemical and physical processes 
represent amounts of entropy that are extremely large compared to 
anything in <a href="https://en.wikipedia.org/wiki/Data_compression" title="Data compression">data compression</a> or <a href="https://en.wikipedia.org/wiki/Signal_processing" title="Signal processing">signal processing</a>.
 Furthermore, in classical thermodynamics the entropy is defined in 
terms of macroscopic measurements and makes no reference to any 
probability distribution, which is central to the definition of 
information entropy.
</p><p>The connection between thermodynamics and what is now known as information theory was first made by <a href="https://en.wikipedia.org/wiki/Ludwig_Boltzmann" title="Ludwig Boltzmann">Ludwig Boltzmann</a> and expressed by his <a href="https://en.wikipedia.org/wiki/Boltzmann%27s_entropy_formula" title="Boltzmann's entropy formula">famous equation</a>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle S=k_{\text{B}}\ln(W)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>S</mi>
        <mo>=</mo>
        <msub>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mtext>B</mtext>
          </mrow>
        </msub>
        <mi>ln</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>W</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S=k_{\text{B}}\ln(W)}</annotation>
  </semantics>
</math></span><img src="3670e6a58c83376dcb968f67ad966d209c711932.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:13.776ex; height:2.843ex;" alt="{\displaystyle S=k_{\text{B}}\ln(W)}"></span></dd></dl>
<p>where <i>S</i> is the thermodynamic entropy of a particular 
macrostate (defined by thermodynamic parameters such as temperature, 
volume, energy, etc.), <i>W</i> is the number of microstates (various 
combinations of particles in various energy states) that can yield the 
given macrostate, and <i>k<sub>B</sub></i> is <a href="https://en.wikipedia.org/wiki/Boltzmann%27s_constant" class="mw-redirect" title="Boltzmann's constant">Boltzmann's constant</a>. It is assumed that each microstate is equally likely, so that the probability of a given microstate is <i>p<sub>i</sub> = 1/W</i>. When these probabilities are substituted into the above expression for the Gibbs entropy (or equivalently <i>k<sub>B</sub></i>
 times the Shannon entropy), Boltzmann's equation results. In 
information theoretic terms, the information entropy of a system is the 
amount of "missing" information needed to determine a microstate, given 
the macrostate.
</p><p>In the view of <a href="https://en.wikipedia.org/wiki/Edwin_Thompson_Jaynes" title="Edwin Thompson Jaynes">Jaynes</a> (1957), thermodynamic entropy, as explained by <a href="https://en.wikipedia.org/wiki/Statistical_mechanics" title="Statistical mechanics">statistical mechanics</a>, should be seen as an <i>application</i>
 of Shannon's information theory: the thermodynamic entropy is 
interpreted as being proportional to the amount of further Shannon 
information needed to define the detailed microscopic state of the 
system, that remains uncommunicated by a description solely in terms of 
the macroscopic variables of classical thermodynamics, with the constant
 of proportionality being just the <a href="https://en.wikipedia.org/wiki/Boltzmann_constant" title="Boltzmann constant">Boltzmann constant</a>.
 For example, adding heat to a system increases its thermodynamic 
entropy because it increases the number of possible microscopic states 
of the system that are consistent with the measurable values of its 
macroscopic variables, thus making any complete state description 
longer. (See article: <i><a href="https://en.wikipedia.org/wiki/Maximum_entropy_thermodynamics" title="Maximum entropy thermodynamics">maximum entropy thermodynamics</a></i>). <a href="https://en.wikipedia.org/wiki/Maxwell%27s_demon" title="Maxwell's demon">Maxwell's demon</a>
 can (hypothetically) reduce the thermodynamic entropy of a system by 
using information about the states of individual molecules; but, as <a href="https://en.wikipedia.org/wiki/Rolf_Landauer" title="Rolf Landauer">Landauer</a>
 (from 1961) and co-workers have shown, to function the demon himself 
must increase thermodynamic entropy in the process, by at least the 
amount of Shannon information he proposes to first acquire and store; 
and so the total thermodynamic entropy does not decrease (which resolves
 the paradox). <a href="https://en.wikipedia.org/wiki/Landauer%27s_principle" title="Landauer's principle">Landauer's principle</a>
 imposes a lower bound on the amount of heat a computer must generate to
 process a given amount of information, though modern computers are far 
less efficient.
</p>
<h3><span class="mw-headline" id="Entropy_as_information_content">Entropy as information content</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=7" title="Edit section: Entropy as information content">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem" title="Shannon's source coding theorem">Shannon's source coding theorem</a></div>
<p>Entropy is defined in the context of a probabilistic model. 
Independent fair coin flips have an entropy of 1 bit per flip. A source 
that always generates a long string of B's has an entropy of 0, since 
the next character will always be a 'B'.
</p><p>The entropy rate of a data source means the average number of <a href="https://en.wikipedia.org/wiki/Bit" title="Bit">bits</a>
 per symbol needed to encode it. Shannon's experiments with human 
predictors show an information rate between 0.6 and 1.3 bits per 
character in English;<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">[9]</a></sup> the <a href="https://en.wikipedia.org/wiki/PPM_compression_algorithm" class="mw-redirect" title="PPM compression algorithm">PPM compression algorithm</a> can achieve a compression ratio of 1.5 bits per character in English text.
</p><p>From the preceding example, note the following points:
</p>
<ol><li>The amount of entropy is not always an integer number of bits.</li>
<li>Many data bits may not convey information. For example, data 
structures often store information redundantly, or have identical 
sections regardless of the information in the data structure.</li></ol>
<p>Shannon's definition of entropy, when applied to an information 
source, can determine the minimum channel capacity required to reliably 
transmit the source as encoded binary digits (see caveat below in 
italics). The formula can be derived by calculating the mathematical 
expectation of the <i>amount of information</i> contained in a digit from the information source.  <i>See also</i> <a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Hartley_theorem" title="Shannon–Hartley theorem">Shannon–Hartley theorem</a>.
</p><p>Shannon's entropy measures the information contained in a message
 as opposed to the portion of the message that is determined (or 
predictable). <i>Examples of the latter include redundancy in language 
structure or statistical properties relating to the occurrence 
frequencies of letter or word pairs, triplets etc.</i> See <a href="https://en.wikipedia.org/wiki/Markov_chain" title="Markov chain">Markov chain</a>.
</p>
<h3><span class="mw-headline" id="Entropy_as_a_measure_of_diversity">Entropy as a measure of diversity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=8" title="Edit section: Entropy as a measure of diversity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Diversity_index" title="Diversity index">Diversity index</a></div>
<p>Entropy is one of several ways to measure diversity.  Specifically, Shannon entropy is the logarithm of <span class="texhtml"><sup>1</sup>D</span>, the <a href="https://en.wikipedia.org/wiki/True_diversity" class="mw-redirect" title="True diversity">true diversity</a> index with parameter equal to 1.
</p>
<h3><span class="mw-headline" id="Data_compression">Data compression</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=9" title="Edit section: Data compression">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Data_compression" title="Data compression">Data compression</a></div>
<p>Entropy effectively bounds the performance of the strongest lossless 
compression possible, which can be realized in theory by using the <a href="https://en.wikipedia.org/wiki/Typical_set" title="Typical set">typical set</a> or in practice using <a href="https://en.wikipedia.org/wiki/Huffman_coding" title="Huffman coding">Huffman</a>, <a href="https://en.wikipedia.org/wiki/LZW" class="mw-redirect" title="LZW">Lempel–Ziv</a> or <a href="https://en.wikipedia.org/wiki/Arithmetic_coding" title="Arithmetic coding">arithmetic coding</a>. See also <a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity" title="Kolmogorov complexity">Kolmogorov complexity</a>. In practice, compression algorithms deliberately include some judicious redundancy in the form of <a href="https://en.wikipedia.org/wiki/Checksum" title="Checksum">checksums</a> to protect against errors.
</p>
<h3><span id="World.27s_technological_capacity_to_store_and_communicate_information"></span><span class="mw-headline" id="World's_technological_capacity_to_store_and_communicate_information">World's technological capacity to store and communicate information</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=10" title="Edit section: World's technological capacity to store and communicate information">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A 2011 study in <i><a href="https://en.wikipedia.org/wiki/Science_%28journal%29" title="Science (journal)">Science</a></i>
 estimates the world's technological capacity to store and communicate 
optimally compressed information normalized on the most effective 
compression algorithms available in the year 2007, therefore estimating 
the entropy of the technologically available sources.<sup id="cite_ref-HilbertLopez2011_10-0" class="reference"><a href="#cite_note-HilbertLopez2011-10">[10]</a></sup> 
</p>
<table class="wikitable">
<caption>
All figures in entropically compressed <a href="https://en.wikipedia.org/wiki/Exabytes" class="mw-redirect" title="Exabytes">exabytes</a>
</caption>
<tbody><tr>
<th>Type of Information</th>
<th>1986</th>
<th>2007
</th></tr>
<tr>
<td>Storage</td>
<td>2.6</td>
<td>295
</td></tr>
<tr>
<td>Broadcast</td>
<td>432</td>
<td>1900
</td></tr>
<tr>
<td>Telecommunications</td>
<td>0.281</td>
<td>65
</td></tr></tbody></table>
<p>The authors estimate humankind technological capacity to store 
information (fully entropically compressed) in 1986 and again in 2007. 
They break the information into three categories—to store information on
 a medium, to receive information through a one-way <a href="https://en.wikipedia.org/wiki/Broadcast" class="mw-redirect" title="Broadcast">broadcast</a> networks, or to exchange information through two-way <a href="https://en.wikipedia.org/wiki/Telecommunication" title="Telecommunication">telecommunication</a> networks.<sup id="cite_ref-HilbertLopez2011_10-1" class="reference"><a href="#cite_note-HilbertLopez2011-10">[10]</a></sup>
</p>
<h3><span class="mw-headline" id="Limitations_of_entropy_as_information_content">Limitations of entropy as information content</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=11" title="Edit section: Limitations of entropy as information content">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>There are a number of entropy-related concepts that mathematically quantify information content in some way:
</p>
<ul><li>the <b><a href="https://en.wikipedia.org/wiki/Self-information" title="Self-information">self-information</a></b> of an individual message or symbol taken from a given probability distribution,</li>
<li>the <b>entropy</b> of a given probability distribution of messages or symbols, and</li>
<li>the <b><a href="https://en.wikipedia.org/wiki/Entropy_rate" title="Entropy rate">entropy rate</a></b> of a <a href="https://en.wikipedia.org/wiki/Stochastic_process" title="Stochastic process">stochastic process</a>.</li></ul>
<p>(The "rate of self-information" can also be defined for a particular 
sequence of messages or symbols generated by a given stochastic process:
 this will always be equal to the entropy rate in the case of a <a href="https://en.wikipedia.org/wiki/Stationary_process" title="Stationary process">stationary process</a>.) Other <a href="https://en.wikipedia.org/wiki/Quantities_of_information" title="Quantities of information">quantities of information</a> are also used to compare or relate different sources of information.
</p><p>It is important not to confuse the above concepts. Often it is 
only clear from context which one is meant. For example, when someone 
says that the "entropy" of the English language is about 1 bit per 
character, they are actually modeling the English language as a 
stochastic process and talking about its entropy <i>rate</i>. Shannon himself used the term in this way.
</p><p>However, if we use very large blocks, then the estimate of 
per-character entropy rate may become artificially low. This is because 
in reality, the probability distribution of the sequence is not knowable
 exactly; it is only an estimate. For example, suppose one considers the
 text of every book ever published as a sequence, with each symbol being
 the text of a complete book. If there are <span class="texhtml"><i>N</i></span> published books, and each book is only published once, the estimate of the probability of each book is <span class="texhtml">1/<i>N</i></span>, and the entropy (in bits) is <span class="texhtml">−log<sub>2</sub>(1/<i>N</i>) = log<sub>2</sub>(<i>N</i>)</span>. As a practical code, this corresponds to assigning each book a <a href="https://en.wikipedia.org/wiki/ISBN" class="mw-redirect" title="ISBN">unique identifier</a>
 and using it in place of the text of the book whenever one wants to 
refer to the book. This is enormously useful for talking about books, 
but it is not so useful for characterizing the information content of an
 individual book, or of language in general: it is not possible to 
reconstruct the book from its identifier without knowing the probability
 distribution, that is, the complete text of all the books. The key idea
 is that the complexity of the probabilistic model must be considered. <a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity" title="Kolmogorov complexity">Kolmogorov complexity</a>
 is a theoretical generalization of this idea that allows the 
consideration of the information content of a sequence independent of 
any particular probability model; it considers the shortest <a href="https://en.wikipedia.org/wiki/Computer_program" title="Computer program">program</a> for a <a href="https://en.wikipedia.org/wiki/Universal_computer" class="mw-redirect" title="Universal computer">universal computer</a>
 that outputs the sequence. A code that achieves the entropy rate of a 
sequence for a given model, plus the codebook (i.e. the probabilistic 
model), is one such program, but it may not be the shortest.
</p><p>For example, the Fibonacci sequence is 1, 1, 2, 3, 5, 8, 13, …. 
Treating the sequence as a message and each number as a symbol, there 
are almost as many symbols as there are characters in the message, 
giving an entropy of approximately <span class="texhtml">log<sub>2</sub>(<i>n</i>)</span>.
 So the first 128 symbols of the Fibonacci sequence has an entropy of 
approximately 7 bits/symbol. However, the sequence can be expressed 
using a formula [<span class="texhtml">F(<i>n</i>) = F(<i>n</i>−1) + F(<i>n</i>−2)</span> for <span class="texhtml"><i>n</i> = 3, 4, 5, …</span>, <span class="texhtml">F(1) =1</span>, <span class="texhtml">F(2) = 1</span>] and this formula has a much lower entropy and applies to any length of the Fibonacci sequence.
</p>
<h3><span class="mw-headline" id="Limitations_of_entropy_in_cryptography">Limitations of entropy in cryptography</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=12" title="Edit section: Limitations of entropy in cryptography">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In <a href="https://en.wikipedia.org/wiki/Cryptanalysis" title="Cryptanalysis">cryptanalysis</a>, entropy is often roughly used as a measure of the unpredictability of a cryptographic key, though its real <a href="https://en.wikipedia.org/wiki/Uncertainty_principle" title="Uncertainty principle">uncertainty</a>
 is unmeasurable. For example, a 128-bit key that is uniformly randomly 
generated has 128 bits of entropy. It also takes (on average) <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle 2^{128-1}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mn>2</mn>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>128</mn>
            <mo>−<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 2^{128-1}}</annotation>
  </semantics>
</math></span><img src="7ab6613c7afec22ebc5150343f8fde74a4d45f5e.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.961ex; height:2.676ex;" alt="2^{128-1}"></span>
 guesses to break by brute force. However, entropy fails to capture the 
number of guesses required if the possible keys are not chosen 
uniformly.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">[11]</a></sup><sup id="cite_ref-12" class="reference"><a href="#cite_note-12">[12]</a></sup> Instead, a measure called <i>guesswork</i> can be used to measure the effort required for a brute force attack.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">[13]</a></sup>
</p><p>Other problems may arise from non-uniform distributions used in cryptography. For example, consider a 1000000-digit binary <a href="https://en.wikipedia.org/wiki/One-time_pad" title="One-time pad">one-time pad</a>
 using exclusive or. If the pad has 1000000 bits of entropy, it is 
perfect. If the pad has 999999 bits of entropy, evenly distributed (each
 individual bit of the pad having 0.999999 bits of entropy) it may 
provide good security. But if the pad has 999999 bits of entropy, where 
the first bit is fixed and the remaining 999999 bits are perfectly 
random, then the first bit of the ciphertext will not be encrypted at 
all.
</p>
<h3><span class="mw-headline" id="Data_as_a_Markov_process">Data as a Markov process</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=13" title="Edit section: Data as a Markov process">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A common way to define entropy for text is based on the <a href="https://en.wikipedia.org/wiki/Markov_model" title="Markov model">Markov model</a> of text. For an order-0 source (each character is selected independent of the last characters), the binary entropy is:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} ({\mathcal {S}})=-\sum p_{i}\log _{2}p_{i},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">S</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <mo>∑<!-- ∑ --></mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} ({\mathcal {S}})=-\sum p_{i}\log _{2}p_{i},}</annotation>
  </semantics>
</math></span><img src="bb2be13cf9d493d512ed7374d2c5aec6d5139a76.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.338ex; width:23.466ex; height:3.843ex;" alt="{\displaystyle \mathrm {H} ({\mathcal {S}})=-\sum p_{i}\log _{2}p_{i},}"></span></dd></dl>
<p>where <span class="texhtml"><i>p</i><sub><i>i</i></sub></span> is the probability of <span class="texhtml"><i>i</i></span>. For a first-order <a href="https://en.wikipedia.org/wiki/Markov_source" class="mw-redirect" title="Markov source">Markov source</a> (one in which the probability of selecting a character is dependent only on the immediately preceding character), the <b><a href="https://en.wikipedia.org/wiki/Entropy_rate" title="Entropy rate">entropy rate</a></b> is:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} ({\mathcal {S}})=-\sum _{i}p_{i}\sum _{j}\ p_{i}(j)\log _{2}p_{i}(j),}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">S</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </munder>
        <mtext>&nbsp;</mtext>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>j</mi>
        <mo stretchy="false">)</mo>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>j</mi>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} ({\mathcal {S}})=-\sum _{i}p_{i}\sum _{j}\ p_{i}(j)\log _{2}p_{i}(j),}</annotation>
  </semantics>
</math></span><img src="9b9c0789aa85c0e6afa087de65116192f7ac966b.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:35.679ex; height:5.843ex;" alt="{\displaystyle \mathrm {H} ({\mathcal {S}})=-\sum _{i}p_{i}\sum _{j}\ p_{i}(j)\log _{2}p_{i}(j),}"></span><sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (April 2013)">citation needed</span></a></i>]</sup></dd></dl>
<p>where <span class="texhtml"><i>i</i></span> is a <b>state</b> (certain preceding characters) and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle p_{i}(j)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>j</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p_{i}(j)}</annotation>
  </semantics>
</math></span><img src="9f6dc803194d31f085708dacbf700848e2a4a3a5.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:4.826ex; height:2.843ex;" alt="p_{i}(j)"></span> is the probability of <span class="texhtml"><i>j</i></span> given <span class="texhtml"><i>i</i></span> as the previous character.
</p><p>For a second order Markov source, the entropy rate is
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} ({\mathcal {S}})=-\sum _{i}p_{i}\sum _{j}p_{i}(j)\sum _{k}p_{i,j}(k)\ \log _{2}\ p_{i,j}(k).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">S</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </munder>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </munder>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>j</mi>
        <mo stretchy="false">)</mo>
        <munder>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </munder>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>,</mo>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>k</mi>
        <mo stretchy="false">)</mo>
        <mtext>&nbsp;</mtext>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mtext>&nbsp;</mtext>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>,</mo>
            <mi>j</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>k</mi>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} ({\mathcal {S}})=-\sum _{i}p_{i}\sum _{j}p_{i}(j)\sum _{k}p_{i,j}(k)\ \log _{2}\ p_{i,j}(k).}</annotation>
  </semantics>
</math></span><img src="768dc6ea735c399de133e7a27e9549bbdbf48f66.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.338ex; width:47.902ex; height:5.843ex;" alt="{\displaystyle \mathrm {H} ({\mathcal {S}})=-\sum _{i}p_{i}\sum _{j}p_{i}(j)\sum _{k}p_{i,j}(k)\ \log _{2}\ p_{i,j}(k).}"></span></dd></dl>
<h3><span class="mw-headline" id="b-ary_entropy"><span class="texhtml"><i>b</i></span>-ary entropy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=14" title="Edit section: b-ary entropy">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In general the <b><span class="texhtml"><i>b</i></span>-ary entropy</b> of a source <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\mathcal {S}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">S</mi>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {S}}}</annotation>
  </semantics>
</math></span><img src="2302a18e269dbecc43c57c0c2aced3bfae15278d.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.492ex; height:2.176ex;" alt="{\mathcal {S}}"></span> <span class="texhtml">= (<i>S</i>, <i>P</i>)</span> with <a href="https://en.wikipedia.org/w/index.php?title=Source_alphabet&amp;action=edit&amp;redlink=1" class="new" title="Source alphabet (page does not exist)">source alphabet</a> <span class="texhtml"><i>S</i> = {<i>a</i><sub>1</sub>, …, <i>a</i><sub><i>n</i></sub></span>} and <a href="https://en.wikipedia.org/wiki/Discrete_probability_distribution" class="mw-redirect" title="Discrete probability distribution">discrete probability distribution</a> <span class="texhtml"><i>P</i> = {<i>p</i><sub>1</sub>, …, <i>p</i><sub><i>n</i></sub></span>} where <span class="texhtml"><i>p</i><sub><i>i</i></sub></span> is the probability of <span class="texhtml"><i>a</i><sub><i>i</i></sub></span> (say <span class="texhtml"><i>p</i><sub><i>i</i></sub> = <i>p</i>(<i>a</i><sub><i>i</i></sub>))</span> is defined by:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} _{b}({\mathcal {S}})=-\sum _{i=1}^{n}p_{i}\log _{b}p_{i},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>b</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">S</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>b</mi>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} _{b}({\mathcal {S}})=-\sum _{i=1}^{n}p_{i}\log _{b}p_{i},}</annotation>
  </semantics>
</math></span><img src="989dc3da5b5758e79df910fe3c8d977205fa89cb.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:24.287ex; height:6.843ex;" alt="{\displaystyle \mathrm {H} _{b}({\mathcal {S}})=-\sum _{i=1}^{n}p_{i}\log _{b}p_{i},}"></span></dd></dl>
<p>Note: the <span class="texhtml"><i>b</i></span> in "<span class="texhtml"><i>b</i></span>-ary entropy" is the number of different symbols of the <i>ideal alphabet</i> used as a standard yardstick to measure source alphabets. In information theory, two symbols are <a href="https://en.wikipedia.org/wiki/Necessary_and_sufficient" class="mw-redirect" title="Necessary and sufficient">necessary and sufficient</a> for an alphabet to encode information. Therefore, the default is to let <span class="texhtml"><i>b</i> = 2</span>
 ("binary entropy"). Thus, the entropy of the source alphabet, with its 
given empiric probability distribution, is a number equal to the number 
(possibly fractional) of symbols of the "ideal alphabet", with an 
optimal probability distribution, necessary to encode for each symbol of
 the source alphabet. Also note that "optimal probability distribution" 
here means a <a href="https://en.wikipedia.org/wiki/Uniform_distribution_%28discrete%29" class="mw-redirect" title="Uniform distribution (discrete)">uniform distribution</a>: a source alphabet with <span class="texhtml"><i>n</i></span> symbols has the highest possible entropy (for an alphabet with <span class="texhtml"><i>n</i></span> symbols) when the probability distribution of the alphabet is uniform. This optimal entropy turns out to be <span class="texhtml">log<sub><i>b</i></sub>(<i>n</i>)</span>.
</p>
<h2><span class="mw-headline" id="Efficiency">Efficiency</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=15" title="Edit section: Efficiency">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A source alphabet with non-uniform distribution will have less 
entropy than if those symbols had uniform distribution (i.e. the 
"optimized alphabet"). This deficiency in entropy can be expressed as a 
ratio called efficiency<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Inline_citation#When_you_must_use_inline_citations" title="Wikipedia:Inline citation"><span title="The text near this tag needs a citation. (July 2014)">This quote needs a citation</span></a></i>]</sup>:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \eta (X)=-\sum _{i=1}^{n}{\frac {p(x_{i})\log _{b}(p(x_{i}))}{\log _{b}(n)}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>η<!-- η --></mi>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mrow>
              <mi>p</mi>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
              <msub>
                <mi>log</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>b</mi>
                </mrow>
              </msub>
              <mo>⁡<!-- ⁡ --></mo>
              <mo stretchy="false">(</mo>
              <mi>p</mi>
              <mo stretchy="false">(</mo>
              <msub>
                <mi>x</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
              </msub>
              <mo stretchy="false">)</mo>
              <mo stretchy="false">)</mo>
            </mrow>
            <mrow>
              <msub>
                <mi>log</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>b</mi>
                </mrow>
              </msub>
              <mo>⁡<!-- ⁡ --></mo>
              <mo stretchy="false">(</mo>
              <mi>n</mi>
              <mo stretchy="false">)</mo>
            </mrow>
          </mfrac>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \eta (X)=-\sum _{i=1}^{n}{\frac {p(x_{i})\log _{b}(p(x_{i}))}{\log _{b}(n)}}}</annotation>
  </semantics>
</math></span><img src="bff2cc5cad60c3f5e32ccce36b5cb089f1335b73.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:31.153ex; height:6.843ex;" alt="\eta (X)=-\sum _{i=1}^{n}{\frac {p(x_{i})\log _{b}(p(x_{i}))}{\log _{b}(n)}}"></span><sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (July 2014)">clarification needed</span></a></i>]</sup></dd></dl>
<p>Efficiency has utility in quantifying the effective use of a 
communications channel. This formulation is also referred to as the 
normalized entropy, as the entropy is divided by the maximum entropy <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\log _{b}(n)}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <msub>
            <mi>log</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>b</mi>
            </mrow>
          </msub>
          <mo>⁡<!-- ⁡ --></mo>
          <mo stretchy="false">(</mo>
          <mi>n</mi>
          <mo stretchy="false">)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\log _{b}(n)}}</annotation>
  </semantics>
</math></span><img src="fe599301f4d93cbad754ccaef77b4053d14c2c93.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:7.113ex; height:2.843ex;" alt="{\log _{b}(n)}"></span>.
</p>
<h2><span class="mw-headline" id="Characterization">Characterization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=16" title="Edit section: Characterization">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Shannon entropy is <a href="https://en.wikipedia.org/wiki/Characterization_%28mathematics%29" title="Characterization (mathematics)">characterized</a> by a small number of criteria, listed below. Any definition of entropy satisfying these assumptions has the form
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle -K\sum _{i=1}^{n}p_{i}\log(p_{i})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo>−<!-- − --></mo>
        <mi>K</mi>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle -K\sum _{i=1}^{n}p_{i}\log(p_{i})}</annotation>
  </semantics>
</math></span><img src="592e29f3fe395f34b44511d7d93e6e5f3b220f6a.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:17.11ex; height:6.843ex;" alt="-K\sum _{i=1}^{n}p_{i}\log(p_{i})"></span></dd></dl>
<p>where <span class="texhtml"><i>K</i></span> is a constant corresponding to a choice of measurement units.
</p><p>In the following, <span class="texhtml"><i>p</i><sub><i>i</i></sub> = Pr(<i>X</i> = <i>x</i><sub><i>i</i></sub>)</span> and <span class="texhtml">Η<sub><i>n</i></sub>(<i>p</i><sub>1</sub>, …, <i>p</i><sub><i>n</i></sub>) = Η(<i>X</i>)</span>.
</p>
<h3><span class="mw-headline" id="Continuity">Continuity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=17" title="Edit section: Continuity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The measure should be <a href="https://en.wikipedia.org/wiki/Continuous_function" title="Continuous function">continuous</a>, so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount.
</p>
<h3><span class="mw-headline" id="Symmetry">Symmetry</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=18" title="Edit section: Symmetry">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The measure should be unchanged if the outcomes <span class="texhtml"><i>x</i><sub><i>i</i></sub></span> are re-ordered.
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} _{n}\left(p_{1},p_{2},\ldots \right)=\mathrm {H} _{n}\left(p_{2},p_{1},\ldots \right)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msub>
              <mi>p</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
              </mrow>
            </msub>
            <mo>,</mo>
            <msub>
              <mi>p</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msub>
            <mo>,</mo>
            <mo>…<!-- … --></mo>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msub>
              <mi>p</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>2</mn>
              </mrow>
            </msub>
            <mo>,</mo>
            <msub>
              <mi>p</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mn>1</mn>
              </mrow>
            </msub>
            <mo>,</mo>
            <mo>…<!-- … --></mo>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} _{n}\left(p_{1},p_{2},\ldots \right)=\mathrm {H} _{n}\left(p_{2},p_{1},\ldots \right)}</annotation>
  </semantics>
</math></span><img src="dba763f78449ae0ffef8f6d3a3b9f0043503624d.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:31.891ex; height:2.843ex;" alt="\mathrm {H} _{n}\left(p_{1},p_{2},\ldots \right)=\mathrm {H} _{n}\left(p_{2},p_{1},\ldots \right)"></span> etc.</dd></dl>
<h3><span class="mw-headline" id="Maximum">Maximum</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=19" title="Edit section: Maximum">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The measure should be maximal if all the outcomes are equally likely 
(uncertainty is highest when all possible events are equiprobable).
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} _{n}(p_{1},\ldots ,p_{n})\leq \mathrm {H} _{n}\left({\frac {1}{n}},\ldots ,{\frac {1}{n}}\right)=\log _{b}(n).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>…<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>≤<!-- ≤ --></mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mn>1</mn>
                <mi>n</mi>
              </mfrac>
            </mrow>
            <mo>,</mo>
            <mo>…<!-- … --></mo>
            <mo>,</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mn>1</mn>
                <mi>n</mi>
              </mfrac>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>b</mi>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>n</mi>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} 
_{n}(p_{1},\ldots ,p_{n})\leq \mathrm {H} _{n}\left({\frac 
{1}{n}},\ldots ,{\frac {1}{n}}\right)=\log _{b}(n).}</annotation>
  </semantics>
</math></span><img src="3309ba744597dc74c1178560a2627f6d3241b805.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.505ex; width:44.928ex; height:6.176ex;" alt="\mathrm {H} _{n}(p_{1},\ldots ,p_{n})\leq \mathrm {H} _{n}\left({\frac {1}{n}},\ldots ,{\frac {1}{n}}\right)=\log _{b}(n)."></span></dd></dl>
<p>For equiprobable events the entropy should increase with the number of outcomes.
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} _{n}{\bigg (}\underbrace {{\frac {1}{n}},\ldots ,{\frac {1}{n}}} _{n}{\bigg )}=\log _{b}(n)&lt;\log _{b}(n+1)=\mathrm {H} _{n+1}{\bigg (}\underbrace {{\frac {1}{n+1}},\ldots ,{\frac {1}{n+1}}} _{n+1}{\bigg )}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo maxsize="2.047em" minsize="2.047em">(</mo>
          </mrow>
        </mrow>
        <munder>
          <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
            <munder>
              <mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mfrac>
                    <mn>1</mn>
                    <mi>n</mi>
                  </mfrac>
                </mrow>
                <mo>,</mo>
                <mo>…<!-- … --></mo>
                <mo>,</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mfrac>
                    <mn>1</mn>
                    <mi>n</mi>
                  </mfrac>
                </mrow>
              </mrow>
              <mo>⏟<!-- ⏟ --></mo>
            </munder>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo maxsize="2.047em" minsize="2.047em">)</mo>
          </mrow>
        </mrow>
        <mo>=</mo>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>b</mi>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>n</mi>
        <mo stretchy="false">)</mo>
        <mo>&lt;</mo>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>b</mi>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>n</mi>
        <mo>+</mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo maxsize="2.047em" minsize="2.047em">(</mo>
          </mrow>
        </mrow>
        <munder>
          <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">
            <munder>
              <mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mfrac>
                    <mn>1</mn>
                    <mrow>
                      <mi>n</mi>
                      <mo>+</mo>
                      <mn>1</mn>
                    </mrow>
                  </mfrac>
                </mrow>
                <mo>,</mo>
                <mo>…<!-- … --></mo>
                <mo>,</mo>
                <mrow class="MJX-TeXAtom-ORD">
                  <mfrac>
                    <mn>1</mn>
                    <mrow>
                      <mi>n</mi>
                      <mo>+</mo>
                      <mn>1</mn>
                    </mrow>
                  </mfrac>
                </mrow>
              </mrow>
              <mo>⏟<!-- ⏟ --></mo>
            </munder>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </munder>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo maxsize="2.047em" minsize="2.047em">)</mo>
          </mrow>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} 
_{n}{\bigg (}\underbrace {{\frac {1}{n}},\ldots ,{\frac {1}{n}}} 
_{n}{\bigg )}=\log _{b}(n)&lt;\log _{b}(n+1)=\mathrm {H} _{n+1}{\bigg 
(}\underbrace {{\frac {1}{n+1}},\ldots ,{\frac {1}{n+1}}} _{n+1}{\bigg 
)}.}</annotation>
  </semantics>
</math></span><img src="3875ab180a74da2e2bc0e4c25aee6737d7e3cb15.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -5.505ex; width:70.323ex; height:9.176ex;" alt="\mathrm {H} _{n}{\bigg (}\underbrace {{\frac {1}{n}},\ldots ,{\frac {1}{n}}} _{n}{\bigg )}=\log _{b}(n)&lt;\log _{b}(n+1)=\mathrm {H} _{n+1}{\bigg (}\underbrace {{\frac {1}{n+1}},\ldots ,{\frac {1}{n+1}}} _{n+1}{\bigg )}."></span></dd></dl>
<p>For continuous random variables, the multivariate Gaussian is the distribution with maximum <a href="https://en.wikipedia.org/wiki/Differential_entropy" title="Differential entropy">differential entropy</a>.
</p>
<h3><span class="mw-headline" id="Additivity">Additivity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=20" title="Edit section: Additivity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The amount of entropy should be independent of how the process is regarded as being divided into parts.
</p><p>This last functional relationship characterizes the entropy of a 
system with sub-systems. It demands that the entropy of a system can be 
calculated from the entropies of its sub-systems if the interactions 
between the sub-systems are known.
</p><p>Given an ensemble of <span class="texhtml"><i>n</i></span> uniformly distributed elements that are divided into <span class="texhtml"><i>k</i></span> boxes (sub-systems) with <span class="texhtml"><i>b</i><sub>1</sub>, ..., <i>b</i><sub><i>k</i></sub></span>
 elements each, the entropy of the whole ensemble should be equal to the
 sum of the entropy of the system of boxes and the individual entropies 
of the boxes, each weighted with the probability of being in that 
particular box.
</p><p>For <a href="https://en.wikipedia.org/wiki/Positive_integers" class="mw-redirect" title="Positive integers">positive integers</a> <span class="texhtml"><i>b</i><sub><i>i</i></sub></span> where <span class="texhtml"><i>b</i><sub>1</sub> + … + <i>b</i><sub><i>k</i></sub> = <i>n</i></span>,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} _{n}\left({\frac {1}{n}},\ldots ,{\frac {1}{n}}\right)=\mathrm {H} _{k}\left({\frac {b_{1}}{n}},\ldots ,{\frac {b_{k}}{n}}\right)+\sum _{i=1}^{k}{\frac {b_{i}}{n}}\,\mathrm {H} _{b_{i}}\left({\frac {1}{b_{i}}},\ldots ,{\frac {1}{b_{i}}}\right).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mn>1</mn>
                <mi>n</mi>
              </mfrac>
            </mrow>
            <mo>,</mo>
            <mo>…<!-- … --></mo>
            <mo>,</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mn>1</mn>
                <mi>n</mi>
              </mfrac>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </msub>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <msub>
                  <mi>b</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>1</mn>
                  </mrow>
                </msub>
                <mi>n</mi>
              </mfrac>
            </mrow>
            <mo>,</mo>
            <mo>…<!-- … --></mo>
            <mo>,</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <msub>
                  <mi>b</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>k</mi>
                  </mrow>
                </msub>
                <mi>n</mi>
              </mfrac>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>+</mo>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <msub>
              <mi>b</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mi>n</mi>
          </mfrac>
        </mrow>
        <mspace width="thinmathspace"></mspace>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>b</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </msub>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mn>1</mn>
                <msub>
                  <mi>b</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                  </mrow>
                </msub>
              </mfrac>
            </mrow>
            <mo>,</mo>
            <mo>…<!-- … --></mo>
            <mo>,</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mn>1</mn>
                <msub>
                  <mi>b</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                  </mrow>
                </msub>
              </mfrac>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} 
_{n}\left({\frac {1}{n}},\ldots ,{\frac {1}{n}}\right)=\mathrm {H} 
_{k}\left({\frac {b_{1}}{n}},\ldots ,{\frac {b_{k}}{n}}\right)+\sum 
_{i=1}^{k}{\frac {b_{i}}{n}}\,\mathrm {H} _{b_{i}}\left({\frac 
{1}{b_{i}}},\ldots ,{\frac {1}{b_{i}}}\right).}</annotation>
  </semantics>
</math></span><img src="4624ac72030f84fee42c1843b1d930fe2b39b773.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:65.333ex; height:7.343ex;" alt="\mathrm {H} _{n}\left({\frac {1}{n}},\ldots ,{\frac {1}{n}}\right)=\mathrm {H} _{k}\left({\frac {b_{1}}{n}},\ldots ,{\frac {b_{k}}{n}}\right)+\sum _{i=1}^{k}{\frac {b_{i}}{n}}\,\mathrm {H} _{b_{i}}\left({\frac {1}{b_{i}}},\ldots ,{\frac {1}{b_{i}}}\right)."></span></dd></dl>
<p>Choosing <span class="texhtml"><i>k</i> = <i>n</i></span>, <span class="texhtml"><i>b</i><sub>1</sub> = … = <i>b</i><sub><i>n</i></sub> = 1</span> this implies that the entropy of a certain outcome is zero: <span class="texhtml">Η<sub>1</sub>(1) = 0</span>. This implies that the efficiency of a source alphabet with <span class="texhtml"><i>n</i></span> symbols can be defined simply as being equal to its <span class="texhtml"><i>n</i></span>-ary entropy. See also <a href="https://en.wikipedia.org/wiki/Redundancy_%28information_theory%29" title="Redundancy (information theory)">Redundancy (information theory)</a>.
</p>
<h2><span class="mw-headline" id="Further_properties">Further properties</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=21" title="Edit section: Further properties">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The Shannon entropy satisfies the following properties, for some of 
which it is useful to interpret entropy as the amount of information 
learned (or uncertainty eliminated) by revealing the value of a random 
variable <span class="texhtml"><i>X</i></span>:
</p>
<ul><li>Adding or removing an event with probability zero does not contribute to the entropy:</li></ul>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} _{n+1}(p_{1},\ldots ,p_{n},0)=\mathrm {H} _{n}(p_{1},\ldots ,p_{n})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>…<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <mn>0</mn>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>…<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} _{n+1}(p_{1},\ldots ,p_{n},0)=\mathrm {H} _{n}(p_{1},\ldots ,p_{n})}</annotation>
  </semantics>
</math></span><img src="e60fde6d5ed8599e40adfcffd78b354a796631d3.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:36.517ex; height:2.843ex;" alt="\mathrm {H} _{n+1}(p_{1},\ldots ,p_{n},0)=\mathrm {H} _{n}(p_{1},\ldots ,p_{n})"></span>.</dd></dl></dd></dl>
<ul><li>The entropy of a discrete random variable is a nonnegative number:</li></ul>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (X)\geq 0}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>≥<!-- ≥ --></mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} (X)\geq 0}</annotation>
  </semantics>
</math></span><img src="d214a0ad5c1f9351c77619eff9d4f91be4d756f2.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:9.793ex; height:2.843ex;" alt="{\displaystyle \mathrm {H} (X)\geq 0}"></span>.</dd></dl></dd></dl>
<ul><li>It can be confirmed using the <a href="https://en.wikipedia.org/wiki/Jensen_inequality" class="mw-redirect" title="Jensen inequality">Jensen inequality</a> that</li></ul>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (X)=\operatorname {E} \left[\log _{b}\left({\frac {1}{p(X)}}\right)\right]\leq \log _{b}\left(\operatorname {E} \left[{\frac {1}{p(X)}}\right]\right)=\log _{b}(n)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi mathvariant="normal">E</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mrow>
          <mo>[</mo>
          <mrow>
            <msub>
              <mi>log</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>b</mi>
              </mrow>
            </msub>
            <mo>⁡<!-- ⁡ --></mo>
            <mrow>
              <mo>(</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mfrac>
                  <mn>1</mn>
                  <mrow>
                    <mi>p</mi>
                    <mo stretchy="false">(</mo>
                    <mi>X</mi>
                    <mo stretchy="false">)</mo>
                  </mrow>
                </mfrac>
              </mrow>
              <mo>)</mo>
            </mrow>
          </mrow>
          <mo>]</mo>
        </mrow>
        <mo>≤<!-- ≤ --></mo>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>b</mi>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mi mathvariant="normal">E</mi>
            <mo>⁡<!-- ⁡ --></mo>
            <mrow>
              <mo>[</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mfrac>
                  <mn>1</mn>
                  <mrow>
                    <mi>p</mi>
                    <mo stretchy="false">(</mo>
                    <mi>X</mi>
                    <mo stretchy="false">)</mo>
                  </mrow>
                </mfrac>
              </mrow>
              <mo>]</mo>
            </mrow>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>b</mi>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>n</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} 
(X)=\operatorname {E} \left[\log _{b}\left({\frac 
{1}{p(X)}}\right)\right]\leq \log _{b}\left(\operatorname {E} 
\left[{\frac {1}{p(X)}}\right]\right)=\log _{b}(n)}</annotation>
  </semantics>
</math></span><img src="e25cc561059371b9c345934747aa8b3902daf136.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.671ex; width:56.268ex; height:6.343ex;" alt="\mathrm {H} (X)=\operatorname {E} \left[\log _{b}\left({\frac {1}{p(X)}}\right)\right]\leq \log _{b}\left(\operatorname {E} \left[{\frac {1}{p(X)}}\right]\right)=\log _{b}(n)"></span>.</dd></dl></dd>
<dd>This maximal entropy of <span class="texhtml">log<sub><i>b</i></sub>(<i>n</i>)</span>
 is effectively attained by a source alphabet having a uniform 
probability distribution: uncertainty is maximal when all possible 
events are equiprobable.</dd></dl>
<ul><li>The entropy or the amount of information revealed by evaluating <span class="texhtml">(<i>X</i>,<i>Y</i>)</span> (that is, evaluating <span class="texhtml"><i>X</i></span> and <span class="texhtml"><i>Y</i></span> simultaneously) is equal to the information revealed by conducting two consecutive experiments: first evaluating the value of <span class="texhtml"><i>Y</i></span>, then revealing the value of <span class="texhtml"><i>X</i></span> given that you know the value of <span class="texhtml"><i>Y</i></span>. This may be written as</li></ul>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (X,Y)=\mathrm {H} (X|Y)+\mathrm {H} (Y)=\mathrm {H} (Y|X)+\mathrm {H} (X).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo>,</mo>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Y</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} (X,Y)=\mathrm {H} (X|Y)+\mathrm {H} (Y)=\mathrm {H} (Y|X)+\mathrm {H} (X).}</annotation>
  </semantics>
</math></span><img src="9b2edc3776d6b2fe29eb6fd14f4f7ca2f449514f.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:47.627ex; height:2.843ex;" alt="\mathrm {H} (X,Y)=\mathrm {H} (X|Y)+\mathrm {H} (Y)=\mathrm {H} (Y|X)+\mathrm {H} (X)."></span></dd></dl></dd></dl>
<ul><li>If <span class="texhtml"><i>Y</i> = <i>f</i>(<i>X</i>)</span> where <span class="texhtml"><i>f</i></span> is a function, then <span class="texhtml">Η(<i>f</i>(<i>X</i>)|<i>X</i>) = 0</span>. Applying the previous formula to <span class="texhtml">Η(<i>X</i>, <i>f</i>(<i>X</i>))</span> yields</li></ul>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (X)+\mathrm {H} (f(X)|X)=\mathrm {H} (f(X))+\mathrm {H} (X|f(X)),}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} (X)+\mathrm {H} (f(X)|X)=\mathrm {H} (f(X))+\mathrm {H} (X|f(X)),}</annotation>
  </semantics>
</math></span><img src="c014b21f7e8367a660117530376fbb8707cf9588.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:46.073ex; height:2.843ex;" alt="\mathrm {H} (X)+\mathrm {H} (f(X)|X)=\mathrm {H} (f(X))+\mathrm {H} (X|f(X)),"></span></dd></dl></dd>
<dd>so <span class="texhtml">Η(<i>f</i>(<i>X</i>)) ≤ Η(<i>X</i>)</span>, thus the entropy of a variable can only decrease when the latter is passed through a function.</dd></dl>
<ul><li>If <span class="texhtml"><i>X</i></span> and <span class="texhtml"><i>Y</i></span> are two independent random variables, then knowing the value of <span class="texhtml"><i>Y</i></span> doesn't influence our knowledge of the value of <span class="texhtml"><i>X</i></span> (since the two don't influence each other by independence):</li></ul>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (X|Y)=\mathrm {H} (X).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} (X|Y)=\mathrm {H} (X).}</annotation>
  </semantics>
</math></span><img src="25cf8f0bd38a5185410bd3a8e680cefd715b123b.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:17.23ex; height:2.843ex;" alt="\mathrm {H} (X|Y)=\mathrm {H} (X)."></span></dd></dl></dd></dl>
<ul><li>The entropy of two simultaneous events is no more than the sum 
of the entropies of each individual event, and are equal if the two 
events are independent. More specifically, if <span class="texhtml"><i>X</i></span> and <span class="texhtml"><i>Y</i></span> are two random variables on the same probability space, and <span class="texhtml">(<i>X</i>, <i>Y</i>)</span> denotes their Cartesian product, then</li></ul>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (X,Y)\leq \mathrm {H} (X)+\mathrm {H} (Y).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo>,</mo>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
        <mo>≤<!-- ≤ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} (X,Y)\leq \mathrm {H} (X)+\mathrm {H} (Y).}</annotation>
  </semantics>
</math></span><img src="07b839fd91a05610c16d7e49eb86ec76fc231dc9.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:25.783ex; height:2.843ex;" alt="\mathrm {H} (X,Y)\leq \mathrm {H} (X)+\mathrm {H} (Y)."></span></dd></dl></dd></dl>
<ul><li>The entropy <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (p)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>p</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} (p)}</annotation>
  </semantics>
</math></span><img src="76d5ade34024658505d6df9f6114079845d005b5.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:4.722ex; height:2.843ex;" alt="{\displaystyle \mathrm {H} (p)}"></span> is <a href="https://en.wikipedia.org/wiki/Concave_function" title="Concave function">concave</a> in the probability mass function <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle p}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>p</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p}</annotation>
  </semantics>
</math></span><img src="81eac1e205430d1f40810df36a0edffdc367af36.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;" alt="p"></span>, i.e.</li></ul>
<dl><dd><dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (\lambda p_{1}+(1-\lambda )p_{2})\geq \lambda \mathrm {H} (p_{1})+(1-\lambda )\mathrm {H} (p_{2})}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>λ<!-- λ --></mi>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>+</mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>−<!-- − --></mo>
        <mi>λ<!-- λ --></mi>
        <mo stretchy="false">)</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>≥<!-- ≥ --></mo>
        <mi>λ<!-- λ --></mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>+</mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>−<!-- − --></mo>
        <mi>λ<!-- λ --></mi>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} (\lambda p_{1}+(1-\lambda )p_{2})\geq \lambda \mathrm {H} (p_{1})+(1-\lambda )\mathrm {H} (p_{2})}</annotation>
  </semantics>
</math></span><img src="3de6e437d5ad1c97d294e134c35edc35598185ea.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:45.376ex; height:2.843ex;" alt="{\displaystyle \mathrm {H} (\lambda p_{1}+(1-\lambda )p_{2})\geq \lambda \mathrm {H} (p_{1})+(1-\lambda )\mathrm {H} (p_{2})}"></span></dd></dl></dd></dl>
<p>for all probability mass functions <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle p_{1},p_{2}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>p</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p_{1},p_{2}}</annotation>
  </semantics>
</math></span><img src="e4d0cc608181855fb8882bbc2e95d7d4bfbc697f.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:5.571ex; height:2.009ex;" alt="{\displaystyle p_{1},p_{2}}"></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle 0\leq \lambda \leq 1}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mn>0</mn>
        <mo>≤<!-- ≤ --></mo>
        <mi>λ<!-- λ --></mi>
        <mo>≤<!-- ≤ --></mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 0\leq \lambda \leq 1}</annotation>
  </semantics>
</math></span><img src="49961b68da8f38e0534786d9b0faacfd9dd09fec.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.505ex; width:9.877ex; height:2.343ex;" alt="{\displaystyle 0\leq \lambda \leq 1}"></span>.
</p>
<h2><span class="mw-headline" id="Extending_discrete_entropy_to_the_continuous_case">Extending discrete entropy to the continuous case</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=22" title="Edit section: Extending discrete entropy to the continuous case">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Differential_entropy">Differential entropy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=23" title="Edit section: Differential entropy">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Differential_entropy" title="Differential entropy">Differential entropy</a></div>
<p>The Shannon entropy is restricted to random variables taking discrete
 values. The corresponding formula for a continuous random variable with
 <a href="https://en.wikipedia.org/wiki/Probability_density_function" title="Probability density function">probability density function</a> <span class="texhtml"><i>f</i>(<i>x</i>)</span> with finite or infinite support <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathbb {X} }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="double-struck">X</mi>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbb {X} }</annotation>
  </semantics>
</math></span><img src="01c0954f67c8e841542d7ac6a8472c4ea739824b.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.678ex; height:2.176ex;" alt="\mathbb {X} "></span> on the real line is defined by analogy, using the above form of the entropy as an expectation:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle h[f]=\operatorname {E} [-\ln(f(x))]=-\int _{\mathbb {X} }f(x)\ln(f(x))\,dx.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>h</mi>
        <mo stretchy="false">[</mo>
        <mi>f</mi>
        <mo stretchy="false">]</mo>
        <mo>=</mo>
        <mi mathvariant="normal">E</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">[</mo>
        <mo>−<!-- − --></mo>
        <mi>ln</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">]</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <msub>
          <mo>∫<!-- ∫ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="double-struck">X</mi>
            </mrow>
          </mrow>
        </msub>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mi>ln</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mspace width="thinmathspace"></mspace>
        <mi>d</mi>
        <mi>x</mi>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle h[f]=\operatorname {E} [-\ln(f(x))]=-\int _{\mathbb {X} }f(x)\ln(f(x))\,dx.}</annotation>
  </semantics>
</math></span><img src="4fdf6b07591f4dea485923a54568896dcfaf74fe.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.338ex; width:45.19ex; height:5.676ex;" alt="h[f]=\operatorname {E} [-\ln(f(x))]=-\int _{\mathbb {X} }f(x)\ln(f(x))\,dx."></span></dd></dl>
<p>This formula is usually referred to as the <b>continuous entropy</b>, or <a href="https://en.wikipedia.org/wiki/Differential_entropy" title="Differential entropy">differential entropy</a>. A precursor of the continuous entropy <span class="texhtml"><i>h</i>[<i>f</i>]</span> is the expression for the functional <span class="texhtml"><i>Η</i></span> in the <a href="https://en.wikipedia.org/wiki/H-theorem" title="H-theorem">H-theorem</a> of <a href="https://en.wikipedia.org/wiki/Boltzmann" class="mw-redirect" title="Boltzmann">Boltzmann</a>.
</p><p>Although the analogy between both functions is suggestive, the 
following question must be set: is the differential entropy a valid 
extension of the Shannon discrete entropy? Differential entropy lacks a 
number of properties that the Shannon discrete entropy has&nbsp;– it can
 even be negative&nbsp;– and thus corrections have been suggested, 
notably <a href="https://en.wikipedia.org/wiki/Limiting_density_of_discrete_points" title="Limiting density of discrete points">limiting density of discrete points</a>.
</p><p>To answer this question, we must establish a connection between the two functions:
</p><p>We wish to obtain a generally finite measure as the <a href="https://en.wikipedia.org/wiki/Bin_size" class="mw-redirect" title="Bin size">bin size</a> goes to zero. In the discrete case, the bin size is the (implicit) width of each of the <span class="texhtml"><i>n</i></span> (finite or infinite) bins whose probabilities are denoted by <span class="texhtml"><i>p</i><sub><i>n</i></sub></span>. As we generalize to the continuous domain, we must make this width explicit.
</p><p>To do this, start with a continuous function <span class="texhtml"><i>f</i></span> discretized into bins of size <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \Delta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi mathvariant="normal">Δ<!-- Δ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \Delta }</annotation>
  </semantics>
</math></span><img src="32769037c408874e1890f77554c65f39c523ebe2.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.936ex; height:2.176ex;" alt="\Delta "></span>.
By the mean-value theorem there exists a value <span class="texhtml"><i>x</i><sub><i>i</i></sub></span> in each bin such that
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle f(x_{i})\Delta =\int _{i\Delta }^{(i+1)\Delta }f(x)\,dx}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mi mathvariant="normal">Δ<!-- Δ --></mi>
        <mo>=</mo>
        <msubsup>
          <mo>∫<!-- ∫ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mi mathvariant="normal">Δ<!-- Δ --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>i</mi>
            <mo>+</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
            <mi mathvariant="normal">Δ<!-- Δ --></mi>
          </mrow>
        </msubsup>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mspace width="thinmathspace"></mspace>
        <mi>d</mi>
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(x_{i})\Delta =\int _{i\Delta }^{(i+1)\Delta }f(x)\,dx}</annotation>
  </semantics>
</math></span><img src="ec470e9b76c7f1e0803fec45b7da7d0bc5a3cd58.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.338ex; width:26.001ex; height:6.343ex;" alt="f(x_{i})\Delta =\int _{i\Delta }^{(i+1)\Delta }f(x)\,dx"></span></dd></dl>
<p>and thus the integral of the function <span class="texhtml"><i>f</i></span> can be approximated (in the Riemannian sense) by
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \int _{-\infty }^{\infty }f(x)\,dx=\lim _{\Delta \to 0}\sum _{i=-\infty }^{\infty }f(x_{i})\Delta }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mo>∫<!-- ∫ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>−<!-- − --></mo>
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
        </msubsup>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mspace width="thinmathspace"></mspace>
        <mi>d</mi>
        <mi>x</mi>
        <mo>=</mo>
        <munder>
          <mo movablelimits="true" form="prefix">lim</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">Δ<!-- Δ --></mi>
            <mo stretchy="false">→<!-- → --></mo>
            <mn>0</mn>
          </mrow>
        </munder>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mo>−<!-- − --></mo>
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
        </munderover>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mi mathvariant="normal">Δ<!-- Δ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \int _{-\infty }^{\infty }f(x)\,dx=\lim _{\Delta \to 0}\sum _{i=-\infty }^{\infty }f(x_{i})\Delta }</annotation>
  </semantics>
</math></span><img src="33029b4a5eb92c54ba167b879da9e68df5546c1f.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.171ex; width:31.811ex; height:7.009ex;" alt="\int _{-\infty }^{\infty }f(x)\,dx=\lim _{\Delta \to 0}\sum _{i=-\infty }^{\infty }f(x_{i})\Delta "></span></dd></dl>
<p>where this limit and "bin size goes to zero" are equivalent.
</p><p>We will denote
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} ^{\Delta }:=-\sum _{i=-\infty }^{\infty }f(x_{i})\Delta \log \left(f(x_{i})\Delta \right)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">Δ<!-- Δ --></mi>
          </mrow>
        </msup>
        <mo>:=</mo>
        <mo>−<!-- − --></mo>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mo>−<!-- − --></mo>
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
        </munderover>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mi mathvariant="normal">Δ<!-- Δ --></mi>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mrow>
          <mo>(</mo>
          <mrow>
            <mi>f</mi>
            <mo stretchy="false">(</mo>
            <msub>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo stretchy="false">)</mo>
            <mi mathvariant="normal">Δ<!-- Δ --></mi>
          </mrow>
          <mo>)</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} ^{\Delta }:=-\sum _{i=-\infty }^{\infty }f(x_{i})\Delta \log \left(f(x_{i})\Delta \right)}</annotation>
  </semantics>
</math></span><img src="647cb6441ff4d086b48475a6e08597ec58e0ba65.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.171ex; width:33.914ex; height:7.009ex;" alt="{\displaystyle \mathrm {H} ^{\Delta }:=-\sum _{i=-\infty }^{\infty }f(x_{i})\Delta \log \left(f(x_{i})\Delta \right)}"></span></dd></dl>
<p>and expanding the logarithm, we have
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} ^{\Delta }=-\sum _{i=-\infty }^{\infty }f(x_{i})\Delta \log(f(x_{i}))-\sum _{i=-\infty }^{\infty }f(x_{i})\Delta \log(\Delta ).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">H</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">Δ<!-- Δ --></mi>
          </mrow>
        </msup>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mo>−<!-- − --></mo>
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
        </munderover>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mi mathvariant="normal">Δ<!-- Δ --></mi>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mo>−<!-- − --></mo>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mo>−<!-- − --></mo>
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
        </munderover>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mi mathvariant="normal">Δ<!-- Δ --></mi>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi mathvariant="normal">Δ<!-- Δ --></mi>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} 
^{\Delta }=-\sum _{i=-\infty }^{\infty }f(x_{i})\Delta 
\log(f(x_{i}))-\sum _{i=-\infty }^{\infty }f(x_{i})\Delta \log(\Delta 
).}</annotation>
  </semantics>
</math></span><img src="6d62cb1b6725017dc994a3cd7e71a2bf728ba8cd.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.171ex; width:54.23ex; height:7.009ex;" alt="\mathrm {H} ^{\Delta }=-\sum _{i=-\infty }^{\infty }f(x_{i})\Delta \log(f(x_{i}))-\sum _{i=-\infty }^{\infty }f(x_{i})\Delta \log(\Delta )."></span></dd></dl>
<p>As Δ → 0, we have
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\begin{aligned}\sum _{i=-\infty }^{\infty }f(x_{i})\Delta &amp;\to \int _{-\infty }^{\infty }f(x)\,dx=1\\\sum _{i=-\infty }^{\infty }f(x_{i})\Delta \log(f(x_{i}))&amp;\to \int _{-\infty }^{\infty }f(x)\log f(x)\,dx.\end{aligned}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true">
            <mtr>
              <mtd>
                <munderover>
                  <mo>∑<!-- ∑ --></mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mo>−<!-- − --></mo>
                    <mi mathvariant="normal">∞<!-- ∞ --></mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="normal">∞<!-- ∞ --></mi>
                  </mrow>
                </munderover>
                <mi>f</mi>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
                <mi mathvariant="normal">Δ<!-- Δ --></mi>
              </mtd>
              <mtd>
                <mi></mi>
                <mo stretchy="false">→<!-- → --></mo>
                <msubsup>
                  <mo>∫<!-- ∫ --></mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mo>−<!-- − --></mo>
                    <mi mathvariant="normal">∞<!-- ∞ --></mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="normal">∞<!-- ∞ --></mi>
                  </mrow>
                </msubsup>
                <mi>f</mi>
                <mo stretchy="false">(</mo>
                <mi>x</mi>
                <mo stretchy="false">)</mo>
                <mspace width="thinmathspace"></mspace>
                <mi>d</mi>
                <mi>x</mi>
                <mo>=</mo>
                <mn>1</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <munderover>
                  <mo>∑<!-- ∑ --></mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                    <mo>=</mo>
                    <mo>−<!-- − --></mo>
                    <mi mathvariant="normal">∞<!-- ∞ --></mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="normal">∞<!-- ∞ --></mi>
                  </mrow>
                </munderover>
                <mi>f</mi>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
                <mi mathvariant="normal">Δ<!-- Δ --></mi>
                <mi>log</mi>
                <mo>⁡<!-- ⁡ --></mo>
                <mo stretchy="false">(</mo>
                <mi>f</mi>
                <mo stretchy="false">(</mo>
                <msub>
                  <mi>x</mi>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi>i</mi>
                  </mrow>
                </msub>
                <mo stretchy="false">)</mo>
                <mo stretchy="false">)</mo>
              </mtd>
              <mtd>
                <mi></mi>
                <mo stretchy="false">→<!-- → --></mo>
                <msubsup>
                  <mo>∫<!-- ∫ --></mo>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mo>−<!-- − --></mo>
                    <mi mathvariant="normal">∞<!-- ∞ --></mi>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mi mathvariant="normal">∞<!-- ∞ --></mi>
                  </mrow>
                </msubsup>
                <mi>f</mi>
                <mo stretchy="false">(</mo>
                <mi>x</mi>
                <mo stretchy="false">)</mo>
                <mi>log</mi>
                <mo>⁡<!-- ⁡ --></mo>
                <mi>f</mi>
                <mo stretchy="false">(</mo>
                <mi>x</mi>
                <mo stretchy="false">)</mo>
                <mspace width="thinmathspace"></mspace>
                <mi>d</mi>
                <mi>x</mi>
                <mo>.</mo>
              </mtd>
            </mtr>
          </mtable>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 
{\begin{aligned}\sum _{i=-\infty }^{\infty }f(x_{i})\Delta &amp;\to \int
 _{-\infty }^{\infty }f(x)\,dx=1\\\sum _{i=-\infty }^{\infty 
}f(x_{i})\Delta \log(f(x_{i}))&amp;\to \int _{-\infty }^{\infty 
}f(x)\log f(x)\,dx.\end{aligned}}}</annotation>
  </semantics>
</math></span><img src="67ee5dba44e44c7ada865b27d1c7585cf0920c87.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -6.505ex; width:48.053ex; height:14.176ex;" alt="{\begin{aligned}\sum _{i=-\infty }^{\infty }f(x_{i})\Delta &amp;\to \int _{-\infty }^{\infty }f(x)\,dx=1\\\sum _{i=-\infty }^{\infty }f(x_{i})\Delta \log(f(x_{i}))&amp;\to \int _{-\infty }^{\infty }f(x)\log f(x)\,dx.\end{aligned}}"></span></dd></dl>
<p>But note that <span class="texhtml">log(Δ) → −∞</span> as <span class="texhtml">Δ → 0</span>, therefore we need a special definition of the differential or continuous entropy:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle h[f]=\lim _{\Delta \to 0}\left(\mathrm {H} ^{\Delta }+\log \Delta \right)=-\int _{-\infty }^{\infty }f(x)\log f(x)\,dx,}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>h</mi>
        <mo stretchy="false">[</mo>
        <mi>f</mi>
        <mo stretchy="false">]</mo>
        <mo>=</mo>
        <munder>
          <mo movablelimits="true" form="prefix">lim</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">Δ<!-- Δ --></mi>
            <mo stretchy="false">→<!-- → --></mo>
            <mn>0</mn>
          </mrow>
        </munder>
        <mrow>
          <mo>(</mo>
          <mrow>
            <msup>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="normal">H</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="normal">Δ<!-- Δ --></mi>
              </mrow>
            </msup>
            <mo>+</mo>
            <mi>log</mi>
            <mo>⁡<!-- ⁡ --></mo>
            <mi mathvariant="normal">Δ<!-- Δ --></mi>
          </mrow>
          <mo>)</mo>
        </mrow>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <msubsup>
          <mo>∫<!-- ∫ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>−<!-- − --></mo>
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
        </msubsup>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mspace width="thinmathspace"></mspace>
        <mi>d</mi>
        <mi>x</mi>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle h[f]=\lim 
_{\Delta \to 0}\left(\mathrm {H} ^{\Delta }+\log \Delta \right)=-\int 
_{-\infty }^{\infty }f(x)\log f(x)\,dx,}</annotation>
  </semantics>
</math></span><img src="6d522982a9afc1539b88695a28b7aa3ffd2fa2d8.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.505ex; width:51.128ex; height:6.009ex;" alt="h[f]=\lim _{\Delta \to 0}\left(\mathrm {H} ^{\Delta }+\log \Delta \right)=-\int _{-\infty }^{\infty }f(x)\log f(x)\,dx,"></span></dd></dl>
<p>which is, as said before, referred to as the <b>differential entropy</b>. This means that the differential entropy <i>is not</i> a limit of the Shannon entropy for <span class="texhtml"><i>n</i> → ∞</span>. Rather, it differs from the limit of the Shannon entropy by an infinite offset (see also the article on <a href="https://en.wikipedia.org/wiki/Information_dimension" title="Information dimension">information dimension</a>)
</p>
<h3><span class="mw-headline" id="Limiting_density_of_discrete_points">Limiting density of discrete points</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=24" title="Edit section: Limiting density of discrete points">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Limiting_density_of_discrete_points" title="Limiting density of discrete points">Limiting density of discrete points</a></div>
<p>It turns out as a result that, unlike the Shannon entropy, the differential entropy is <i>not</i>
 in general a good measure of uncertainty or information. For example, 
the differential entropy can be negative; also it is not invariant under
 continuous co-ordinate transformations. This problem may be illustrated
 by a change of units when <i>x</i> is a dimensioned variable. <i>f(x)</i> will then have the units of <i>1/x</i>.
 The argument of the logarithm must be dimensionless, otherwise it is 
improper, so that the differential entropy as given above  will be 
improper. If <i>Δ</i> is some "standard" value of <i>x</i> (i.e. "bin size") and therefore has the same units, then a modified differential entropy may be written in proper form as:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle H=\int _{-\infty }^{\infty }f(x)\log(f(x)\,\Delta )\,dx}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>H</mi>
        <mo>=</mo>
        <msubsup>
          <mo>∫<!-- ∫ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>−<!-- − --></mo>
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="normal">∞<!-- ∞ --></mi>
          </mrow>
        </msubsup>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mspace width="thinmathspace"></mspace>
        <mi mathvariant="normal">Δ<!-- Δ --></mi>
        <mo stretchy="false">)</mo>
        <mspace width="thinmathspace"></mspace>
        <mi>d</mi>
        <mi>x</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle H=\int _{-\infty }^{\infty }f(x)\log(f(x)\,\Delta )\,dx}</annotation>
  </semantics>
</math></span><img src="70c3f0b44ae69f4e1a7e872922934b276e09c597.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.505ex; width:29.255ex; height:6.009ex;" alt="{\displaystyle H=\int _{-\infty }^{\infty }f(x)\log(f(x)\,\Delta )\,dx}"></span></dd></dl>
<p>and the result will be the same for any choice of units for <i>x</i>. In fact, the limit of discrete entropy as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle N\rightarrow \infty }">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>N</mi>
        <mo stretchy="false">→<!-- → --></mo>
        <mi mathvariant="normal">∞<!-- ∞ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle N\rightarrow \infty }</annotation>
  </semantics>
</math></span><img src="3c6337428441dbe11ca917f0d9115e8cf3a78f40.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:8.001ex; height:2.176ex;" alt="{\displaystyle N\rightarrow \infty }"></span> would also include a term of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \log(N)}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>N</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \log(N)}</annotation>
  </semantics>
</math></span><img src="2005d83c647855d23f2dd69dfa4472f57d1770b5.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:6.845ex; height:2.843ex;" alt="{\displaystyle \log(N)}"></span>,
 which would in general be infinite. This is expected, continuous 
variables would typically have infinite entropy when discretized. The <a href="https://en.wikipedia.org/wiki/Limiting_density_of_discrete_points" title="Limiting density of discrete points">limiting density of discrete points</a>
 is really a measure of how much easier a distribution is to describe 
than a distribution that is uniform over its quantization scheme.
</p>
<h3><span class="mw-headline" id="Relative_entropy">Relative entropy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=25" title="Edit section: Relative entropy">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="https://en.wikipedia.org/wiki/Generalized_relative_entropy" title="Generalized relative entropy">Generalized relative entropy</a></div>
<p>Another useful measure of entropy that works equally well in the discrete and the continuous case is the <b>relative entropy</b> of a distribution. It is defined as the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" title="Kullback–Leibler divergence">Kullback–Leibler divergence</a> from the distribution to a reference measure <span class="texhtml"><i>m</i></span> as follows. Assume that a probability distribution <span class="texhtml"><i>p</i></span> is <a href="https://en.wikipedia.org/wiki/Absolutely_continuous" class="mw-redirect" title="Absolutely continuous">absolutely continuous</a> with respect to a measure <span class="texhtml"><i>m</i></span>, i.e. is of the form <span class="texhtml"><i>p</i>(<i>dx</i>) = <i>f</i>(<i>x</i>)<i>m</i>(<i>dx</i>)</span> for some non-negative <span class="texhtml"><i>m</i></span>-integrable function <span class="texhtml"><i>f</i></span> with <span class="texhtml"><i>m</i></span>-integral 1, then the relative entropy can be defined as
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle D_{\mathrm {KL} }(p\|m)=\int \log(f(x))p(dx)=\int f(x)\log(f(x))m(dx).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>D</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="normal">K</mi>
              <mi mathvariant="normal">L</mi>
            </mrow>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>p</mi>
        <mo fence="false" stretchy="false">‖<!-- ‖ --></mo>
        <mi>m</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>∫<!-- ∫ --></mo>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mi>p</mi>
        <mo stretchy="false">(</mo>
        <mi>d</mi>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>∫<!-- ∫ --></mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
        <mi>m</mi>
        <mo stretchy="false">(</mo>
        <mi>d</mi>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle D_{\mathrm {KL} }(p\|m)=\int \log(f(x))p(dx)=\int f(x)\log(f(x))m(dx).}</annotation>
  </semantics>
</math></span><img src="53e69accb5de3429ecbe3c239ea3a3f1f650c453.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.338ex; width:57.771ex; height:5.676ex;" alt="D_{\mathrm {KL} }(p\|m)=\int \log(f(x))p(dx)=\int f(x)\log(f(x))m(dx)."></span></dd></dl>
<p>In this form the relative entropy generalises (up to change in sign) both the discrete entropy, where the measure <span class="texhtml"><i>m</i></span> is the <a href="https://en.wikipedia.org/wiki/Counting_measure" title="Counting measure">counting measure</a>, and the differential entropy, where the measure <span class="texhtml"><i>m</i></span> is the <a href="https://en.wikipedia.org/wiki/Lebesgue_measure" title="Lebesgue measure">Lebesgue measure</a>. If the measure <span class="texhtml"><i>m</i></span> is itself a probability distribution, the relative entropy is non-negative, and zero if <span class="texhtml"><i>p</i> = <i>m</i></span>
 as measures. It is defined for any measure space, hence coordinate 
independent and invariant under co-ordinate reparameterizations if one 
properly takes into account the transformation of the measure <span class="texhtml"><i>m</i></span>. The relative entropy, and implicitly entropy and differential entropy, do depend on the "reference" measure <span class="texhtml"><i>m</i></span>.
</p>
<h2><span class="mw-headline" id="Use_in_combinatorics">Use in combinatorics</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=26" title="Edit section: Use in combinatorics">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Entropy has become a useful quantity in <a href="https://en.wikipedia.org/wiki/Combinatorics" title="Combinatorics">combinatorics</a>.
</p>
<h3><span id="Loomis.E2.80.93Whitney_inequality"></span><span class="mw-headline" id="Loomis–Whitney_inequality">Loomis–Whitney inequality</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=27" title="Edit section: Loomis–Whitney inequality">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A simple example of this is an alternate proof of the <a href="https://en.wikipedia.org/wiki/Loomis%E2%80%93Whitney_inequality" title="Loomis–Whitney inequality">Loomis–Whitney inequality</a>: for every subset <span class="texhtml"><i>A</i> ⊆ <b>Z</b><sup><i>d</i></sup></span>, we have
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle |A|^{d-1}\leq \prod _{i=1}^{d}|P_{i}(A)|}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mi>A</mi>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">|</mo>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
            <mo>−<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mo>≤<!-- ≤ --></mo>
        <munderover>
          <mo>∏<!-- ∏ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>A</mi>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle |A|^{d-1}\leq \prod _{i=1}^{d}|P_{i}(A)|}</annotation>
  </semantics>
</math></span><img src="08c4e5f1167360fc69b7e341a6932d223c5d49cf.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:19.822ex; height:7.343ex;" alt="|A|^{d-1}\leq \prod _{i=1}^{d}|P_{i}(A)|"></span></dd></dl>
<p>where <span class="texhtml"><i>P</i><sub><i>i</i></sub></span> is the <a href="https://en.wikipedia.org/wiki/Orthogonal_projection" class="mw-redirect" title="Orthogonal projection">orthogonal projection</a> in the <span class="texhtml"><i>i</i></span>th coordinate:
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle P_{i}(A)=\{(x_{1},\ldots ,x_{i-1},x_{i+1},\ldots ,x_{d}):(x_{1},\ldots ,x_{d})\in A\}.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>A</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo fence="false" stretchy="false">{</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>…<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>−<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>…<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>:</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>…<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>∈<!-- ∈ --></mo>
        <mi>A</mi>
        <mo fence="false" stretchy="false">}</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P_{i}(A)=\{(x_{1},\ldots ,x_{i-1},x_{i+1},\ldots ,x_{d}):(x_{1},\ldots ,x_{d})\in A\}.}</annotation>
  </semantics>
</math></span><img src="60b2ab6204f4fcd05c996ac08d9c5ed7fc737635.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:56.694ex; height:2.843ex;" alt="P_{i}(A)=\{(x_{1},\ldots ,x_{i-1},x_{i+1},\ldots ,x_{d}):(x_{1},\ldots ,x_{d})\in A\}."></span></dd></dl>
<p>The proof follows as a simple corollary of <a href="https://en.wikipedia.org/wiki/Shearer%27s_inequality" title="Shearer's inequality">Shearer's inequality</a>: if <span class="texhtml"><i>X</i><sub>1</sub>, …, <i>X</i><sub><i>d</i></sub></span> are random variables and <span class="texhtml"><i>S</i><sub>1</sub>, …, <i>S</i><sub><i>n</i></sub></span> are subsets of <span class="texhtml">{1, …, <i>d</i></span>} such that every integer between 1 and <span class="texhtml"><i>d</i></span> lies in exactly <span class="texhtml"><i>r</i></span> of these subsets, then
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} [(X_{1},\ldots ,X_{d})]\leq {\frac {1}{r}}\sum _{i=1}^{n}\mathrm {H} [(X_{j})_{j\in S_{i}}]}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">[</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <mo>…<!-- … --></mo>
        <mo>,</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">]</mo>
        <mo>≤<!-- ≤ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mi>r</mi>
          </mfrac>
        </mrow>
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">[</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mo>∈<!-- ∈ --></mo>
            <msub>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </msub>
        <mo stretchy="false">]</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} [(X_{1},\ldots ,X_{d})]\leq {\frac {1}{r}}\sum _{i=1}^{n}\mathrm {H} [(X_{j})_{j\in S_{i}}]}</annotation>
  </semantics>
</math></span><img src="f6d219e5f3250e54a77586cd05731ff2ee0db7ee.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:36.564ex; height:6.843ex;" alt="\mathrm {H} [(X_{1},\ldots ,X_{d})]\leq {\frac {1}{r}}\sum _{i=1}^{n}\mathrm {H} [(X_{j})_{j\in S_{i}}]"></span></dd></dl>
<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle (X_{j})_{j\in S_{i}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mo>∈<!-- ∈ --></mo>
            <msub>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (X_{j})_{j\in S_{i}}}</annotation>
  </semantics>
</math></span><img src="228fae53ed29c1e7b36cac0b1b9281b02ec5c7b7.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:8.282ex; height:3.009ex;" alt="(X_{j})_{j\in S_{i}}"></span> is the Cartesian product of random variables <span class="texhtml"><i>X</i><sub><i>j</i></sub></span> with indexes <span class="texhtml"><i>j</i></span> in <span class="texhtml"><i>S</i><sub><i>i</i></sub></span> (so the dimension of this vector is equal to the size of <span class="texhtml"><i>S</i><sub><i>i</i></sub></span>).
</p><p>We sketch how Loomis–Whitney follows from this: Indeed, let <span class="texhtml"><i>X</i></span> be a uniformly distributed random variable with values in <span class="texhtml"><i>A</i></span> and so that each point in <span class="texhtml"><i>A</i></span> occurs with equal probability. Then (by the further properties of entropy mentioned above) <span class="texhtml">Η(<i>X</i>) = log|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>A</i></span>|</span>, where <span class="texhtml">|<span class="nowrap" style="padding-left:0.1em; padding-right:0.1em;"><i>A</i></span>|</span> denotes the cardinality of <span class="texhtml"><i>A</i></span>. Let <span class="texhtml"><i>S</i><sub><i>i</i></sub> = {1, 2, …, <i>i</i>−1, <i>i</i>+1, …, <i>d</i></span>}. The range of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle (X_{j})_{j\in S_{i}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mo>∈<!-- ∈ --></mo>
            <msub>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (X_{j})_{j\in S_{i}}}</annotation>
  </semantics>
</math></span><img src="228fae53ed29c1e7b36cac0b1b9281b02ec5c7b7.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:8.282ex; height:3.009ex;" alt="(X_{j})_{j\in S_{i}}"></span> is contained in <span class="texhtml"><i>P</i><sub><i>i</i></sub>(<i>A</i>)</span> and hence <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} [(X_{j})_{j\in S_{i}}]\leq \log |P_{i}(A)|}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">[</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
        <msub>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
            <mo>∈<!-- ∈ --></mo>
            <msub>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </msub>
        <mo stretchy="false">]</mo>
        <mo>≤<!-- ≤ --></mo>
        <mi>log</mi>
        <mo>⁡<!-- ⁡ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <msub>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <mi>A</mi>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} [(X_{j})_{j\in S_{i}}]\leq \log |P_{i}(A)|}</annotation>
  </semantics>
</math></span><img src="e775922b5cf13b31bbf4fec16e82d4f492378cba.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:24.914ex; height:3.009ex;" alt="\mathrm {H} [(X_{j})_{j\in S_{i}}]\leq \log |P_{i}(A)|"></span>.
 Now use this to bound the right side of Shearer's inequality and 
exponentiate the opposite sides of the resulting inequality you obtain.
</p>
<h3><span class="mw-headline" id="Approximation_to_binomial_coefficient">Approximation to binomial coefficient</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=28" title="Edit section: Approximation to binomial coefficient">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>For integers <span class="texhtml">0 &lt; <i>k</i> &lt; <i>n</i></span> let <span class="texhtml"><i>q</i> = <i>k</i>/<i>n</i></span>. Then
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\frac {2^{n\mathrm {H} (q)}}{n+1}}\leq {\tbinom {n}{k}}\leq 2^{n\mathrm {H} (q)},}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <msup>
              <mn>2</mn>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>n</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi mathvariant="normal">H</mi>
                </mrow>
                <mo stretchy="false">(</mo>
                <mi>q</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msup>
            <mrow>
              <mi>n</mi>
              <mo>+</mo>
              <mn>1</mn>
            </mrow>
          </mfrac>
        </mrow>
        <mo>≤<!-- ≤ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mstyle displaystyle="false" scriptlevel="0">
            <mrow>
              <mrow class="MJX-TeXAtom-OPEN">
                <mo maxsize="1.2em" minsize="1.2em">(</mo>
              </mrow>
              <mfrac linethickness="0">
                <mi>n</mi>
                <mi>k</mi>
              </mfrac>
              <mrow class="MJX-TeXAtom-CLOSE">
                <mo maxsize="1.2em" minsize="1.2em">)</mo>
              </mrow>
            </mrow>
          </mstyle>
        </mrow>
        <mo>≤<!-- ≤ --></mo>
        <msup>
          <mn>2</mn>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="normal">H</mi>
            </mrow>
            <mo stretchy="false">(</mo>
            <mi>q</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\frac {2^{n\mathrm {H} (q)}}{n+1}}\leq {\tbinom {n}{k}}\leq 2^{n\mathrm {H} (q)},}</annotation>
  </semantics>
</math></span><img src="fea64e76c8c6ee12f5f77c65312e7dff41a3d457.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.005ex; width:22.094ex; height:6.009ex;" alt="{\frac {2^{n\mathrm {H} (q)}}{n+1}}\leq {\tbinom {n}{k}}\leq 2^{n\mathrm {H} (q)},"></span></dd></dl>
<p>where 
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \mathrm {H} (q)=-q\log _{2}(q)-(1-q)\log _{2}(1-q).}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mi mathvariant="normal">H</mi>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>q</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo>−<!-- − --></mo>
        <mi>q</mi>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>q</mi>
        <mo stretchy="false">)</mo>
        <mo>−<!-- − --></mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>−<!-- − --></mo>
        <mi>q</mi>
        <mo stretchy="false">)</mo>
        <msub>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>⁡<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>−<!-- − --></mo>
        <mi>q</mi>
        <mo stretchy="false">)</mo>
        <mo>.</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathrm {H} (q)=-q\log _{2}(q)-(1-q)\log _{2}(1-q).}</annotation>
  </semantics>
</math></span><img src="1130c3e6f7a19506c5615b7c41ad7c12214f253f.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:39.554ex; height:2.843ex;" alt="\mathrm {H} (q)=-q\log _{2}(q)-(1-q)\log _{2}(1-q)."></span><sup id="cite_ref-14" class="reference"><a href="#cite_note-14">[14]</a></sup></dd></dl>
<p>Here is a sketch proof. Note that <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\tbinom {n}{k}}q^{qn}(1-q)^{n-nq}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mstyle displaystyle="false" scriptlevel="0">
            <mrow>
              <mrow class="MJX-TeXAtom-OPEN">
                <mo maxsize="1.2em" minsize="1.2em">(</mo>
              </mrow>
              <mfrac linethickness="0">
                <mi>n</mi>
                <mi>k</mi>
              </mfrac>
              <mrow class="MJX-TeXAtom-CLOSE">
                <mo maxsize="1.2em" minsize="1.2em">)</mo>
              </mrow>
            </mrow>
          </mstyle>
        </mrow>
        <msup>
          <mi>q</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>q</mi>
            <mi>n</mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>−<!-- − --></mo>
        <mi>q</mi>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>−<!-- − --></mo>
            <mi>n</mi>
            <mi>q</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\tbinom {n}{k}}q^{qn}(1-q)^{n-nq}}</annotation>
  </semantics>
</math></span><img src="d853b4c420a9d39ab14ea8a7abda8c7a4029ddf1.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:17.291ex; height:3.176ex;" alt="{\tbinom {n}{k}}q^{qn}(1-q)^{n-nq}"></span> is one term of the expression
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle \sum _{i=0}^{n}{\tbinom {n}{i}}q^{i}(1-q)^{n-i}=(q+(1-q))^{n}=1.}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munderover>
          <mo>∑<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>0</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <mrow class="MJX-TeXAtom-ORD">
          <mstyle displaystyle="false" scriptlevel="0">
            <mrow>
              <mrow class="MJX-TeXAtom-OPEN">
                <mo maxsize="1.2em" minsize="1.2em">(</mo>
              </mrow>
              <mfrac linethickness="0">
                <mi>n</mi>
                <mi>i</mi>
              </mfrac>
              <mrow class="MJX-TeXAtom-CLOSE">
                <mo maxsize="1.2em" minsize="1.2em">)</mo>
              </mrow>
            </mrow>
          </mstyle>
        </mrow>
        <msup>
          <mi>q</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>−<!-- − --></mo>
        <mi>q</mi>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>−<!-- − --></mo>
            <mi>i</mi>
          </mrow>
        </msup>
        <mo>=</mo>
        <mo stretchy="false">(</mo>
        <mi>q</mi>
        <mo>+</mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>−<!-- − --></mo>
        <mi>q</mi>
        <mo stretchy="false">)</mo>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msup>
        <mo>=</mo>
        <mn>1.</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{i=0}^{n}{\tbinom {n}{i}}q^{i}(1-q)^{n-i}=(q+(1-q))^{n}=1.}</annotation>
  </semantics>
</math></span><img src="a7648c43a3af8acd48df75de64dd2aed65ac80a3.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:40.509ex; height:6.843ex;" alt="\sum _{i=0}^{n}{\tbinom {n}{i}}q^{i}(1-q)^{n-i}=(q+(1-q))^{n}=1."></span></dd></dl>
<p>Rearranging gives the upper bound. For the lower bound one first 
shows, using some algebra, that it is the largest term in the summation.
 But then,
</p>
<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle {\tbinom {n}{k}}q^{qn}(1-q)^{n-nq}\geq {\tfrac {1}{n+1}}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mstyle displaystyle="false" scriptlevel="0">
            <mrow>
              <mrow class="MJX-TeXAtom-OPEN">
                <mo maxsize="1.2em" minsize="1.2em">(</mo>
              </mrow>
              <mfrac linethickness="0">
                <mi>n</mi>
                <mi>k</mi>
              </mfrac>
              <mrow class="MJX-TeXAtom-CLOSE">
                <mo maxsize="1.2em" minsize="1.2em">)</mo>
              </mrow>
            </mrow>
          </mstyle>
        </mrow>
        <msup>
          <mi>q</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>q</mi>
            <mi>n</mi>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>−<!-- − --></mo>
        <mi>q</mi>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mo>−<!-- − --></mo>
            <mi>n</mi>
            <mi>q</mi>
          </mrow>
        </msup>
        <mo>≥<!-- ≥ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mstyle displaystyle="false" scriptlevel="0">
            <mfrac>
              <mn>1</mn>
              <mrow>
                <mi>n</mi>
                <mo>+</mo>
                <mn>1</mn>
              </mrow>
            </mfrac>
          </mstyle>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\tbinom {n}{k}}q^{qn}(1-q)^{n-nq}\geq {\tfrac {1}{n+1}}}</annotation>
  </semantics>
</math></span><img src="04d7f3300680c8e13e3ac4a49938c8d044fd1f10.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.338ex; width:24.313ex; height:3.676ex;" alt="{\tbinom {n}{k}}q^{qn}(1-q)^{n-nq}\geq {\tfrac {1}{n+1}}"></span></dd></dl>
<p>since there are <span class="texhtml"><i>n</i> + 1</span> terms in the summation. Rearranging gives the lower bound.
</p><p>A nice interpretation of this is that the number of binary strings of length <span class="texhtml"><i>n</i></span> with exactly <span class="texhtml"><i>k</i></span> many 1's is approximately <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="{\displaystyle 2^{n\mathrm {H} (k/n)}}">
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mn>2</mn>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="normal">H</mi>
            </mrow>
            <mo stretchy="false">(</mo>
            <mi>k</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mo>/</mo>
            </mrow>
            <mi>n</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 2^{n\mathrm {H} (k/n)}}</annotation>
  </semantics>
</math></span><img src="b7c3ba47cc5436c389f86a3f617a191d0dbe4877.svg" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:7.558ex; height:2.843ex;" alt="2^{n\mathrm {H} (k/n)}"></span>.<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">[15]</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=29" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="navigation" aria-label="Portals" class="noprint portal plainlist tright" style="margin:0.5em 0 0.5em 1em;border:solid #aaa 1px">
<ul style="display:table;box-sizing:border-box;padding:0.1em;max-width:175px;background:#f9f9f9;font-size:85%;line-height:110%;font-style:italic;font-weight:bold">
<li style="display:table-row"><span style="display:table-cell;padding:0.2em;vertical-align:middle;text-align:center"><a href="https://en.wikipedia.org/wiki/File:Fisher_iris_versicolor_sepalwidth.svg" class="image"><img alt="icon" src="32px-Fisher_iris_versicolor_sepalwidth.png" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/48px-Fisher_iris_versicolor_sepalwidth.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/64px-Fisher_iris_versicolor_sepalwidth.svg.png 2x" data-file-width="822" data-file-height="567" width="32" height="22"></a></span><span style="display:table-cell;padding:0.2em 0.2em 0.2em 0.3em;vertical-align:middle"><a href="https://en.wikipedia.org/wiki/Portal:Statistics" title="Portal:Statistics">Statistics portal</a></span></li>
<li style="display:table-row"><span style="display:table-cell;padding:0.2em;vertical-align:middle;text-align:center"><a href="https://en.wikipedia.org/wiki/File:Crypto_key.svg" class="image"><img alt="icon" src="32px-Crypto_key.png" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Crypto_key.svg/48px-Crypto_key.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Crypto_key.svg/64px-Crypto_key.svg.png 2x" data-file-width="651" data-file-height="271" width="32" height="13"></a></span><span style="display:table-cell;padding:0.2em 0.2em 0.2em 0.3em;vertical-align:middle"><a href="https://en.wikipedia.org/wiki/Portal:Cryptography" title="Portal:Cryptography">Cryptography portal</a></span></li></ul></div>
<div class="div-col columns column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em;">
<ul><li><a href="https://en.wikipedia.org/wiki/Conditional_entropy" title="Conditional entropy">Conditional entropy</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cross_entropy" title="Cross entropy">Cross entropy</a>
 – is a measure of the average number of bits needed to identify an 
event from a set of possibilities between two probability distributions</li>
<li><a href="https://en.wikipedia.org/wiki/Diversity_index" title="Diversity index">Diversity index</a> – alternative approaches to quantifying diversity in a probability distribution</li>
<li><a href="https://en.wikipedia.org/wiki/Entropy_%28arrow_of_time%29" title="Entropy (arrow of time)">Entropy (arrow of time)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Entropy_encoding" title="Entropy encoding">Entropy encoding</a> – a coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols.</li>
<li><a href="https://en.wikipedia.org/wiki/Entropy_estimation" title="Entropy estimation">Entropy estimation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Entropy_power_inequality" title="Entropy power inequality">Entropy power inequality</a></li>
<li><a href="https://en.wikipedia.org/wiki/Entropy_rate" title="Entropy rate">Entropy rate</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fisher_information" title="Fisher information">Fisher information</a></li>
<li><a href="https://en.wikipedia.org/wiki/Graph_entropy" title="Graph entropy">Graph entropy</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hamming_distance" title="Hamming distance">Hamming distance</a></li>
<li><a href="https://en.wikipedia.org/wiki/History_of_entropy" title="History of entropy">History of entropy</a></li>
<li><a href="https://en.wikipedia.org/wiki/History_of_information_theory" title="History of information theory">History of information theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Information_geometry" title="Information geometry">Information geometry</a></li>
<li><a href="https://en.wikipedia.org/wiki/Joint_entropy" title="Joint entropy">Joint entropy</a> – is the measure how much entropy is contained in a joint system of two random variables.</li>
<li><a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Sinai_entropy" class="mw-redirect" title="Kolmogorov–Sinai entropy">Kolmogorov–Sinai entropy</a> in <a href="https://en.wikipedia.org/wiki/Dynamical_system" title="Dynamical system">dynamical systems</a></li>
<li><a href="https://en.wikipedia.org/wiki/Levenshtein_distance" title="Levenshtein distance">Levenshtein distance</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mutual_information" title="Mutual information">Mutual information</a></li>
<li><a href="https://en.wikipedia.org/wiki/Negentropy" title="Negentropy">Negentropy</a></li>
<li><a href="https://en.wikipedia.org/wiki/Perplexity" title="Perplexity">Perplexity</a></li>
<li><a href="https://en.wikipedia.org/wiki/Qualitative_variation" title="Qualitative variation">Qualitative variation</a> – other measures of <a href="https://en.wikipedia.org/wiki/Statistical_dispersion" title="Statistical dispersion">statistical dispersion</a> for <a href="https://en.wikipedia.org/wiki/Nominal_distributions" class="mw-redirect" title="Nominal distributions">nominal distributions</a></li>
<li><a href="https://en.wikipedia.org/wiki/Quantum_relative_entropy" title="Quantum relative entropy">Quantum relative entropy</a> – a measure of distinguishability between two quantum states.</li>
<li><a href="https://en.wikipedia.org/wiki/R%C3%A9nyi_entropy" title="Rényi entropy">Rényi entropy</a>
 – a generalization of Shannon entropy; it is one of a family of 
functionals for quantifying the diversity, uncertainty or randomness of a
 system.</li>
<li><a href="https://en.wikipedia.org/wiki/Randomness" title="Randomness">Randomness</a></li>
<li><a href="https://en.wikipedia.org/wiki/Shannon_index" class="mw-redirect" title="Shannon index">Shannon index</a></li>
<li><a href="https://en.wikipedia.org/wiki/Theil_index" title="Theil index">Theil index</a></li>
<li><a href="https://en.wikipedia.org/wiki/Typoglycemia" title="Typoglycemia">Typoglycemia</a></li></ul>
</div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=30" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist" style="list-style-type: decimal;">
<div class="mw-references-wrap mw-references-columns"><ol class="references">
<li id="cite_note-shannonPaper-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-shannonPaper_1-0"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="https://en.wikipedia.org/wiki/Claude_Shannon" title="Claude Shannon">Shannon, Claude E.</a> (July–October 1948). "<a href="https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication" title="A Mathematical Theory of Communication">A Mathematical Theory of Communication</a>". <i><a href="https://en.wikipedia.org/wiki/Bell_System_Technical_Journal" title="Bell System Technical Journal">Bell System Technical Journal</a></i>. <b>27</b> (3): 379–423. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1002/j.1538-7305.1948.tb01338.x">10.1002/j.1538-7305.1948.tb01338.x</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bell+System+Technical+Journal&amp;rft.atitle=A+Mathematical+Theory+of+Communication&amp;rft.volume=27&amp;rft.issue=3&amp;rft.pages=379-423&amp;rft.date=1948-07%2F1948-10&amp;rft_id=info%3Adoi%2F10.1002%2Fj.1538-7305.1948.tb01338.x&amp;rft.aulast=Shannon&amp;rft.aufirst=Claude+E.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29" class="Z3988"><span style="display:none;">&nbsp;</span></span> (<a rel="nofollow" class="external text" href="https://web.archive.org/web/20120615000000*/https://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-3-379.pdf">PDF</a>, archived from <a rel="nofollow" class="external text" href="http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-3-379.pdf">here</a>)</span>
</li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation journal">Shannon, Claude E. (1948). <a rel="nofollow" class="external text" href="http://math.harvard.edu/%7Ectm/home/text/others/shannon/entropy/entropy.pdf">"A Mathematical Theory of Communication"</a> <span style="font-size:85%;">(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Bell_Systems_Technical_Journal" class="mw-redirect" title="Bell Systems Technical Journal">Bell Systems Technical Journal</a></i>. <b>27</b>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bell+Systems+Technical+Journal&amp;rft.atitle=A+Mathematical+Theory+of+Communication&amp;rft.volume=27&amp;rft.date=1948&amp;rft.aulast=Shannon&amp;rft.aufirst=Claude+E.&amp;rft_id=http%3A%2F%2Fmath.harvard.edu%2F~ctm%2Fhome%2Ftext%2Fothers%2Fshannon%2Fentropy%2Fentropy.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29" class="Z3988"><span style="display:none;">&nbsp;</span></span>, July and October, pp. 379–423 and 623–656.</span>
</li>
<li id="cite_note-Schneier,_B_page_234-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-Schneier,_B_page_234_3-0"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Schneier, B: <i>Applied Cryptography</i>, Second edition, page 234. John Wiley and Sons.</span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation book">Borda, Monica (2011). <a rel="nofollow" class="external text" href="https://books.google.com/books?id=Lyte2yl1SPAC&amp;pg=PA11"><i>Fundamentals in Information Theory and Coding</i></a>. Springer. p.&nbsp;11. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-3-642-20346-6" title="Special:BookSources/978-3-642-20346-6">978-3-642-20346-6</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Fundamentals+in+Information+Theory+and+Coding&amp;rft.pages=11&amp;rft.pub=Springer&amp;rft.date=2011&amp;rft.isbn=978-3-642-20346-6&amp;rft.au=Borda%2C+Monica&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DLyte2yl1SPAC%26pg%3DPA11&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation book">Han, Te Sun &amp; Kobayashi, Kingo (2002). <a rel="nofollow" class="external text" href="https://books.google.com/books?id=VpRESN24Zj0C&amp;pg=PA19"><i>Mathematics of Information and Coding</i></a>. American Mathematical Society. pp.&nbsp;19–20. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-8218-4256-0" title="Special:BookSources/978-0-8218-4256-0">978-0-8218-4256-0</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Mathematics+of+Information+and+Coding&amp;rft.pages=19-20&amp;rft.pub=American+Mathematical+Society&amp;rft.date=2002&amp;rft.isbn=978-0-8218-4256-0&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DVpRESN24Zj0C%26pg%3DPA19&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29" class="Z3988"><span style="display:none;">&nbsp;</span></span><span class="citation-comment" style="display:none; color:#33aa33; margin-left:0.3em">CS1 maint: Uses authors parameter (<a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">link</a>) </span></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Schneider, T.D, <a rel="nofollow" class="external text" href="http://alum.mit.edu/www/toms/paper/primer/primer.pdf">Information theory primer with an appendix on logarithms</a>, National Cancer Institute, 14 April 2007.</span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation book">Carter, Tom (March 2014). <a rel="nofollow" class="external text" href="http://csustan.csustan.edu/%7Etom/Lecture-Notes/Information-Theory/info-lec.pdf"><i>An introduction to information theory and entropy</i></a> <span style="font-size:85%;">(PDF)</span>. Santa Fe<span class="reference-accessdate">. Retrieved <span class="nowrap">4 August</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=An+introduction+to+information+theory+and+entropy&amp;rft.place=Santa+Fe&amp;rft.date=2014-03&amp;rft.aulast=Carter&amp;rft.aufirst=Tom&amp;rft_id=http%3A%2F%2Fcsustan.csustan.edu%2F~tom%2FLecture-Notes%2FInformation-Theory%2Finfo-lec.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Compare:
 Boltzmann, Ludwig (1896, 1898). Vorlesungen über Gastheorie&nbsp;: 2 
Volumes – Leipzig 1895/98 UB: O 5262-6. English version: Lectures on gas
 theory. Translated by Stephen G. Brush (1964) Berkeley: University of 
California Press; (1995) New York: Dover <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-486-68455-5" title="Special:BookSources/0-486-68455-5">0-486-68455-5</a></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation web">Mark Nelson (24 August 2006). <a rel="nofollow" class="external text" href="http://marknelson.us/2006/08/24/the-hutter-prize/">"The Hutter Prize"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2008-11-27</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Hutter+Prize&amp;rft.date=2006-08-24&amp;rft.au=Mark+Nelson&amp;rft_id=http%3A%2F%2Fmarknelson.us%2F2006%2F08%2F24%2Fthe-hutter-prize%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-HilbertLopez2011-10"><span class="mw-cite-backlink">^ <a href="#cite_ref-HilbertLopez2011_10-0"><span class="cite-accessibility-label">Jump up to: </span><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-HilbertLopez2011_10-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.sciencemag.org/content/332/6025/60">"The World's Technological Capacity to Store, Communicate, and Compute Information"</a>, Martin Hilbert and Priscila López (2011), <a href="https://en.wikipedia.org/wiki/Science_%28journal%29" title="Science (journal)">Science</a>, 332(6025), 60–65; free access to the article through here: martinhilbert.net/WorldInfoCapacity.html</span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation conference">Massey, James (1994). <a rel="nofollow" class="external text" href="http://www.isiweb.ee.ethz.ch/archive/massey_pub/pdf/BI633.pdf">"Guessing and Entropy"</a> <span style="font-size:85%;">(PDF)</span>. <i>Proc. IEEE International Symposium on Information Theory</i><span class="reference-accessdate">. Retrieved <span class="nowrap">31 December</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Guessing+and+Entropy&amp;rft.btitle=Proc.+IEEE+International+Symposium+on+Information+Theory&amp;rft.date=1994&amp;rft.aulast=Massey&amp;rft.aufirst=James&amp;rft_id=http%3A%2F%2Fwww.isiweb.ee.ethz.ch%2Farchive%2Fmassey_pub%2Fpdf%2FBI633.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation conference">Malone, David; Sullivan, Wayne (2005). <a rel="nofollow" class="external text" href="http://www.maths.tcd.ie/%7Edwmalone/p/itt05.pdf">"Guesswork is not a Substitute for Entropy"</a> <span style="font-size:85%;">(PDF)</span>. <i>Proceedings of the Information Technology &amp; Telecommunications Conference</i><span class="reference-accessdate">. Retrieved <span class="nowrap">31 December</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Guesswork+is+not+a+Substitute+for+Entropy&amp;rft.btitle=Proceedings+of+the+Information+Technology+%26+Telecommunications+Conference&amp;rft.date=2005&amp;rft.aulast=Malone&amp;rft.aufirst=David&amp;rft.au=Sullivan%2C+Wayne&amp;rft_id=http%3A%2F%2Fwww.maths.tcd.ie%2F~dwmalone%2Fp%2Fitt05.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text"><cite class="citation conference">Pliam, John (1999). <a rel="nofollow" class="external text" href="https://link.springer.com/chapter/10.1007/3-540-46513-8_5">"Guesswork and variation distance as measures of cipher security"</a>. <i>International Workshop on Selected Areas in Cryptography</i><span class="reference-accessdate">. Retrieved <span class="nowrap">23 October</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Guesswork+and+variation+distance+as+measures+of+cipher+security&amp;rft.btitle=International+Workshop+on+Selected+Areas+in+Cryptography&amp;rft.date=1999&amp;rft.aulast=Pliam&amp;rft.aufirst=John&amp;rft_id=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F3-540-46513-8_5&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29" class="Z3988"><span style="display:none;">&nbsp;</span></span></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Aoki, New Approaches to Macroeconomic Modeling. page 43.</span>
</li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15"><span class="cite-accessibility-label">Jump up </span>^</a></b></span> <span class="reference-text">Probability and Computing, M. Mitzenmacher and E. Upfal, Cambridge University Press</span>
</li>
</ol></div></div>
<p><i>This article incorporates material from <a href="http://planetmath.org/node/30968" class="extiw" title="planetmath:30968">Shannon's entropy</a> on <a href="https://en.wikipedia.org/wiki/PlanetMath" title="PlanetMath">PlanetMath</a>, which is licensed under the <a href="https://en.wikipedia.org/wiki/Wikipedia:CC-BY-SA" class="mw-redirect" title="Wikipedia:CC-BY-SA">Creative Commons Attribution/Share-Alike License</a>.</i>
</p>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=31" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Textbooks_on_information_theory">Textbooks on information theory</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=32" title="Edit section: Textbooks on information theory">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul><li>Arndt, C. (2004), <i>Information Measures: Information and its Description in Science and Engineering</i>, Springer, <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-3-540-40855-0" title="Special:BookSources/978-3-540-40855-0">978-3-540-40855-0</a></li>
<li><a href="https://en.wikipedia.org/wiki/Thomas_M._Cover" title="Thomas M. Cover">Cover, T. M.</a>, Thomas, J. A. (2006), <i>Elements of information theory</i>, 2nd Edition. Wiley-Interscience. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-471-24195-4" title="Special:BookSources/0-471-24195-4">0-471-24195-4</a>.</li>
<li>Gray, R. M. (2011), <i>Entropy and Information Theory</i>, Springer.</li>
<li><a href="https://en.wikipedia.org/wiki/David_J._C._MacKay" title="David J. C. MacKay">MacKay, David J. C.</a>. <i><a rel="nofollow" class="external text" href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">Information Theory, Inference, and Learning Algorithms</a></i> Cambridge: Cambridge University Press, 2003. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-521-64298-1" title="Special:BookSources/0-521-64298-1">0-521-64298-1</a></li>
<li><cite class="citation book">Martin, Nathaniel F.G. &amp; England, James W. (2011). <a rel="nofollow" class="external text" href="https://books.google.com/books?id=_77lvx7y8joC"><i>Mathematical Theory of Entropy</i></a>. Cambridge University Press. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-17738-2" title="Special:BookSources/978-0-521-17738-2">978-0-521-17738-2</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Mathematical+Theory+of+Entropy&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2011&amp;rft.isbn=978-0-521-17738-2&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3D_77lvx7y8joC&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29" class="Z3988"><span style="display:none;">&nbsp;</span></span><span class="citation-comment" style="display:none; color:#33aa33; margin-left:0.3em">CS1 maint: Uses authors parameter (<a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">link</a>) </span></li>
<li><a href="https://en.wikipedia.org/wiki/Claude_Shannon" title="Claude Shannon">Shannon, C.E.</a>, <a href="https://en.wikipedia.org/wiki/Warren_Weaver" title="Warren Weaver">Weaver, W.</a> (1949) <i>The Mathematical Theory of Communication</i>, Univ of Illinois Press. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-252-72548-4" title="Special:BookSources/0-252-72548-4">0-252-72548-4</a></li>
<li>Stone, J. V. (2014), Chapter 1 of <a rel="nofollow" class="external text" href="http://jim-stone.staff.shef.ac.uk/BookInfoTheory/InfoTheoryBookMain.html"><i>Information Theory: A Tutorial Introduction</i></a>, University of Sheffield, England. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0956372857" title="Special:BookSources/978-0956372857">978-0956372857</a>.</li></ul>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit&amp;section=33" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table class="plainlinks metadata ambox ambox-style ambox-external_links" role="presentation"><tbody><tr><td class="mbox-image"><div style="width:52px"><img alt="" src="40px-Edit-clear.png" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/60px-Edit-clear.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/80px-Edit-clear.svg.png 2x" data-file-width="48" data-file-height="48" width="40" height="40"></div></td><td class="mbox-text"><div class="mbox-text-span">This article's <b>use of <a href="https://en.wikipedia.org/wiki/Wikipedia:External_links" title="Wikipedia:External links">external links</a> may not follow Wikipedia's policies or guidelines</b>.<span class="hide-when-compact"> Please <a class="external text" href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit">improve this article</a> by removing <a href="https://en.wikipedia.org/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_a_mirror_or_a_repository_of_links,_images,_or_media_files" title="Wikipedia:What Wikipedia is not">excessive</a> or <a href="https://en.wikipedia.org/wiki/Wikipedia:External_links" title="Wikipedia:External links">inappropriate</a> external links, and converting useful links where appropriate into <a href="https://en.wikipedia.org/wiki/Wikipedia:Citing_sources" title="Wikipedia:Citing sources">footnote references</a>.</span>  <small><i>(June 2015)</i></small><small class="hide-when-compact"><i> (<a href="https://en.wikipedia.org/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>
<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:auto;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%;text-align:left;"><tbody><tr><td style="padding-top:0.4em;line-height:1.2em"><a href="https://en.wikipedia.org/wiki/Wikipedia:LIBRARY" class="mw-redirect" title="Wikipedia:LIBRARY">Library resources</a> about <br> <b>Entropy (information theory)</b> <hr></td></tr><tr><td class="plainlist" style="padding:0 0.1em 0.4em">
<ul><li><a class="external text" href="https://tools.wmflabs.org/ftl/cgi-bin/ftl?st=wp&amp;su=Entropy+%28information+theory%29&amp;library=OLBP">Online books</a></li>
<li><a class="external text" href="https://tools.wmflabs.org/ftl/cgi-bin/ftl?st=wp&amp;su=Entropy+%28information+theory%29">Resources in your library</a></li>
<li><a class="external text" href="https://tools.wmflabs.org/ftl/cgi-bin/ftl?st=wp&amp;su=Entropy+%28information+theory%29&amp;library=0CHOOSE0">Resources in other libraries</a></li></ul></td>
</tr></tbody></table>
<ul><li><cite id="CITEREFHazewinkel2001" class="citation"><a href="https://en.wikipedia.org/wiki/Michiel_Hazewinkel" title="Michiel Hazewinkel">Hazewinkel, Michiel</a>, ed. (2001) [1994], <a rel="nofollow" class="external text" href="https://www.encyclopediaofmath.org/index.php?title=p/e035740">"Entropy"</a>, <i><a href="https://en.wikipedia.org/wiki/Encyclopedia_of_Mathematics" title="Encyclopedia of Mathematics">Encyclopedia of Mathematics</a></i>, Springer Science+Business Media B.V. / Kluwer Academic Publishers, <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&nbsp;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-1-55608-010-4" title="Special:BookSources/978-1-55608-010-4">978-1-55608-010-4</a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Entropy&amp;rft.btitle=Encyclopedia+of+Mathematics&amp;rft.pub=Springer+Science%2BBusiness+Media+B.V.+%2F+Kluwer+Academic+Publishers&amp;rft.date=2001&amp;rft.isbn=978-1-55608-010-4&amp;rft_id=https%3A%2F%2Fwww.encyclopediaofmath.org%2Findex.php%3Ftitle%3Dp%2Fe035740&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AEntropy+%28information+theory%29" class="Z3988"><span style="display:none;">&nbsp;</span></span></li>
<li><a rel="nofollow" class="external text" href="http://pespmc1.vub.ac.be/ENTRINFO.html">Introduction to entropy and information</a> on <a href="https://en.wikipedia.org/wiki/Principia_Cybernetica_Web" class="mw-redirect" title="Principia Cybernetica Web">Principia Cybernetica Web</a></li>
<li><i><a rel="nofollow" class="external text" href="http://www.mdpi.com/journal/entropy">Entropy</a></i> an interdisciplinary journal on all aspect of the entropy concept. Open access.</li>
<li><a rel="nofollow" class="external text" href="http://www.rheingold.com/texts/tft/6.html">Description of information entropy from "Tools for Thought" by Howard Rheingold</a></li>
<li><a rel="nofollow" class="external text" href="http://math.ucsd.edu/%7Ecrypto/java/ENTROPY/">A java applet representing Shannon's Experiment to Calculate the Entropy of English</a></li>
<li><a rel="nofollow" class="external text" href="http://www.autonlab.org/tutorials/infogain.html">Slides on information gain and entropy</a></li>
<li><a href="https://en.wikibooks.org/wiki/An_Intuitive_Guide_to_the_Concept_of_Entropy_Arising_in_Various_Sectors_of_Science" class="extiw" title="wikibooks:An Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science"><i>An Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science</i></a> – a wikibook on the interpretation of the concept of entropy.</li>
<li><a rel="nofollow" class="external text" href="https://researchspace.auckland.ac.nz/handle/2292/3427">Network Event Detection With Entropy Measures</a>,
 Dr. Raimund Eimann, University of Auckland, PDF; 5993&nbsp;kB – a PhD 
thesis demonstrating how entropy measures may be used in network anomaly
 detection.</li>
<li><a rel="nofollow" class="external text" href="http://rosettacode.org/wiki/Entropy"></a><a href="https://en.wikipedia.org/wiki/Rosetta_Code" title="Rosetta Code">Rosetta Code</a> repository of implementations of Shannon entropy in different programming languages.</li>
<li><a rel="nofollow" class="external text" href="http://tuvalu.santafe.edu/%7Esimon/it.pdf">"Information Theory for Intelligent People"</a>.
 Short introduction to the axioms of information theory, entropy, mutual
 information, Kullback–Liebler divergence, and Jensen–Shannon distance.</li>
<li><a rel="nofollow" class="external text" href="http://www.shannonentropy.netmark.pl/">Online tool for calculating entropy (plain text)</a></li>
<li><a rel="nofollow" class="external text" href="https://servertest.online/entropy">Online tool for calculating entropy (binary)</a></li></ul>
<div role="navigation" class="navbox" aria-labelledby="Data_compression_methods" style="padding:3px"><table class="nowraplinks hlist collapsible autocollapse navbox-inner mw-collapsible mw-made-collapsible mw-collapsed" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2"><span class="mw-collapsible-toggle mw-collapsible-toggle-default mw-collapsible-toggle-collapsed" role="button" tabindex="0"><a class="mw-collapsible-text">show</a></span><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="https://en.wikipedia.org/wiki/Template:Compression_methods" title="Template:Compression methods"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="https://en.wikipedia.org/wiki/Template_talk:Compression_methods" title="Template talk:Compression methods"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Compression_methods&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Data_compression_methods" style="font-size:114%;margin:0 4em"><a href="https://en.wikipedia.org/wiki/Data_compression" title="Data compression">Data compression</a> methods</div></th></tr><tr style="display: none;"><th scope="row" class="navbox-group" style="width:1%"><a href="https://en.wikipedia.org/wiki/Lossless_compression" title="Lossless compression">Lossless</a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;"><a href="https://en.wikipedia.org/wiki/Entropy_encoding" title="Entropy encoding">Entropy type</a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/Unary_coding" title="Unary coding">Unary</a></li>
<li><a href="https://en.wikipedia.org/wiki/Arithmetic_coding" title="Arithmetic coding">Arithmetic</a></li>
<li><a href="https://en.wikipedia.org/wiki/Asymmetric_numeral_systems" title="Asymmetric numeral systems">Asymmetric numeral systems</a></li>
<li><a href="https://en.wikipedia.org/wiki/Golomb_coding" title="Golomb coding">Golomb</a></li>
<li><a href="https://en.wikipedia.org/wiki/Huffman_coding" title="Huffman coding">Huffman</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Adaptive_Huffman_coding" title="Adaptive Huffman coding">Adaptive</a></li>
<li><a href="https://en.wikipedia.org/wiki/Canonical_Huffman_code" title="Canonical Huffman code">Canonical</a></li>
<li><a href="https://en.wikipedia.org/wiki/Modified_Huffman_coding" title="Modified Huffman coding">Modified</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Range_encoding" title="Range encoding">Range</a></li>
<li><a href="https://en.wikipedia.org/wiki/Shannon_coding" title="Shannon coding">Shannon</a></li>
<li><a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano_coding" title="Shannon–Fano coding">Shannon–Fano</a></li>
<li><a href="https://en.wikipedia.org/wiki/Shannon%E2%80%93Fano%E2%80%93Elias_coding" title="Shannon–Fano–Elias coding">Shannon–Fano–Elias</a></li>
<li><a href="https://en.wikipedia.org/wiki/Tunstall_coding" title="Tunstall coding">Tunstall</a></li>
<li><a href="https://en.wikipedia.org/wiki/Universal_code_%28data_compression%29" title="Universal code (data compression)">Universal</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Exponential-Golomb_coding" title="Exponential-Golomb coding">Exp-Golomb</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fibonacci_coding" title="Fibonacci coding">Fibonacci</a></li>
<li><a href="https://en.wikipedia.org/wiki/Elias_gamma_coding" title="Elias gamma coding">Gamma</a></li>
<li><a href="https://en.wikipedia.org/wiki/Levenshtein_coding" title="Levenshtein coding">Levenshtein</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;"><a href="https://en.wikipedia.org/wiki/Dictionary_coder" title="Dictionary coder">Dictionary type</a></th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" title="Byte pair encoding">Byte pair encoding</a></li>
<li><a href="https://en.wikipedia.org/wiki/DEFLATE" title="DEFLATE">DEFLATE</a></li>
<li><a href="https://en.wikipedia.org/wiki/Snappy_%28compression%29" title="Snappy (compression)">Snappy</a></li>
<li><a href="https://en.wikipedia.org/wiki/LZ77_and_LZ78" title="LZ77 and LZ78">Lempel–Ziv</a>
<ul><li><a href="https://en.wikipedia.org/wiki/LZ77_and_LZ78" title="LZ77 and LZ78">LZ77&nbsp;/ LZ78 (LZ1&nbsp;/ LZ2)</a></li>
<li><a href="https://en.wikipedia.org/wiki/LZFSE" title="LZFSE">LZFSE</a></li>
<li><a href="https://en.wikipedia.org/wiki/LZJB" title="LZJB">LZJB</a></li>
<li><a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Markov_chain_algorithm" title="Lempel–Ziv–Markov chain algorithm">LZMA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Oberhumer" title="Lempel–Ziv–Oberhumer">LZO</a></li>
<li><a href="https://en.wikipedia.org/wiki/LZRW" title="LZRW">LZRW</a></li>
<li><a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Stac" title="Lempel–Ziv–Stac">LZS</a></li>
<li><a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Storer%E2%80%93Szymanski" title="Lempel–Ziv–Storer–Szymanski">LZSS</a></li>
<li><a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Welch" title="Lempel–Ziv–Welch">LZW</a></li>
<li><a href="https://en.wikipedia.org/wiki/LZWL" title="LZWL">LZWL</a></li>
<li><a href="https://en.wikipedia.org/wiki/LZX_%28algorithm%29" title="LZX (algorithm)">LZX</a></li>
<li><a href="https://en.wikipedia.org/wiki/LZ4_%28compression_algorithm%29" title="LZ4 (compression algorithm)">LZ4</a></li>
<li><a href="https://en.wikipedia.org/wiki/Brotli" title="Brotli">Brotli</a></li>
<li><a href="https://en.wikipedia.org/wiki/Zstandard" title="Zstandard">Zstandard</a></li></ul></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;">Other types</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform" title="Burrows–Wheeler transform">BWT</a></li>
<li><a href="https://en.wikipedia.org/wiki/Context_tree_weighting" title="Context tree weighting">CTW</a></li>
<li><a href="https://en.wikipedia.org/wiki/Delta_encoding" title="Delta encoding">Delta</a></li>
<li><a href="https://en.wikipedia.org/wiki/Dynamic_Markov_compression" title="Dynamic Markov compression">DMC</a></li>
<li><a href="https://en.wikipedia.org/wiki/Move-to-front_transform" title="Move-to-front transform">MTF</a></li>
<li><a href="https://en.wikipedia.org/wiki/PAQ" title="PAQ">PAQ</a></li>
<li><a href="https://en.wikipedia.org/wiki/Prediction_by_partial_matching" title="Prediction by partial matching">PPM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Run-length_encoding" title="Run-length encoding">RLE</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr style="display: none;"><th scope="row" class="navbox-group" style="width:1%"><a href="https://en.wikipedia.org/wiki/Data_compression#Audio" title="Data compression">Audio</a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;">Concepts</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/Bit_rate" title="Bit rate">Bit rate</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Average_bitrate" title="Average bitrate">average (ABR)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Constant_bitrate" title="Constant bitrate">constant (CBR)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Variable_bitrate" title="Variable bitrate">variable (VBR)</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Companding" title="Companding">Companding</a></li>
<li><a href="https://en.wikipedia.org/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="https://en.wikipedia.org/wiki/Dynamic_range" title="Dynamic range">Dynamic range</a></li>
<li><a href="https://en.wikipedia.org/wiki/Latency_%28audio%29" title="Latency (audio)">Latency</a></li>
<li><a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem" title="Nyquist–Shannon sampling theorem">Nyquist–Shannon theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sampling_%28signal_processing%29" title="Sampling (signal processing)">Sampling</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sound_quality" title="Sound quality">Sound quality</a></li>
<li><a href="https://en.wikipedia.org/wiki/Speech_coding" title="Speech coding">Speech coding</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sub-band_coding" title="Sub-band coding">Sub-band coding</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;"><a href="https://en.wikipedia.org/wiki/Audio_codec" title="Audio codec">Codec</a> parts</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/A-law_algorithm" title="A-law algorithm">A-law</a></li>
<li><a href="https://en.wikipedia.org/wiki/%CE%9C-law_algorithm" title="Μ-law algorithm">μ-law</a></li>
<li><a href="https://en.wikipedia.org/wiki/Algebraic_code-excited_linear_prediction" title="Algebraic code-excited linear prediction">ACELP</a></li>
<li><a href="https://en.wikipedia.org/wiki/Adaptive_differential_pulse-code_modulation" title="Adaptive differential pulse-code modulation">ADPCM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Code-excited_linear_prediction" title="Code-excited linear prediction">CELP</a></li>
<li><a href="https://en.wikipedia.org/wiki/Differential_pulse-code_modulation" title="Differential pulse-code modulation">DPCM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fourier_transform" title="Fourier transform">Fourier transform</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_predictive_coding" title="Linear predictive coding">LPC</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Log_area_ratio" title="Log area ratio">LAR</a></li>
<li><a href="https://en.wikipedia.org/wiki/Line_spectral_pairs" title="Line spectral pairs">LSP</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Modified_discrete_cosine_transform" title="Modified discrete cosine transform">MDCT</a></li>
<li><a href="https://en.wikipedia.org/wiki/Psychoacoustics" title="Psychoacoustics">Psychoacoustic model</a></li>
<li><a href="https://en.wikipedia.org/wiki/Warped_linear_predictive_coding" title="Warped linear predictive coding">WLPC</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr style="display: none;"><th scope="row" class="navbox-group" style="width:1%"><a href="https://en.wikipedia.org/wiki/Image_compression" title="Image compression">Image</a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;">Concepts</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/Chroma_subsampling" title="Chroma subsampling">Chroma subsampling</a></li>
<li><a href="https://en.wikipedia.org/wiki/Coding_tree_unit" title="Coding tree unit">Coding tree unit</a></li>
<li><a href="https://en.wikipedia.org/wiki/Color_space" title="Color space">Color space</a></li>
<li><a href="https://en.wikipedia.org/wiki/Compression_artifact" title="Compression artifact">Compression artifact</a></li>
<li><a href="https://en.wikipedia.org/wiki/Image_resolution" title="Image resolution">Image resolution</a></li>
<li><a href="https://en.wikipedia.org/wiki/Macroblock" title="Macroblock">Macroblock</a></li>
<li><a href="https://en.wikipedia.org/wiki/Pixel" title="Pixel">Pixel</a></li>
<li><a href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio" title="Peak signal-to-noise ratio">PSNR</a></li>
<li><a href="https://en.wikipedia.org/wiki/Quantization_%28image_processing%29" title="Quantization (image processing)">Quantization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Standard_test_image" title="Standard test image">Standard test image</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;">Methods</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/Chain_code" title="Chain code">Chain code</a></li>
<li><a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform" title="Discrete cosine transform">DCT</a></li>
<li><a href="https://en.wikipedia.org/wiki/Embedded_Zerotrees_of_Wavelet_transforms" title="Embedded Zerotrees of Wavelet transforms">EZW</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fractal_compression" title="Fractal compression">Fractal</a></li>
<li><a href="https://en.wikipedia.org/wiki/Karhunen%E2%80%93Lo%C3%A8ve_theorem" title="Karhunen–Loève theorem">KLT</a></li>
<li><a href="https://en.wikipedia.org/wiki/Pyramid_%28image_processing%29" title="Pyramid (image processing)">LP</a></li>
<li><a href="https://en.wikipedia.org/wiki/Run-length_encoding" title="Run-length encoding">RLE</a></li>
<li><a href="https://en.wikipedia.org/wiki/Set_partitioning_in_hierarchical_trees" title="Set partitioning in hierarchical trees">SPIHT</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wavelet_transform#Wavelet_compression" title="Wavelet transform">Wavelet</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr style="display: none;"><th scope="row" class="navbox-group" style="width:1%"><a href="https://en.wikipedia.org/wiki/Data_compression#Video" title="Data compression">Video</a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;">Concepts</th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/Bit_rate" title="Bit rate">Bit rate</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Average_bitrate" title="Average bitrate">average (ABR)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Constant_bitrate" title="Constant bitrate">constant (CBR)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Variable_bitrate" title="Variable bitrate">variable (VBR)</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Display_resolution" title="Display resolution">Display resolution</a></li>
<li><a href="https://en.wikipedia.org/wiki/Film_frame" title="Film frame">Frame</a></li>
<li><a href="https://en.wikipedia.org/wiki/Frame_rate" title="Frame rate">Frame rate</a></li>
<li><a href="https://en.wikipedia.org/wiki/Video_compression_picture_types" title="Video compression picture types">Frame types</a></li>
<li><a href="https://en.wikipedia.org/wiki/Interlaced_video" title="Interlaced video">Interlace</a></li>
<li><a href="https://en.wikipedia.org/wiki/Video#Characteristics_of_video_streams" title="Video">Video characteristics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Video_quality" title="Video quality">Video quality</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:7.0em;font-weight:normal;"><a href="https://en.wikipedia.org/wiki/Video_codec" title="Video codec">Codec</a> parts</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/Lapped_transform" title="Lapped transform">Lapped transform</a></li>
<li><a href="https://en.wikipedia.org/wiki/Discrete_cosine_transform" title="Discrete cosine transform">DCT</a></li>
<li><a href="https://en.wikipedia.org/wiki/Deblocking_filter" title="Deblocking filter">Deblocking filter</a></li>
<li><a href="https://en.wikipedia.org/wiki/Motion_compensation" title="Motion compensation">Motion compensation</a></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr style="display: none;"><th scope="row" class="navbox-group" style="width:1%"><a href="https://en.wikipedia.org/wiki/Information_theory" title="Information theory">Theory</a></th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a class="mw-selflink selflink">Entropy</a></li>
<li><a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity" title="Kolmogorov complexity">Kolmogorov complexity</a></li>
<li><a href="https://en.wikipedia.org/wiki/Lossy_compression" title="Lossy compression">Lossy</a></li>
<li><a href="https://en.wikipedia.org/wiki/Quantization_%28signal_processing%29" title="Quantization (signal processing)">Quantization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Rate%E2%80%93distortion_theory" title="Rate–distortion theory">Rate–distortion</a></li>
<li><a href="https://en.wikipedia.org/wiki/Redundancy_%28information_theory%29" title="Redundancy (information theory)">Redundancy</a></li>
<li><a href="https://en.wikipedia.org/wiki/Timeline_of_information_theory" title="Timeline of information theory">Timeline of information theory</a></li></ul>
</div></td></tr><tr style="display: none;"><td class="navbox-abovebelow" colspan="2"><div>
<ul><li><img alt="Template" src="16px-Symbol_template_class.png" title="Template" srcset="//upload.wikimedia.org/wikipedia/en/thumb/5/5c/Symbol_template_class.svg/23px-Symbol_template_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/5/5c/Symbol_template_class.svg/31px-Symbol_template_class.svg.png 2x" data-file-width="180" data-file-height="185" width="16" height="16"> <a href="https://en.wikipedia.org/wiki/Template:Compression_formats" title="Template:Compression formats">Compression formats</a></li>
<li><img alt="Template" src="16px-Symbol_template_class.png" title="Template" srcset="//upload.wikimedia.org/wikipedia/en/thumb/5/5c/Symbol_template_class.svg/23px-Symbol_template_class.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/5/5c/Symbol_template_class.svg/31px-Symbol_template_class.svg.png 2x" data-file-width="180" data-file-height="185" width="16" height="16"> <a href="https://en.wikipedia.org/wiki/Template:Compression_software" title="Template:Compression software">Compression software (codecs)</a></li></ul>
</div></td></tr></tbody></table></div>
<div role="navigation" class="navbox" aria-labelledby="Authority_control_frameless_&amp;#124;text-top_&amp;#124;10px_&amp;#124;alt=Edit_this_at_Wikidata_&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q204570&amp;#124;Edit_this_at_Wikidata" style="padding:3px"><table class="nowraplinks hlist navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th id="Authority_control_frameless_&amp;#124;text-top_&amp;#124;10px_&amp;#124;alt=Edit_this_at_Wikidata_&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q204570&amp;#124;Edit_this_at_Wikidata" scope="row" class="navbox-group" style="width:1%"><a href="https://en.wikipedia.org/wiki/Help:Authority_control" title="Help:Authority control">Authority control</a> <a href="https://www.wikidata.org/wiki/Q204570" title="Edit this at Wikidata"><img alt="Edit this at Wikidata" src="10px-Blue_pencil.png" style="vertical-align: text-top" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/73/Blue_pencil.svg/15px-Blue_pencil.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/73/Blue_pencil.svg/20px-Blue_pencil.svg.png 2x" data-file-width="600" data-file-height="600" width="10" height="10"></a></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><span class="nowrap"><a href="https://en.wikipedia.org/wiki/Biblioteca_Nacional_de_Espa%C3%B1a" title="Biblioteca Nacional de España">BNE</a>: <span class="uid"><a rel="nofollow" class="external text" href="http://catalogo.bne.es/uhtbin/authoritybrowse.cgi?action=display&amp;authority_id=XX535116">XX535116</a></span></span></li>
<li><span class="nowrap"><a href="https://en.wikipedia.org/wiki/Biblioth%C3%A8que_nationale_de_France" title="Bibliothèque nationale de France">BNF</a>: <span class="uid"><a rel="nofollow" class="external text" href="http://catalogue.bnf.fr/ark:/12148/cb11985913j">cb11985913j</a> <a rel="nofollow" class="external text" href="http://data.bnf.fr/ark:/12148/cb11985913j">(data)</a></span></span></li>
<li><span class="nowrap"><a href="https://en.wikipedia.org/wiki/Integrated_Authority_File" title="Integrated Authority File">GND</a>: <span class="uid"><a rel="nofollow" class="external text" href="https://d-nb.info/gnd/4743861-7">4743861-7</a></span></span></li>
<li><span class="nowrap"><a href="https://en.wikipedia.org/wiki/Library_of_Congress_Control_Number" title="Library of Congress Control Number">LCCN</a>: <span class="uid"><a rel="nofollow" class="external text" href="http://id.loc.gov/authorities/subjects/sh85044152">sh85044152</a></span></span></li>
<li><span class="nowrap"><a href="https://en.wikipedia.org/wiki/National_Diet_Library" title="National Diet Library">NDL</a>: <span class="uid"><a rel="nofollow" class="external text" href="https://id.ndl.go.jp/auth/ndlna/01191172">01191172</a></span></span></li></ul>
</div></td></tr></tbody></table></div>

<!-- 
NewPP limit report
Parsed by mw1230
Cached time: 20180727165705
Cache expiry: 1900800
Dynamic content: false
CPU time usage: 0.696 seconds
Real time usage: 0.966 seconds
Preprocessor visited node count: 6496/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 122379/2097152 bytes
Template argument size: 13556/2097152 bytes
Highest expansion depth: 16/40
Expensive parser function count: 14/500
Unstrip recursion depth: 0/20
Unstrip post‐expand size: 16124/5000000 bytes
Number of Wikibase entities loaded: 1/400
Lua time usage: 0.206/10.000 seconds
Lua memory usage: 5.7 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  542.870      1 -total
 25.36%  137.656      1 Template:Reflist
 12.70%   68.953      4 Template:Ambox
  9.86%   53.519      6 Template:Isbn
  9.75%   52.927      1 Template:Multiple_issues
  8.56%   46.479      3 Template:Fix
  7.00%   37.974      1 Template:Authority_control
  6.91%   37.524      2 Template:Cite_journal
  6.25%   33.955      4 Template:Delink
  6.17%   33.469    184 Template:Math
-->
</div>
<!-- Saved in parser cache with key enwiki:pcache:idhash:15445-0!canonical!math=5 and timestamp 20180727165705 and revision id 852254681
 -->
<noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;oldid=852254681">https://en.wikipedia.org/w/index.php?title=Entropy_(information_theory)&amp;oldid=852254681</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="https://en.wikipedia.org/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Entropy_and_information" title="Category:Entropy and information">Entropy and information</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Information_theory" title="Category:Information theory">Information theory</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Statistical_randomness" title="Category:Statistical randomness">Statistical randomness</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">CS1 maint: Uses authors parameter</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_that_are_too_technical_from_November_2017" title="Category:Wikipedia articles that are too technical from November 2017">Wikipedia articles that are too technical from November 2017</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_articles_that_are_too_technical" title="Category:All articles that are too technical">All articles that are too technical</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_needing_expert_attention_from_November_2017" title="Category:Articles needing expert attention from November 2017">Articles needing expert attention from November 2017</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_articles_needing_expert_attention" title="Category:All articles needing expert attention">All articles needing expert attention</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_needing_additional_references_from_April_2012" title="Category:Articles needing additional references from April 2012">Articles needing additional references from April 2012</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_articles_needing_additional_references" title="Category:All articles needing additional references">All articles needing additional references</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_multiple_maintenance_issues" title="Category:Articles with multiple maintenance issues">Articles with multiple maintenance issues</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Use_dmy_dates_from_July_2013" title="Category:Use dmy dates from July 2013">Use dmy dates from July 2013</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_May_2018" title="Category:Articles with unsourced statements from May 2018">Articles with unsourced statements from May 2018</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_April_2013" title="Category:Articles with unsourced statements from April 2013">Articles with unsourced statements from April 2013</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_quotes" title="Category:Articles with unsourced quotes">Articles with unsourced quotes</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_needing_clarification_from_July_2014" title="Category:Wikipedia articles needing clarification from July 2014">Wikipedia articles needing clarification from July 2014</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_incorporating_text_from_PlanetMath" title="Category:Wikipedia articles incorporating text from PlanetMath">Wikipedia articles incorporating text from PlanetMath</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_external_links_cleanup_from_June_2015" title="Category:Wikipedia external links cleanup from June 2015">Wikipedia external links cleanup from June 2015</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_spam_cleanup_from_June_2015" title="Category:Wikipedia spam cleanup from June 2015">Wikipedia spam cleanup from June 2015</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_with_BNE_identifiers" title="Category:Wikipedia articles with BNE identifiers">Wikipedia articles with BNE identifiers</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_with_BNF_identifiers" title="Category:Wikipedia articles with BNF identifiers">Wikipedia articles with BNF identifiers</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_with_GND_identifiers" title="Category:Wikipedia articles with GND identifiers">Wikipedia articles with GND identifiers</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_with_LCCN_identifiers" title="Category:Wikipedia articles with LCCN identifiers">Wikipedia articles with LCCN identifiers</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_with_NDL_identifiers" title="Category:Wikipedia articles with NDL identifiers">Wikipedia articles with NDL identifiers</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="https://en.wikipedia.org/wiki/Special:MyTalk" title="Discussion about edits from this IP address [Alt+Shift+n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="https://en.wikipedia.org/wiki/Special:MyContributions" title="A list of edits made from this IP address [Alt+Shift+y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&amp;returnto=Entropy+%28information+theory%29&amp;returntoquery=oldid%3D852254681" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="https://en.wikipedia.org/w/index.php?title=Special:UserLogin&amp;returnto=Entropy+%28information+theory%29&amp;returntoquery=oldid%3D852254681" title="You're encouraged to log in; however, it's not mandatory. [Alt+Shift+o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><span><a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29" title="View the content page [Alt+Shift+c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="https://en.wikipedia.org/wiki/Talk:Entropy_%28information_theory%29" rel="discussion" title="Discussion about the content page [Alt+Shift+t]" accesskey="t">Talk</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<input class="vectorMenuCheckbox" aria-labelledby="p-variants-label" type="checkbox">
						<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=edit" title="Edit this page [Alt+Shift+e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=history" title="Past revisions of this page [Alt+Shift+h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label" style="">
						<input class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" type="checkbox">
						<h3 id="p-cactions-label"><span>More</span></h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input name="search" placeholder="Search Wikipedia" title="Search Wikipedia [Alt+Shift+f]" accesskey="f" id="searchInput" tabindex="1" autocomplete="off" type="search"><input value="Special:Search" name="title" type="hidden"><input name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton" type="submit">							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">
			<h3 id="p-navigation-label">Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page [Alt+Shift+z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="https://en.wikipedia.org/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="https://en.wikipedia.org/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="https://en.wikipedia.org/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="https://en.wikipedia.org/wiki/Special:Random" title="Load a random article [Alt+Shift+x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="https://shop.wikimedia.org/" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">
			<h3 id="p-interaction-label">Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="https://en.wikipedia.org/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="https://en.wikipedia.org/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="https://en.wikipedia.org/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [Alt+Shift+r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">
			<h3 id="p-tb-label">Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="https://en.wikipedia.org/wiki/Special:WhatLinksHere/Entropy_%28information_theory%29" title="List of all English Wikipedia pages containing links to this page [Alt+Shift+j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Entropy_%28information_theory%29" rel="nofollow" title="Recent changes in pages linked from this page [Alt+Shift+k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [Alt+Shift+u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="https://en.wikipedia.org/wiki/Special:SpecialPages" title="A list of all special pages [Alt+Shift+q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;oldid=852254681" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q204570" title="Link to connected data repository item [Alt+Shift+g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=Entropy_%28information_theory%29&amp;id=852254681" title="Information on how to cite this page">Cite this page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">
			<h3 id="p-coll-print_export-label">Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Entropy+%28information+theory%29">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="https://en.wikipedia.org/w/index.php?title=Special:ElectronPdf&amp;page=Entropy+%28information+theory%29&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="https://en.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;printable=yes" title="Printable version of this page [Alt+Shift+p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-wikibase-otherprojects" aria-labelledby="p-wikibase-otherprojects-label">
			<h3 id="p-wikibase-otherprojects-label">In other projects</h3>
			<div class="body">
								<ul>
					<li class="wb-otherproject-link wb-otherproject-commons"><a href="https://commons.wikimedia.org/wiki/Category:Entropy_and_information" hreflang="en">Wikimedia Commons</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label"><button class="uls-settings-trigger" title="Language settings"></button>
			<h3 id="p-lang-label">Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-ar" style=""><a href="https://ar.wikipedia.org/wiki/%D8%A7%D8%B9%D8%AA%D9%84%D8%A7%D8%AC_%28%D9%85%D8%B9%D9%84%D9%88%D9%85%D8%A7%D8%AA%29" title="اعتلاج (معلومات) – Arabic" hreflang="ar" class="interlanguage-link-target" lang="ar">العربية</a></li><li class="interlanguage-link interwiki-bg" style="display: none;"><a href="https://bg.wikipedia.org/wiki/%D0%95%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%8F_%D0%BD%D0%B0_%D0%A8%D0%B0%D0%BD%D1%8A%D0%BD" title="Ентропия на Шанън – Bulgarian" hreflang="bg" class="interlanguage-link-target" lang="bg">Български</a></li><li class="interlanguage-link interwiki-bar" style="display: none;"><a href="https://bar.wikipedia.org/wiki/Entropie_%28Informationstheorie%29" title="Entropie (Informationstheorie) – Bavarian" hreflang="bar" class="interlanguage-link-target" lang="bar">Boarisch</a></li><li class="interlanguage-link interwiki-bs" style="display: none;"><a href="https://bs.wikipedia.org/wiki/Entropija_%28teorija_informacija%29" title="Entropija (teorija informacija) – Bosnian" hreflang="bs" class="interlanguage-link-target" lang="bs">Bosanski</a></li><li class="interlanguage-link interwiki-ca" style="display: none;"><a href="https://ca.wikipedia.org/wiki/Entropia_de_Shannon" title="Entropia de Shannon – Catalan" hreflang="ca" class="interlanguage-link-target" lang="ca">Català</a></li><li class="interlanguage-link interwiki-cy" style="display: none;"><a href="https://cy.wikipedia.org/wiki/Entropi_gwybodaeth" title="Entropi gwybodaeth – Welsh" hreflang="cy" class="interlanguage-link-target" lang="cy">Cymraeg</a></li><li class="interlanguage-link interwiki-da" style="display: none;"><a href="https://da.wikipedia.org/wiki/Entropi_%28informationsteori%29" title="Entropi (informationsteori) – Danish" hreflang="da" class="interlanguage-link-target" lang="da">Dansk</a></li><li class="interlanguage-link interwiki-de" style=""><a href="https://de.wikipedia.org/wiki/Entropie_%28Informationstheorie%29" title="Entropie (Informationstheorie) – German" hreflang="de" class="interlanguage-link-target" lang="de">Deutsch</a></li><li class="interlanguage-link interwiki-el" style="display: none;"><a href="https://el.wikipedia.org/wiki/%CE%95%CE%BD%CF%84%CF%81%CE%BF%CF%80%CE%AF%CE%B1_%CF%80%CE%BB%CE%B7%CF%81%CE%BF%CF%86%CE%BF%CF%81%CE%B9%CF%8E%CE%BD" title="Εντροπία πληροφοριών – Greek" hreflang="el" class="interlanguage-link-target" lang="el">Ελληνικά</a></li><li class="interlanguage-link interwiki-es" style=""><a href="https://es.wikipedia.org/wiki/Entrop%C3%ADa_%28informaci%C3%B3n%29" title="Entropía (información) – Spanish" hreflang="es" class="interlanguage-link-target" lang="es">Español</a></li><li class="interlanguage-link interwiki-fa" style="display: none;"><a href="https://fa.wikipedia.org/wiki/%D8%A2%D9%86%D8%AA%D8%B1%D9%88%D9%BE%DB%8C_%D8%A7%D8%B7%D9%84%D8%A7%D8%B9%D8%A7%D8%AA" title="آنتروپی اطلاعات – Persian" hreflang="fa" class="interlanguage-link-target" lang="fa">فارسی</a></li><li class="interlanguage-link interwiki-fr" style=""><a href="https://fr.wikipedia.org/wiki/Entropie_de_Shannon" title="Entropie de Shannon – French" hreflang="fr" class="interlanguage-link-target" lang="fr">Français</a></li><li class="interlanguage-link interwiki-gl" style="display: none;"><a href="https://gl.wikipedia.org/wiki/Entrop%C3%ADa_da_informaci%C3%B3n" title="Entropía da información – Galician" hreflang="gl" class="interlanguage-link-target" lang="gl">Galego</a></li><li class="interlanguage-link interwiki-ko" style="display: none;"><a href="https://ko.wikipedia.org/wiki/%EC%A0%95%EB%B3%B4_%EC%97%94%ED%8A%B8%EB%A1%9C%ED%94%BC" title="정보 엔트로피 – Korean" hreflang="ko" class="interlanguage-link-target" lang="ko">한국어</a></li><li class="interlanguage-link interwiki-it" style="display: none;"><a href="https://it.wikipedia.org/wiki/Entropia_%28teoria_dell%27informazione%29" title="Entropia (teoria dell'informazione) – Italian" hreflang="it" class="interlanguage-link-target" lang="it">Italiano</a></li><li class="interlanguage-link interwiki-he" style="display: none;"><a href="https://he.wikipedia.org/wiki/%D7%90%D7%A0%D7%98%D7%A8%D7%95%D7%A4%D7%99%D7%94_%28%D7%A1%D7%98%D7%98%D7%99%D7%A1%D7%98%D7%99%D7%A7%D7%94%29" title="אנטרופיה (סטטיסטיקה) – Hebrew" hreflang="he" class="interlanguage-link-target" lang="he">עברית</a></li><li class="interlanguage-link interwiki-lt" style="display: none;"><a href="https://lt.wikipedia.org/wiki/Entropija_%28informacijos_teorija%29" title="Entropija (informacijos teorija) – Lithuanian" hreflang="lt" class="interlanguage-link-target" lang="lt">Lietuvių</a></li><li class="interlanguage-link interwiki-hu" style="display: none;"><a href="https://hu.wikipedia.org/wiki/Shannon-entr%C3%B3piaf%C3%BCggv%C3%A9ny" title="Shannon-entrópiafüggvény – Hungarian" hreflang="hu" class="interlanguage-link-target" lang="hu">Magyar</a></li><li class="interlanguage-link interwiki-nl" style="display: none;"><a href="https://nl.wikipedia.org/wiki/Entropie_%28informatietheorie%29" title="Entropie (informatietheorie) – Dutch" hreflang="nl" class="interlanguage-link-target" lang="nl">Nederlands</a></li><li class="interlanguage-link interwiki-ja" style=""><a href="https://ja.wikipedia.org/wiki/%E6%83%85%E5%A0%B1%E9%87%8F" title="情報量 – Japanese" hreflang="ja" class="interlanguage-link-target" lang="ja">日本語</a></li><li class="interlanguage-link interwiki-mhr" style="display: none;"><a href="https://mhr.wikipedia.org/wiki/%D0%A8%D0%B5%D0%BD%D0%BD%D0%BE%D0%BD%D1%8B%D0%BD_%D1%84%D0%BE%D1%80%D0%BC%D1%83%D0%BB%D0%B6%D0%BE" title="Шеннонын формулжо – Eastern Mari" hreflang="mhr" class="interlanguage-link-target" lang="mhr">Олык марий</a></li><li class="interlanguage-link interwiki-pa" style="display: none;"><a href="https://pa.wikipedia.org/wiki/%E0%A8%90%E0%A8%A8%E0%A8%9F%E0%A9%8D%E0%A8%B0%E0%A9%8C%E0%A8%AA%E0%A9%80_%28%E0%A8%87%E0%A8%A8%E0%A8%AB%E0%A9%8D%E0%A8%B0%E0%A8%AE%E0%A9%87%E0%A8%B8%E0%A8%BC%E0%A8%A8_%E0%A8%A5%E0%A8%BF%E0%A8%8A%E0%A8%B0%E0%A9%80%29" title="ਐਨਟ੍ਰੌਪੀ (ਇਨਫ੍ਰਮੇਸ਼ਨ ਥਿਊਰੀ) – Punjabi" hreflang="pa" class="interlanguage-link-target" lang="pa">ਪੰਜਾਬੀ</a></li><li class="interlanguage-link interwiki-pl" style="display: none;"><a href="https://pl.wikipedia.org/wiki/Entropia_%28teoria_informacji%29" title="Entropia (teoria informacji) – Polish" hreflang="pl" class="interlanguage-link-target" lang="pl">Polski</a></li><li class="interlanguage-link interwiki-pt" style=""><a href="https://pt.wikipedia.org/wiki/Entropia_da_informa%C3%A7%C3%A3o" title="Entropia da informação – Portuguese" hreflang="pt" class="interlanguage-link-target" lang="pt">Português</a></li><li class="interlanguage-link interwiki-ro" style="display: none;"><a href="https://ro.wikipedia.org/wiki/Entropie_informa%C8%9Bional%C4%83" title="Entropie informațională – Romanian" hreflang="ro" class="interlanguage-link-target" lang="ro">Română</a></li><li class="interlanguage-link interwiki-ru" style=""><a href="https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%8F" title="Информационная энтропия – Russian" hreflang="ru" class="interlanguage-link-target" lang="ru">Русский</a></li><li class="interlanguage-link interwiki-simple" style="display: none;"><a href="https://simple.wikipedia.org/wiki/Information_entropy" title="Information entropy – Simple English" hreflang="simple" class="interlanguage-link-target" lang="simple">Simple English</a></li><li class="interlanguage-link interwiki-sk" style="display: none;"><a href="https://sk.wikipedia.org/wiki/Entropia_%28te%C3%B3ria_inform%C3%A1ci%C3%AD%29" title="Entropia (teória informácií) – Slovak" hreflang="sk" class="interlanguage-link-target" lang="sk">Slovenčina</a></li><li class="interlanguage-link interwiki-sl" style="display: none;"><a href="https://sl.wikipedia.org/wiki/Entropija_%28informatika%29" title="Entropija (informatika) – Slovenian" hreflang="sl" class="interlanguage-link-target" lang="sl">Slovenščina</a></li><li class="interlanguage-link interwiki-ckb" style="display: none;"><a href="https://ckb.wikipedia.org/wiki/%D8%A6%D8%A7%D9%86%D8%AA%D8%B1%DB%86%D9%BE%DB%8C%DB%8C_%D8%B2%D8%A7%D9%86%DB%8C%D8%A7%D8%B1%DB%8C" title="ئانترۆپیی زانیاری – Central Kurdish" hreflang="ckb" class="interlanguage-link-target" lang="ckb">کوردی</a></li><li class="interlanguage-link interwiki-sr" style="display: none;"><a href="https://sr.wikipedia.org/wiki/%D0%95%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D1%98%D0%B0_%28%D1%82%D0%B5%D0%BE%D1%80%D0%B8%D1%98%D0%B0_%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D1%98%D0%B0%29" title="Ентропија (теорија информација) – Serbian" hreflang="sr" class="interlanguage-link-target" lang="sr">Српски / srpski</a></li><li class="interlanguage-link interwiki-su" style="display: none;"><a href="https://su.wikipedia.org/wiki/%C3%89ntropi_informasi" title="Éntropi informasi – Sundanese" hreflang="su" class="interlanguage-link-target" lang="su">Basa Sunda</a></li><li class="interlanguage-link interwiki-sv" style="display: none;"><a href="https://sv.wikipedia.org/wiki/Entropi_%28informationsteori%29" title="Entropi (informationsteori) – Swedish" hreflang="sv" class="interlanguage-link-target" lang="sv">Svenska</a></li><li class="interlanguage-link interwiki-te" style="display: none;"><a href="https://te.wikipedia.org/wiki/%E0%B0%B8%E0%B0%AE%E0%B0%BE%E0%B0%9A%E0%B0%BE%E0%B0%B0_%E0%B0%B8%E0%B0%82%E0%B0%95%E0%B0%B0%E0%B0%A4_%28%E0%B0%87%E0%B0%A8%E0%B1%8D%E0%B0%AB%E0%B0%B0%E0%B1%8D%E0%B0%AE%E0%B1%87%E0%B0%B7%E0%B0%A8%E0%B1%8D_%E0%B0%8E%E0%B0%82%E0%B0%9F%E0%B1%8D%E0%B0%B0%E0%B1%8A%E0%B0%AA%E0%B1%80%29" title="సమాచార సంకరత (ఇన్ఫర్మేషన్ ఎంట్రొపీ) – Telugu" hreflang="te" class="interlanguage-link-target" lang="te">తెలుగు</a></li><li class="interlanguage-link interwiki-th" style="display: none;"><a href="https://th.wikipedia.org/wiki/%E0%B9%80%E0%B8%AD%E0%B8%99%E0%B9%82%E0%B8%97%E0%B8%A3%E0%B8%9B%E0%B8%B5%E0%B8%82%E0%B8%AD%E0%B8%87%E0%B8%82%E0%B9%89%E0%B8%AD%E0%B8%A1%E0%B8%B9%E0%B8%A5" title="เอนโทรปีของข้อมูล – Thai" hreflang="th" class="interlanguage-link-target" lang="th">ไทย</a></li><li class="interlanguage-link interwiki-tr" style="display: none;"><a href="https://tr.wikipedia.org/wiki/Entropi_%28bilgi_teorisi%29" title="Entropi (bilgi teorisi) – Turkish" hreflang="tr" class="interlanguage-link-target" lang="tr">Türkçe</a></li><li class="interlanguage-link interwiki-uk" style="display: none;"><a href="https://uk.wikipedia.org/wiki/%D0%86%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D1%96%D0%B9%D0%BD%D0%B0_%D0%B5%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D1%96%D1%8F" title="Інформаційна ентропія – Ukrainian" hreflang="uk" class="interlanguage-link-target" lang="uk">Українська</a></li><li class="interlanguage-link interwiki-ur" style=""><a href="https://ur.wikipedia.org/wiki/%D8%AF%D8%B1%D9%85%D8%A7%D8%A6%D9%84%D8%AA_%28%D8%A7%D8%B7%D9%84%D8%A7%D8%B9%D8%A7%D8%AA%DB%8C_%D9%86%D8%B8%D8%B1%DB%8C%DB%81%29" title="درمائلت (اطلاعاتی نظریہ) – Urdu" hreflang="ur" class="interlanguage-link-target" lang="ur">اردو</a></li><li class="interlanguage-link interwiki-vi" style="display: none;"><a href="https://vi.wikipedia.org/wiki/Entropy_th%C3%B4ng_tin" title="Entropy thông tin – Vietnamese" hreflang="vi" class="interlanguage-link-target" lang="vi">Tiếng Việt</a></li><li class="interlanguage-link interwiki-zh" style=""><a href="https://zh.wikipedia.org/wiki/%E7%86%B5_%28%E4%BF%A1%E6%81%AF%E8%AE%BA%29" title="熵 (信息论) – Chinese" hreflang="zh" class="interlanguage-link-target" lang="zh">中文</a></li>				<button class="mw-interlanguage-selector mw-ui-button" title="All languages (initial selection from common choices by you and others)">31 more</button></ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q204570#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 27 July 2018, at 16:57<span class="anonymous-show">&nbsp;(UTC)</span>.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="https://creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="https://wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="https://wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="https://www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="https://en.m.wikipedia.org/w/index.php?title=Entropy_%28information_theory%29&amp;oldid=852254681&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							</ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://foundation.wikimedia.org/"><img src="wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" alt="Wikimedia Foundation" width="88" height="31"></a>					</li>
										<li id="footer-poweredbyico">
						<a href="https://www.mediawiki.org/"><img src="poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.696","walltime":"0.966","ppvisitednodes":{"value":6496,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":122379,"limit":2097152},"templateargumentsize":{"value":13556,"limit":2097152},"expansiondepth":{"value":16,"limit":40},"expensivefunctioncount":{"value":14,"limit":500},"unstrip-depth":{"value":0,"limit":20},"unstrip-size":{"value":16124,"limit":5000000},"entityaccesscount":{"value":1,"limit":400},"timingprofile":["100.00%  542.870      1 -total"," 25.36%  137.656      1 Template:Reflist"," 12.70%   68.953      4 Template:Ambox","  9.86%   53.519      6 Template:Isbn","  9.75%   52.927      1 Template:Multiple_issues","  8.56%   46.479      3 Template:Fix","  7.00%   37.974      1 Template:Authority_control","  6.91%   37.524      2 Template:Cite_journal","  6.25%   33.955      4 Template:Delink","  6.17%   33.469    184 Template:Math"]},"scribunto":{"limitreport-timeusage":{"value":"0.206","limit":"10.000"},"limitreport-memusage":{"value":5976529,"limit":52428800}},"cachereport":{"origin":"mw1230","timestamp":"20180727165705","ttl":1900800,"transientcontent":false}}});mw.config.set({"wgBackendResponseTime":134,"wgHostname":"mw1324"});});</script>
	

<div style="display: none; font-size: 13px;" class="suggestions"><div class="suggestions-results"></div><div class="suggestions-special"></div></div></body></html>