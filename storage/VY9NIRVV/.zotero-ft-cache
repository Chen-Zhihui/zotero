arXiv:1805.09298v4 [cs.LG] 16 Jun 2018

Learning towards Minimum Hyperspherical Energy
Weiyang Liu1,*, Rongmei Lin2,*, Zhen Liu1,*, Lixin Liu1, Zhiding Yu3, Bo Dai1, Le Song1 1Georgia Institute of Technology 2Emory University 3NVIDIA
{wyliu,liuzhen1994}@gatech.edu, rongmei.lin@emory.edu, lsong@cc.gatech.edu
Abstract
Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to ﬁt complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics – Thomson problem, where one seeks to ﬁnd a state that distributes N electrons on a unit sphere as even as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our method, by showing the superior performance with MHE regularization.
1 Introduction
The recent success of deep neural networks has led to its wide applications in a variety of tasks. With the over-parametrization nature and deep layered architecture, current deep networks [13, 44, 40] are able to achieve impressive performance on large-scale problems. Despite such success, having redundant and highly correlated neurons (e.g., weights of kernels/ﬁlters in convolutional neural networks (CNNs)) caused by over-parametrization presents an issue [35, 39], which motivated a series of inﬂuential works in network compression [9, 1] and parameter-efﬁcient network architectures [15, 17, 60]. These works either compress the network by pruning redundant neurons or directly modify the network architecture, aiming to achieve comparable performance while using fewer parameters. Yet, it remains an open problem to ﬁnd a uniﬁed and principled theory that guides the network compression in the context of optimal generalization ability.
Another stream of works seek to further release the network generalization power by alleviating redundancy through diversiﬁcation [55, 54, 4, 34] as rigorously analyzed by [57]. Most of these works address the redundancy problem by enforcing relatively large diversity between pairwise projection bases via regularization. Our work broadly falls into this category by sharing similar high-level target, but the spirit and motivation behind our proposed models are distinct. In particular, there is a recent trend of studies that feature the signiﬁcance of angular learning at both loss and convolution levels [27, 26, 28, 25], based on the observation that the angles in deep embeddings learned by CNNs tend to encode semantic difference. The key intuition is that angles preserve the most abundant and discriminative information for visual recognition. As a result, hyperspherical geodesic distances between neurons naturally plays a key role in this context, and thus, it is intuitively desired to impose discriminativeness by keeping their projections on the hypersphere as far away from
* indicates equal contributions.
Technical Report, May 2018

each other as possible. While the concept of imposing large angular diversities was also considered in [57, 55, 54, 34], they do not consider diversity in terms of global equidistribution of embeddings on the hypersphere, which fails to achieve the state-of-the-art performances.

Given the above motivation, we draw inspiration from a well-known physics problem, called Thomson problem [46, 41]. The goal of Thomson problem is to determine the minimum electrostatic potential energy conﬁguration of N mutually-repelling electrons on the surface of a unit sphere. We identify the intrinsic resemblance between the Thomson problem and our target, in the sense that diversiﬁng neurons can be seen as searching for an optimal conﬁguration of electron locations. Similarly, we characterize the diversity for a group of neurons by deﬁning a generic hyperspherical potential energy using their pairwise relationship. Higher energy implies higher redundancy, while lower energy indicates that these neurons are more diverse and more uniformly spaced. To reduce the redundancy of neurons and improve the neural networks, we propose a novel minimum hyperspherical energy (MHE) regularization framework, where the diversity of neurons is promoted by minimizing the hyperspherical energy in each layer. As veriﬁed by comprehensive experiments on multiple tasks, MHE is able to consistently improve the generalization power of neural networks.

MHE faces different situations when it is

applied to hidden layers and output lay-

ers. For hidden layers, applying MHE

straightforwardly may still encourage

some degree of redundancy since it will

produce co-linear bases pointing to op-

posite directions (see Fig. 1 middle). In

order to avoid such redundancy, we propose the half-space MHE which constructs a group of virtual neurons and minimize the hyperspherical energy of both existing and virtual neurons. For output layers, MHE aims to distribute the classiﬁer neuron1 as uniformly as

Orthonormal

MHE

Half-space MHE

Figure 1: Orthonormal, MHE and half-space MHE regularization. The red dots denote the neurons optimized by the gradient of the corresponding regularization. The rightmost pink dots denote the virtual negative neurons. We randomly initialize the weights of 10 neurons on a 3D Sphere and optimize them with SGD.

possible to improve the inter-class feature separability. Different from MHE in hidden layers, classi-

ﬁer neurons should be distributed in the full space for the best classiﬁcation performance [27, 26].

An intuitive comparison among the widely used orthonormal regularization, the proposed MHE and

half-space MHE is provided in Fig. 1. One can observe that both MHE and half-space MHE are able

to uniformly distribute the neurons over the hypersphere and half-space hypershpere, respectively. In

contrast, conventional orthonormal regularization tends to group neurons closer, especially when the

number of neurons is greater than the dimension.

MHE is originally deﬁned on Euclidean distance, as indicated in Thomson problem. However, we further consider minimizing hyperspherical energy deﬁned with respect to angular distance, which we will refer to as angular-MHE (A-MHE) in the following paper. In addition, we give some theoretical insights of MHE regularization, by discussing the asymptotic behavior and generalization error. Last, we apply MHE regularization to multiple vision tasks, including generic object recognition, class-imbalance learning, and face recognition. In the experiments, we show that MHE is architectureagnostic and can considerably improve the generalization ability.

2 Related Works
Diversity regularization is shown useful in sparse coding [30, 33], ensemble learning [24, 22], selfpaced learning [19], metric learning [56] etc. Early studies in sparse coding [30, 33] show that the generalization ability of codebook can be improved via diversity regularization, where the diversity is often modeled using the (empirical) covariance matrix. More recently, a series of studies have featured diversity regularization in neural networks [57, 55, 54, 4, 34, 53], where regularization is mostly achieved via promoting large angle/orthogonality, or reducing covariance between bases. Our work differs from these studies by formulating the diversity of neurons on the entire hypersphere, therefore promoting diversity from a more global, top-down perspective.
Methods other than diversity-promoting regularization have been widely proposed to improve CNNs [42, 18, 31, 28] and generative adversarial nets (GANs) [3, 32]. MHE can be regarded as a complement that can be applied on top of these methods.
1Classiﬁer neurons are the projection bases of the last layer (i.e., output layer) before input to softmax.

2

3 Learning Neurons towards Minimum Hyperspherical Energy

3.1 Formulation of Minimum Hyperspherical Energy

Minimum hyperspherical energy deﬁnes an equilibrium state of the conﬁguration of neuron’s direc-
tions. We argue that the power of neural representation of each layer can be characterized by the
hyperspherical energy of its neurons, and therefore a minimal energy conﬁguration of neurons can
induce better generalization. Before delving into details, we ﬁrst deﬁne the hyperspherical potential energy functional for N neurons with d + 1-dimension WN = {w1, · · · , wN ∈ Rd+1} as

NN

Es,d(wˆi|Ni=1) =

fs wˆi − wˆj =

i=1 j=1,j=i

i=j wˆi − wˆj −s , s > 0

,

i=j log wˆi − wˆj , s = 0

(1)

where

·

denotes Euclidean distance, fs(·) is a decreasing real-valued function, and wˆi =

wi wi

is

the i-th neuron weight projected to the unit hypersphere Sd = {w ∈ Rd+1| w = 1}. We also denote

Wˆ N = {wˆ1, · · · , wˆN ∈ Sd}, and Es = Es,d(wˆi|Ni=1) for short. There are plenty of choices for fs(·), but in this paper we use fs(z) = z−s, s > 0, known as Riesz s-kernels. Particularly, as s → 0, z−s → s log(z−1) + 1, which is an afﬁne transformation of log(z−1). It follows that optimizing the

logarithmic hyperspherical energy E0 = i=j log( wˆi − wˆj ) is essentially the limiting case of optimizing the hyperspherical energy Es. We therefore deﬁne f0(z) = log(z−1) for convenience.

The goal of the MHE criterion is to minimize the energy in Eq. (1) by varying the orientations of the
neuron weights w1, · · · , wN . To be precise, we solve an optimization problem: minWN Es with s ≥ 0. In particular, when s = 0, we solve the logarithmic energy minimization problem:

arg min E0 = arg min exp(E0) = arg min wˆi − wˆj ,

(2)

WN

WN

WN i=j

in which we maximize the product of Euclidean distances. Note that Thomson problem corresponds

to minimizing E1, which is a NP-hard problem. Therefore in practice we can only compute its approximate solution by heuristics. In the case of neural networks, such a differentiable objective can

be directly optimized via gradient descent.

3.2 Logarithmic Hyperspherical Energy as a Relaxation

Optimizing the original energy in Eq. (1) is equivalent to optimize its logarithmic form log Es. To efﬁciently solve this difﬁcult optimization problem, we can instead optimize the lower bound of log Es as a surrogate energy, by applying Jensen’s inequality:

N

N

arg min Elog :=

log fs wˆi − wˆj

(3)

Wˆ N

i=1 j=1,j=i

With fs(z) = z−s, s > 0, we observe that Elog becomes sE0 = −s i=j log( wˆi − wˆj ), which is identical to the logarithmic hyperspherical energy E0 up to a multiplicative factor s. Hence solving the maximization of the product of distances E0 can also be viewed as a relaxation for Es for s > 0.

3.3 MHE as Regularization for Neural Networks

Now that we have introduced the formulation of MHE, we propose MHE regularization for neural networks. In supervised neural network learning, the entire objective function is shown as follows:

1m L=
m j=1

( wiout, xj

ci=1, yj ) +

λh

·

L−1 j=1

1 Nj (Nj

−

1) {Es}j

+ λo

·

1 NL(NL

− 1) Es(wˆiout|ci=1)

(4)

training data ﬁtting

Th: hyperspherical energy for hidden layers

To: hyperspherical energy for output layer

where xi is the feature of the i-th training sample entering the output layer, wiout is the classiﬁer neuron for the i-th class in the output fully-connected layer and wˆiout denotes its normalized version. {Es}i denotes the hyperspherical energy for the neurons in the i-th layer. c is the number of classes,
m is the batch size, L is the number of layers of the neural network, and Ni is the number of neurons in the i-th layer. Es(wˆiout|ci=1) denotes the hyperspherical energy of neurons {wˆ1out, · · · , wˆcout}. The 2 weight decay is omitted here for simplicity, but we will use it by default in practice. An alternative

interpretation of the MHE regularization from decoupled view is given in Appendix C. The different

effect of the MHE regularization in hidden layers and output layers are discussed separately.

MHE for hidden layers. To make neurons in the hidden layers more discriminative and less redundant, we propose to use MHE as a form of regularization. MHE encourages the normalized neurons to

3

be uniformly distributed on a unit hypersphere, which is partially inspired by the observation in [28] that angular difference in neurons preserves semantic (label-related) information. To some extent, MHE maximizes the average angular difference between neurons (speciﬁcally, the hyperspherical energy of neurons in every hidden layer). For instance, in CNNs we minimize the hyperpsherical energy of kernels in convolutional and fully-connected layers except the output layer.
MHE for output layers. For the output layer, we propose to enhance the inter-class feature separability with MHE to learn discriminative and well-separated features. For classiﬁcation tasks, MHE regularization is complementary to the softmax cross-entropy loss in CNNs. The softmax loss focuses more on the intra-class compactness, while MHE encourages the inter-class separability. Therefore, MHE on output layers can induce features with better generalization power.

3.4 MHE in Half Space

Directly applying the MHE formulation may still encouter some

w^ 1

redundancy. An example in Fig. 2, with two neurons in a 2-

dimensional space, illustrates this potential issue. Directly im-

posing the original MHE regularization leads to a solution that

w^ 1 w^ 2
-w^ 2

two neurons are colinear but with opposite directions. To avoid

w^ 2

-w^ 1

such redundancy, we propose the half-space MHE regularization which constructs some virtual neurons and minimizes the hyperspherical energy of both original and virtual neurons together.

Original MHE

Half-space MHE

Figure 2: Half-space MHE.

Speciﬁcally, half-space MHE constructs a colinear virtual neuron with opposite direction for every

existing neuron. Therefore, we end up with minimizing the hyperspherical energy with 2Ni neurons
in the i-th layer (i.e., minimizing Es({wˆk, −wˆk}|2kN=i1)). This half-space variant will encourage the neurons to be less correlated and less redundant, as illustrated in Fig. 5. Note that, Half-space

MHE can only be used in hidden layers, because the colinear neurons does not constitute redundancy

in output layers, as shown in [27]. Still, colinearity is not very likely in high-dimensional spaces,

especially when the neurons are optimized to ﬁt training data. This may be the reason that the original

MHE regularization still improves the baselines.

3.5 MHE beyond Euclidean Distance

The hyperspherical energy is originally deﬁned based on the Euclidean distance on a hypersphere,

which can be viewed as an angular measure. In addition to Euclidean distance, we further consider

the geodesic distance on a unit hypersphere as a distance measure for neurons, which is exactly
the same as the angle between neurons. Speciﬁcally, we consider to use arccos(wˆi wˆj) to replace wˆi − wˆj in hyperspherical energies. Following this idea, we propose angular MHE (A-MHE) as a
simple extension, where the hyperspherical energy is rewritten as:

N
Esa,d(wˆi|Ni=1) =

N
fs arccos(wˆi wˆj ) =

i=1 j=1,j=i

i=j arccos(wˆi wˆj )−s, s > 0 i=j log arccos(wˆi wˆj ) , s = 0

(5)

which can be viewed as redeﬁning MHE based on geodesic distance on hyperspheres (i.e., angle), and can be used as an alternative to the original hyperspherical energy Es in Eq. (4). Note that, A-MHE can also be learned in full-space or half-space, leading to similar variants as original MHE. The key difference between MHE and A-MHE lies in the optimization dynamics, because their gradients w.r.t the neuron weights are quite different. A-MHE is also computationally expensive than MHE.

3.6 Mini-batch Approximation for MHE

With a large number of neurons in one layer, calculating MHE can be computationally expensive as it requires computing the pair-wise distances between neurons. To address this issue, we propose to the mini-batch version of MHE to approximate the MHE (either original or half-space) objective.

Mini-batch approximation for MHE on hidden layers. For hidden layers, mini-batch approximation iteratively takes a random batch of neurons as input and minimizes their hyperspherical energy as an approximation to the MHE. Note that the gradients of the mini-batch objective is an unbiased estimation of the original gradient of MHE.

Data-dependent mini-batch approximation for output layers. For the output layer, the data-

dependent mini-batch approximation iteratively takes the classiﬁer neurons corresponding to the

classes

that

exist

in

mini-batches.

It

minimizes

1 m(N −1)

m i=1

N j=1,j=yi

fs(

wˆyi − wˆj

) in each

iteration, where yi denotes the class label of the i-th sample in each mini-batch, m is the mini-batch

size, and N is the number of neurons (in a particular layer).

4

3.7 Discussions
Connections to scientiﬁc problems. The hyperspherical energy minimization has close relationships with scientiﬁc problems. When s = 1, Eq. (1) reduces to Thomson problem [46, 41] (in physics) where one needs to determine the minimum electrostatic potential energy conﬁguration of N mutuallyrepelling electrons on a unit sphere. When s = ∞, Eq. (1) becomes Tammes problem [45] (in geometry) where the goal is to pack a given number of circles on the surface of a sphere such that the minimum distance between circles is maximized. When s = 0, Eq. (1) becomes Whyte’s problem where the goal is to maximize product of Euclidean distances as shown in Eq. (2). Our work aims to make use of important insights from these problems to improve neural networks.

Comparison to orthogonality/angle-promoting regularization. Promoting orthogonality or large angles between bases has been a popular choice for encouraging diversity. Probably the most related and widely used one is the orthonormal regularization which aims to minimize W W − I F , where W denotes the weights of a group of neurons with each column being one neuron and I is an identity matrix. However, these methods models diversity regularization at a more local level, while MHE regularization seeks to model the problem in a more top-down manner.

Weighted MHE? Since we use the normalized neurons for MHE, a natural question may arise: what if we use the original (i.e., unnormalized) neurons to compute MHE? This is essentially to compute a weighted MHE where the norm of the neuron is taken into consideration. Empirically, if we keep the norm of neuron weights in MHE, then the norm will simply keep increasing instead of changing its angular distance with the other neurons in order to minimize the objective. Therefore, applying weighted MHE to neural networks may reduce the recognition accuracy, further validating the importance of the hyperspherical (i.e., angular) diversity, as has been emphasized in [26, 28, 25].

4 Theoretical Insights

Different from a rigorous theoretical analysis, our goal here is to leverage existing theoretical results from [36, 21, 11, 23, 10, 21, 7] and provide theoretical yet intuitive understandings about MHE.

4.1 Asymptotic Behavior

This subsection shows how the hyperspherical energy behaves asymptotically. Speciﬁcally, as N → ∞, we show that the solution Wˆ N is uniformly distributed on Sd when the hyperspherical energy deﬁned in Eq. (1) achieves its minimum.

Deﬁnition 1 (minimal hyperspherical s-energy). We deﬁne the minimal s-energy for N points on the unit hypersphere Sd = {w ∈ Rd+1| w = 1} as

εs,d(N ) := inf Es,d(wˆi|Ni=1)

(6)

Wˆ N ⊂Sd

where the inﬁmum is taken over all possible Wˆ N on Sd. Any conﬁguration of Wˆ N to attain the

inﬁmum is called an s-extremal conﬁguration. Usually εs,d(N ) = ∞ if N is greater than d and

εs,d(N ) = 0 if N = 0, 1.

We discuss the asymptotic behavior (N → ∞) in three cases: 0 < s < d, s = d, and s > d. We ﬁrst write the energy integral as Is(µ) = Sd×Sd u − v −sdµ(u)dµ(v), which is taken over all probability measure µ supported on Sd. With 0 < s < d, Is(µ) is minimal when µ is the spherical measure σd = Hd(·)|Sd /Hd(Sd) on Sd, where Hd(·) denotes the d-dimensional Hausdorff measure. When s ≥ d, Is(µ) becomes inﬁnity, which therefore requires different analysis. In general, we can say all
s-extremal conﬁgurations asymptotically converge to uniformly distribution on a hypersphere, as
stated in Theorem 1. This asymptotic behavior has been heavily studied in [36, 21, 11].

Theorem 1 (asymptotic uniform distribution on hypersphere). Any sequence of optimal s-energy

conﬁgurations (Wˆ N )|∞ 2 ⊂ Sd is asymptotically uniformly distributed on Sd in the sense of the weakstar topology of measures, namely

1 N

δv → σd, as N → ∞

(7)

v∈Wˆ N

where δv denotes the unit point mass at v, and σd is the spherical measure on Sd.

Theorem 2 (asymptotics of the minimal hyperspherical s-energy).

We have that limN→∞

εs,d(N ) p(N )

exists for the minimal s-energy. For 0 < s < d, p(N ) = N 2. For s = d, p(N ) = N 2 log N . For s > d,

p(N ) = N 1+s/d.

Particularly

if

0 < s < d,

we

have

limN →∞

εs,d(N ) N2

= Is(σd).

5

Theorem 2 tells us the growth power of the minimal hyperspherical s-energy when N goes to inﬁnity. Therefore, different potential power s leads to different optimization dynamics. In the light of the behavior of the energy integral, MHE regularization will focus more on local inﬂuence from neighborhood neurons instead of global inﬂuences from all the neurons as the power s increases.
4.2 Generalization and Optimality
As rigorously proved in [52], in one-hidden-layer ReLU neural network, the diversity of units can effectively eliminate the spurious local minima despite the non-convexity in learning dynamics of neural networks. Following such argument, our MHE regularization, which encourages the diversity of neurons, naturally matches the theorectical intuition in [52], and can effectively promote the generalization of the corresponding neural networks. This is also well veriﬁed by our comprehensive experiments in Section 5. While hyperspherical energy is minimized such that neurons become increasingly more uniformly distributed on a hypersphere, then such hyperspherical diversity is closely related to the generalization error, as shown in main results of [52].

5 Applications and Experiments
5.1 Improving Network Generalization

We use MHE to improve the generalization of CNNs and study how MHE performs in a variety of scenarios. First, we perform ablation study and some exploratory experiments on MHE. Then we apply MHE to large-scale object recognition and class-imbalance learning. For all the experiments on CIFAR-10 and CIFAR-100 in the paper, we use moderate data augmentation, following [13, 25]. For ImageNet-2012, we follow the same data augmentation in [28]. We train all the networks using SGD with momentum 0.9, and the network initialization follows [12]. All the networks use BN [18] and ReLU if not otherwise speciﬁed. Experimental details are given in each subsection and Appendix A.

5.1.1 Ablation Study and Exploratory Experiments

Variants of MHE. We ﬁrst evaluate all different variants of MHE on CIFAR-10

Method

CIFAR-10 s=2 s=1 s=0

CIFAR-100 s=2 s=1 s=0

and CIFAR-100. The compared variants include the original MHE (with the power

MHE Half-space MHE
A-MHE

6.22

6.74

6.44 27.15 27.09 26.16

6.28

6.54

6.30 25.61 26.30 26.18

6.21

6.77

6.45 26.17 27.31 27.90

s = 0, 1, 2) and half-space MHE (with the

Half-space A-MHE 6.52 Baseline

6.49 7.75

6.44 26.03 26.52 26.47 28.13

power s = 0, 1, 2) in both Euclidean and

angular distance. In this experiment, all Table 1: Testing error (%) of different MHE on CIFAR-10/100.

the methods use the CNN-9 (see Appendix A). The results in Table 1 show that all the variants of

MHE performs consistently better than the baseline. Speciﬁcally, the half-space MHE has signiﬁcant

performance gain compared to the other MHE variants, and MHE in Euclidean and angular distance

perform similarly. MHE with s = 2 performs best among s = 0, 1, 2. In the following experiments,

we use s = 2 and Euclidean distance in both MHE and half-space MHE if not otherwise speciﬁed.

Network width. We evaluate MHE with dif-

Method

16/32/64 32/64/128 64/128/256 128/256/512

ferent network width. We use CNN-9 as our

Baseline

47.72

38.64

28.13

24.95

base network, and change its ﬁlter number

MHE Half-space MHE

36.84 35.16

30.05 29.33

26.75 25.96

24.05 23.38

in Conv1.x, Conv2.x and Conv3.x (see Ap- Table 2: Testing error (%) of different width on CIFAR-100.

pendix A). We experiment different network

width including 16/32/64, 32/64/128, 64/128/256 and 128/256/512. Results is given in Table 2. One

can observe that both MHE and half-space MHE consistently outperforms the baseline, showing the

stronger generalization. Interestingly, both MHE and half-space MHE have more signiﬁcant gain

while the ﬁlter number is smaller in each layer, indicating that our MHE regularization can help the

network to make better use of the neurons. Moreover, the half-space MHE is consistently better than

MHE, showing the necessity of reducing colinearity redundancy.

Network depth. We perform experiments with different net-

Method

CNN-6 CNN-9 CNN-15

work depth to better evaluate the performance of MHE. We

Baseline

32.08

28.13

N/C

ﬁx the ﬁlter number in Conv1.x, Conv2.x and Conv3.x to 64,

MHE Half-space MHE

28.16 27.56

26.75 25.96

26.9 25.84

128 and 256, respectively. We compare 6-layer CNN, 9-layer Table 3: Testing error (%) of different

CNN and 15-layer CNN. The results are given in Table 3. depth on CIFAR-100. N/C: not converged.

Both MHE and half-space MHE performs signiﬁcantly better

than the baseline. More interestingly, baseline CNN-15 can not converge, while CNN-15 is able

to converge reasonably well if we use MHE to regularize the network. Moreover, we also see that

half-space MHE can consistently show better generalization than MHE with different network depth.

6

Ablation study. Since the current MHE regularizes the neurons in the hidden layers and the output layer simultaneously, we perform ablation study for MHE to further investigate where the gain comes from. This experiment uses the CNN-9. The results are given in Table 4. “H” means that we apply MHE

Method
MHE Half-space MHE
A-MHE Half-space A-MHE
Baseline

H √O ×
26.85 N/A 27.8 N/A

√H O ×
26.55 26.28 26.56 26.64 28.13

√H √O
26.16 25.61 26.17 26.03

to all the hidden layers, while “O” means that we apply MHE Table 4: Ablation study on CIFAR-100. to the output layer. Because the half-space MHE can not be

applied to the output layer, so there is “N/A” in the table. In general, we ﬁnd that applying MHE

to both the hidden layers and the output layer yields the best performance, and using MHE in the

hidden layers usually produces better accuracy than using MHE in the output layer.

Hyperparameter experiment. We evaluate how the selection of hyperparameter affects the performance. We experiment with different hyperparameters from 10−2 to 102 on CIFAR-100 with the CNN-9. HS-MHE denotes the half-space MHE. We evaluate MHE variants by separately applying MHE to the output layer (“O”), MHE to the hidden layers (“H”), and the half-space MHE to the hidden layers (“H”). The results in Fig. 3 show that the our MHE is not very hyperparametersensitive and can consistently outperform the baseline. The half-space MHE can consistently outperform the original MHE under different hyperparameter settings. Moreover, applying MHE only to hidden layers can yields better accuracy than applying MHE only to output layer.

Testing Error on CIFAR-100 (%)

28

27.5

27

26.5

26

Baseline

25.5

MHE (O) MHE (H)

HS-MHE (H)

25 10-2

10-1

100

101

102

Value of Hyperparameter

Figure 3: Hyperparameter.

MHE for ResNets. Besides the standard CNN, we also

Method

CIFAR-10 CIFAR-100

evaluate MHE on ResNet-32 to show that our MHE is architecture-agnostic and can improve accuracy on multiple types of architectures. Detailed architecture settings are given in Appendix A. The results on CIFAR-10 and CIFAR-100 are given in Table 5. One can observe that applying MHE to ResNet also achieves considerable im-

ResNet-110-original [13]

6.61

ResNet-1001 [14]

4.92

ResNet-1001 (64 batch) [14]

4.64

baseline

5.19

MHE

4.72

Half-space MHE

4.66

25.16 22.71
-
22.87 22.19 22.04

Table 5: Error (%) of ResNet-32.

provement. Most importantly, adding MHE regularization will not affect the original settings of the

architecture, and it can improve the network generalization at a very small computational cost.

5.1.2 Large-scale Object Recognition

We evaluate MHE on large-scale ImageNet-2012 datasets. Specif-

Method

ResNet-18 ResNet-34

ically, we perform experiment using ResNets, and then report

baseline

33.95

30.04

the top-1 validation error (center crop) in Table 6. From the re-

Orthnormal MHE

33.61 33.50

29.75 29.60

sults, we still observe that both MHE and half-space MHE yields Half-space MHE 33.45

29.50

consistently better recognition accuracy than the baseline and the Table 6: Top1 error (%) on ImageNet.

orthonormal regularization (after tuning its hyperparameter). To

better evaluate the consistency of MHE’s performance gain, we use two ResNets with different depth:

ResNet-18 and ResNet-34. On ResNet-18 and ResNet-34, both MHE and half-space MHE show

better generalization power, and half-space MHE performs slightly better than full-space MHE.

5.1.3 Class-imbalance Learning

Because MHE aims to maximize the hyperspherical mar-

gin between different classiﬁer neurons in the output

layer, we can naturally apply MHE to class-imbalance

learning where the number of training samples in differ-

ent classes is imbalanced. We demonstrate the power of

MHE in class-imbalance learning through a toy exper-

iment. We ﬁrst randomly throw away 98% training data

(a) CNN without MHE

(b) CNN with MHE

Figure 4: Class-imbalance learning on MNIST.

for digit 0 in MNIST (only 100 samples are preserved

for digit 0), and then train a 6-layer CNN on this imbalance MNIST dataset. To visualize the learned

features, we set the output feature dimension as 2. The learned features and classiﬁer neurons on

training set is shown in Fig. 4 where each color denotes a digit and red arrows denote the classiﬁer

neurons. From Fig. 4, one can observe that the CNN without MHE only tends to ignore the imbal-

anced class (digit 0) and the learned classiﬁer neuron is highly biased to another digit. In contrast,

the CNN with MHE can learn reasonably separable distribution even if digit 0 only has 2% samples

compared to the other classes. Using MHE can improve the accuracy on the full testing set from

88.5% to 98%. Most importantly, the classiﬁer neuron for digit 0 is also properly learned.

7

We experiment MHE in two data imbalance settings on

Method

Single Err. (S) Multiple

CIFAR-10: 1) single class imbalance (S) - All classes have

Baseline

9.80

30.40

12.00

the same number of images but one single class has signif-

Orthonormal MHE

8.34

26.80

7.98

25.80

10.80 10.25

icantly less number, and 2) multiple class imbalance (M) -

Half-space MHE

7.90

26.40

9.59

The number of images decreases as the class index decreases

A-MHE

7.96

Half-space A-MHE 7.59

26.00 25.90

9.88 9.89

from 9 to 0. We use CNN-9 for all the compared regular- Table 7: Error on imbalanced CIFAR-10. izations. Details are provided in Appendix A. In Table 7,

we report the error rate on the whole testing set. In addition, we report the error rate (denoted by

Err. (S)) on the imbalance class (single imbalance setting) in the full testing set. From the results,

one can observe that CNN-9 with MHE is able to effectively perform recognition when classes are

imbalanced. Even only given a small portion of training data in a few classes, CNN-9 with MHE can

achieve competitive accuracy on the full testing set, showing MHE’s superior generalization power.

5.2 SphereFace+: Improving Inter-class Feature Separability via MHE for Face Recognition

We have shown that full-space MHE for output layers can encourage classiﬁer neurons to distribute

more evenly on hypersphere and improve inter-class feature separability. Intuitively, the classiﬁer

neurons serve as the approximate center for features from each class, and can therefore guide the

feature learning. We also observe that open-set face recognition (e.g., face veriﬁcation) requires the

feature centers to be as separable as possible [26]. This connection inspires us to apply MHE to

face recognition. Speciﬁcally, we propose SphereFace+ by applying MHE to SphereFace [26]. The

objective of SphereFace, angular softmax loss ( SF) that encourages intra-class feature compactness, is naturally complimentary to that of MHE. The objective function of SphereFace+ is deﬁned as

LSF+ =

1m m

SF( wiout, xj ci=1, yj , mSF)

j=1

1

m

+ λM · m(N − 1)

N
fs( wˆyouit − wˆjout )

i=1 j=1,j=yi

(8)

Angular softmax loss: promoting intra-class compactness

MHE: promoting inter-class separability

where c is the number of classes, m is the mini-batch size, N is the number of classiﬁer neurons, xi the deep feature of the i-th face (yi is its groundtruth label), and wiout is the i-th classiﬁer neuron. mSF is a hyperparameter for SphereFace, controlling the degree of intra-class feature compactness. Because the face dateset usually has thousands of identities, we will use the data-dependent mini-
batch approximation MHE in the output layer, as shown in Eq. (8). MHE completes a missing piece
for SphereFace by promoting the inter-class separability. Our SphereFace+ consistently outperforms
SphereFace, and achieves state-of-the-art performance on LFW [16] and MegaFace [20] datasets.

mSF
1 2 3 4

LFW SphereFace SphereFace+

96.35 98.87 98.97 99.26

97.15 99.05 99.13 99.32

MegaFace SphereFace SphereFace+

39.12 60.48 63.71 70.68

45.90 68.51 66.89 71.30

mSF
1 2 3 4

LFW SphereFace SphereFace+

96.93 99.03 99.25 99.42

97.47 99.22 99.35 99.47

MegaFace SphereFace SphereFace+

41.07 62.01 69.69 72.72

45.55 67.07 70.89 73.03

Table 8: Accuracy (%) on SphereFace-20 network. Table 9: Accuracy (%) on SphereFace-64 network.

Performance under different mSF. We evaluate SphereFace+ with two different architectures (SphereFace-20 and SphereFace-64) proposed in [26]. Speciﬁcally, SphereFace-20 and SphereFace64 are 20-layer and 64-layer modiﬁed residual networks, respectively. We train our network with the publicly available CASIA-Webface dataset [58], and then test the learned model on LFW and MegaFace dataset. In MegaFace dataset, the reported accuracy indicates rank-1 identiﬁcation accuracy with 1 million distractors. All the results in Table 8 and Table 9 are computed without model ensemble and PCA. One can observe that SphereFace+ consistently outperforms SphereFace by a considerable margin on both LFW and MegaFace datasets under all different settings of mSF. Moreover, the performance gain generalizes across different architectures.

Comparison to state-of-the-art methods. We also compare

Method

LFW MegaFace

our methods with some widely used loss functions. All these

Softmax Loss

97.88

54.86

compared methods use SphereFace-64 network that are trained

Softmax+Contrastive [43] Triplet Loss [38]

98.78 98.70

65.22 64.80

with CASIA dataset. All the results are given in Table 10

L-Softmax Loss [27]

99.10

67.13

computed without model ensemble and PCA. Compared to the

Softmax+Center Loss [51] CosineFace [49, 47]

99.05 99.10

65.49 75.10

other state-of-the-art methods, SphereFace+ achieves the best

SphereFace

99.42

72.72

accuracy on LFW dataset, while being comparable to the best

SphereFace+ (ours)

99.47

73.03

accuracy on MegaFace dataset. Current state-of-the-art face Table 10: Comparison to state-of-the-art.

recognition methods [47, 26, 49, 5, 29] usually only focus on compressing the intra-class features,

which makes MHE a potentially useful tool in order to further improve these face recognition methods.

8

6 Concluding Remarks
We borrow the idea from physics and propose a novel regularization method, minimum hyperspherical energy (MHE), to encourage the diversity of neural representation. Neural nets with MHE show consistent improvements on performance in our experiments. Finally, MHE provides a new point of view on the role of neuron weights and may have many additional potential applications in model compression, image generation, etc.
Acknowledgements
We would like to thank NVIDIA corporation for donating Titan Xp GPUs to support our research. We also thank Tuo Zhao (Georgia Tech) for the valuable discussions and suggestions.
References
[1] Alireza Aghasi, Nam Nguyen, and Justin Romberg. Net-trim: A layer-wise convex pruning of deep neural networks. In AISTATS, 2017.
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.
[3] Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. In ICLR, 2017.
[4] Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overﬁtting in deep networks by decorrelating representations. In ICLR, 2016.
[5] Jiankang Deng, Jia Guo, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. arXiv preprint arXiv:1801.07698, 2018.
[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
[7] Mario Götz and Edward B Saff. Note on d—extremal conﬁgurations for the sphere in r d+1. In Recent Progress in Multivariate Approximation, pages 159–162. Springer, 2001.
[8] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In NIPS, 2017.
[9] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In ICLR, 2016.
[10] DP Hardin and EB Saff. Minimal riesz energy point conﬁgurations for rectiﬁable d-dimensional manifolds. arXiv preprint math-ph/0311024, 2003.
[11] DP Hardin and EB Saff. Discretizing manifolds via minimum energy points. Notices of the AMS, 51(10):1186–1194, 2004.
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing human-level performance on imagenet classiﬁcation. In ICCV, 2015.
[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.
[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In ECCV, 2016.
[15] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
[16] Gary B Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. Technical report, Technical Report, 2007.
9

[17] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016.
[18] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.
[19] Lu Jiang, Deyu Meng, Shoou-I Yu, Zhenzhong Lan, Shiguang Shan, and Alexander Hauptmann. Self-paced learning with diversity. In NIPS, 2014.
[20] Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard. The megaface benchmark: 1 million faces for recognition at scale. In CVPR, 2016.
[21] Arno Kuijlaars and E Saff. Asymptotics for minimal discrete energy on the sphere. Transactions of the American Mathematical Society, 350(2):523–538, 1998.
[22] Ludmila I Kuncheva and Christopher J Whitaker. Measures of diversity in classiﬁer ensembles and their relationship with the ensemble accuracy. Machine learning, 51(2):181–207, 2003.
[23] Naum Samouilovich Landkof. Foundations of modern potential theory, volume 180. Springer, 1972.
[24] Nan Li, Yang Yu, and Zhi-Hua Zhou. Diversity regularized ensemble pruning. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, 2012.
[25] Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James M Rehg, and Le Song. Decoupled networks. CVPR, 2018.
[26] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In CVPR, 2017.
[27] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolutional neural networks. In ICML, 2016.
[28] Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep hyperspherical learning. In NIPS, 2017.
[29] Yu Liu, Hongyang Li, and Xiaogang Wang. Rethinking feature discrimination and polymerization for large-scale recognition. arXiv preprint arXiv:1710.00870, 2017.
[30] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online dictionary learning for sparse coding. In ICML, 2009.
[31] Dmytro Mishkin and Jiri Matas. All you need is a good init. In ICLR, 2016.
[32] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative adversarial networks. In ICLR, 2018.
[33] Ignacio Ramirez, Pablo Sprechmann, and Guillermo Sapiro. Classiﬁcation and clustering via dictionary learning with structured incoherence and shared features. In CVPR, 2010.
[34] Pau Rodríguez, Jordi Gonzalez, Guillem Cucurull, Josep M Gonfaus, and Xavier Roca. Regularizing cnns with locally constrained decorrelations. In ICLR, 2017.
[35] Aruni RoyChowdhury, Prakhar Sharma, Erik Learned-Miller, and Aruni Roy. Reducing duplicate ﬁlters in deep neural networks. In NIPS workshop on Deep Learning: Bridging Theory and Practice, 2017.
[36] Edward B Saff and Amo BJ Kuijlaars. Distributing many points on a sphere. The mathematical intelligencer, 19(1):5–11, 1997.
[37] Tim Salimans and Diederik P Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In NIPS, 2016.
[38] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embedding for face recognition and clustering. In CVPR, 2015.
[39] Wenling Shang, Kihyuk Sohn, Diogo Almeida, and Honglak Lee. Understanding and improving convolutional neural networks via concatenated rectiﬁed linear units. In ICML, 2016.
[40] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556, 2014.
10

[41] Steve Smale. Mathematical problems for the next century. The mathematical intelligencer, 20(2):7–15, 1998.
[42] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overﬁtting. JMLR, 15(1):1929–1958, 2014.
[43] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation from predicting 10,000 classes. In CVPR, 2014.
[44] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.
[45] Pieter Merkus Lambertus Tammes. On the origin of number and arrangement of the places of exit on the surface of pollen-grains. Recueil des travaux botaniques néerlandais, 27(1):1–84, 1930.
[46] Joseph John Thomson. Xxiv. on the structure of the atom: an investigation of the stability and periods of oscillation of a number of corpuscles arranged at equal intervals around the circumference of a circle; with application of the results to the theory of atomic structure. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 7(39):237–265, 1904.
[47] Feng Wang, Weiyang Liu, Haijun Liu, and Jian Cheng. Additive margin softmax for face veriﬁcation. arXiv preprint arXiv:1801.05599, 2018.
[48] Feng Wang, Xiang Xiang, Jian Cheng, and Alan L Yuille. Normface: L2 hypersphere embedding for face veriﬁcation. arXiv preprint arXiv:1704.06369, 2017.
[49] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Zhifeng Li, Dihong Gong, Jingchao Zhou, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. arXiv preprint arXiv:1801.09414, 2018.
[50] David Warde-Farley and Yoshua Bengio. Improving generative adversarial networks with denoising feature matching. In ICLR, 2017.
[51] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach for deep face recognition. In ECCV, 2016.
[52] Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. arXiv preprint arXiv:1611.03131, 2016.
[53] Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution for training extremely deep convolutional neural networks with orthonormality and modulation. arXiv:1703.01827, 2017.
[54] Pengtao Xie, Yuntian Deng, Yi Zhou, Abhimanu Kumar, Yaoliang Yu, James Zou, and Eric P Xing. Learning latent space models with angular constraints. In ICML, 2017.
[55] Pengtao Xie, Aarti Singh, and Eric P Xing. Uncorrelation and evenness: a new diversity-promoting regularizer. In ICML, 2017.
[56] Pengtao Xie, Wei Wu, Yichen Zhu, and Eric P Xing. Orthogonality-promoting distance metric learning: convex relaxation and theoretical analysis. In ICML, 2018.
[57] Pengtao Xie, Jun Zhu, and Eric Xing. Diversity-promoting bayesian learning of latent variable models. In ICML, 2016.
[58] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv:1411.7923, 2014.
[59] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint face detection and alignment using multitask cascaded convolutional networks. IEEE Signal Processing Letters, 23(10):1499–1503, 2016.
[60] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient convolutional neural network for mobile devices. arXiv preprint arXiv:1707.01083, 2017.
11

A Experimental Details

Layer
Conv1.x Pool1
Conv2.x Pool2
Conv3.x Pool3
Fully Connected

CNN-6

CNN-9

CNN-15

[3×3, 64]×2 [3×3, 64]×3 [3×3, 64]×5

2×2 Max Pooling, Stride 2

[3×3, 128]×2 [3×3, 128]×3 [3×3, 128]×5

2×2 Max Pooling, Stride 2

[3×3, 256]×2 [3×3, 256]×3 [3×3, 256]×5

2×2 Max Pooling, Stride 2

256

256

256

Table 11: Our plain CNN architectures with different convolutional layers. Conv1.x, Conv2.x and Conv3.x denote convolution units that may contain multiple convolution layers. E.g., [3×3, 64]×3 denotes 3 cascaded convolution layers with 64 ﬁlters of size 3×3.

Layer Conv0.x Conv1.x
Conv2.x Conv3.x Conv4.x

ResNet-32 for CIFAR-10/100
N/A [3×3, 64]×1 3 × 3, 64
×5 3 × 3, 64 3 × 3, 128
×5 3 × 3, 128 3 × 3, 256
×5 3 × 3, 256
N/A

ResNet-18 for ImageNet-2012 [7×7, 64], Stride 2
3×3, Max Pooling, Stride 2
3 × 3, 64 ×2
3 × 3, 64
3 × 3, 128 ×2
3 × 3, 128 3 × 3, 256
×2 3 × 3, 256 3 × 3, 512
×2 3 × 3, 512 Average Pooling

ResNet-34 for ImageNet-2012 [7×7, 64], Stride 2
3×3, Max Pooling, Stride 2
3 × 3, 64 ×3
3 × 3, 64
3 × 3, 128 ×4
3 × 3, 128 3 × 3, 256
×6 3 × 3, 256 3 × 3, 512
×3 3 × 3, 512

Table 12: Our ResNet architectures with different convolutional layers. Conv0.x, Conv1.x, Conv2.x, Conv3.x and Conv4.x denote convolution units that may contain multiple convolutional layers, and residual units are shown in double-column brackets. Conv1.x, Conv2.x and Conv3.x usually operate on different size feature maps. These networks are essentially the same as [13], but some may have different number of ﬁlters in each layer. The downsampling is performed by convolutions with a stride of 2. E.g., [3×3, 64]×4 denotes 4 cascaded convolution layers with 64 ﬁlters of size 3×3, and S2 denotes stride 2.

General settings. The network architectures used in the paper are elaborated in Table 11 Table 12. For CIFAR-10 and CIFAR-100, we use batch size 128. We start with learning rate 0.1, divide it by 10 at 20k, 30k and 37.5k iterations, and terminate training at 42.5k iterations. For ImageNet-2012, we use batch size 64 and start with learning rate 0.1. The learning rate is divided by 10 at 150k, 300k and 400k iterations, and the training is terminated at 500k iterations. Note that, for all the compared methods, we always use the best possible hyperparameters to make sure that the comparison is fair. The baseline has exactly the same architecture and training settings as the one that MHE uses, and the only difference is an additional MHE regularization. For full-space MHE in hidden layers, we set λh as 10 for all experiments. For half-space MHE in hidden layers, we set λh as 1 for all experiments. For MHE in output layers, we set λo as 1 for all experiments. We use 1e − 5 for the orthonormal regularization. If not otherwise speciﬁed, standard 2 weight decay (1e − 4) is applied to all the neural network including baselines and the networks that use MHE regularization. A very minor issue for the hyperparameters λh is that it may increase as the number of layers increases, so we can potentially further divide the hyperspherical energy for the hidden layers by the number of layers. It will probably change the current optimal hyperparameter setting by a constant multiplier. For notation simplicity, we do not explicitly write out the weight decay term in the loss function in the main paper. Note that, all the neuron weights in the neural network are not normalized, and the MHE will normalize the neuron weights while computing the regularization loss. As a result, MHE does not need to modify any component of the original neural networks, and it can simply be viewed as an extra regularization loss that can boost the performance.
Class-imbalance learning. There are 50000 training images in the original CIFAR-10 dataset, with 5000 images per class. For the single class imbalance setting, we keep original images of class 1-9 and randomly throw away 90% images of class 0. The total number of training images in this setting is 45500. For the multiple class imbalance setting, we set the number of each class equals to 500 × (class_index + 1). For instance, class 0 has 500 images, class 1 has 1000 images and class 9 has 5000 images. The total number of training images in this setting is 27500.

12

SphereFace+. SphereFace+ uses the same face detection and alignment method [59] as SphereFace [26]. The testing protocol on LFW and MegaFace is also the same as SphereFace. We use exactly the same preprocessing as in the SphereFace repository. Detailed network architecture settings of SphereFace-20 and SphereFace-64 can be found in [26]. Speciﬁcally, we use full-space MHE with Euclidean distance and s = 2 in the output layer. Essentially, we treat MHE as an additional loss function which aims to enlarge the inter-class angular distance of features and serves a complimentary role to the angular softmax in SphereFace. Note that, for the results of CosineFace [49], we directly use the results (with the same training settings and without using feature normalization) reported in the paper. Since ours also does not perform feature normalization, it is a fair comparison. With feature normalization, we ﬁnd that the performance of SphereFace+ will also be improved signiﬁcantly. However, feature normalization makes the results more tricky, because it will involve another hyperparameter that controls the projection radius of feature normalization.
13

B Proof of Theorem 1 and Theorem 2

Theorem 1 and Theorem 2 are natural results from classic potential theory [23] and spherical conﬁguration [10, 21, 7]. We discuss the asymptotic behavior (N → ∞) in three cases: 0 < s < d, s = d, and s > d. We ﬁrst write the energy integral as

Is(µ) =

u − v −sdµ(u)dµ(v),

(9)

Sd ×Sd

which is taken over all probability measure µ supported on Sd. With 0 < s < d, Is(µ) is minimal when
µ is the spherical measure σd = Hd(·)|Sd /Hd(Sd) on Sd, where Hd(·) denotes the d-dimensional Hausdorff measure. When s ≥ d, Is(µ) becomes inﬁnity, which therefore requires different analysis.

First, the classic potential theory [23] can directly give the following results for the case where 0 < s < d:

Lemma 1. If 0 < s < d,

lim
N →∞

εs,d(N ) N2

=

Is

(

Hd(·)|Sd Hd(Sd)

),

(10)

where Is is deﬁned in the main paper. Moreover, any sequence of optimal hyperspherical s-enerygy

conﬁgurations (Wˆ N )|∞ 2 ⊂ Sd is asymptotically uniformly distributed in the sense that for the weak-

star topology measures,

1 N

δv → σd, as N → ∞

(11)

v∈Wˆ N

where δv denotes the unit point mass at v, and σd is the spherical measure on Sd.

which directly concludes Theorem 1 and Theorem 2 in the case of 0 < s < d.

For the case where s = d, we have from [21, 7] the following results: Lemma 2. Let Bd := B¯(0, 1) be the closed unit ball in Rd. For s = d,

lim
N →∞

εs,d(N ) N 2 log N

=

Hd(Bd) Hd(Sd)

=

1 d

√Γπ( dΓ+2(1d2))

,

(12)

and any sequence (Wˆ N )|∞ 2 ⊂ Sd of optimal s-energy conﬁgurations satisﬁes Eq. 11.

which concludes the case of s = d. Therefore, we are left with the case where s > d. For this case, we can use the results from [10]:
Lemma 3. Let A ⊂ Rd be compact with Hd(A) > 0, and W˜ N = {xk,N }Ni=1 be a sequence of asymptotically optimal N -point conﬁgurations in A in the sense that for some s > d,

lim
N →∞

Es(W˜ N ) N 1+s/d

=

Cs,d Hd(A)s/d

(13)

or

lim
N →∞

Es(W˜ N ) N 2 log N

=

Hd(Bd) Hd(A) .

(14)

where Cs,d is a ﬁnite positive constant independent of A. Let δx be the unit point mass at the point x. Then in the weak-star topology of measures we have

1 N

N

δxi,N

→

Hd(·)|A Hd(A)

,

asN → ∞.

(15)

i=1

The results naturally prove the case of s > d. Combining these three lemmas, we have proved Theorem 1 and Theorem 2.

14

C Understanding MHE from Decoupled View

Inspired by decoupled networks [25], we can view the original convolution as the multiplication of the angular function g(·) and the magnitude function h(·):

f (w, x) = w x · cos(θ)

(16)

where θ is the angle between the kernel w and the input x. From the equation above, we can see that the norm of the kernel and the direction (i.e., angle) of the kernel affect the inner product similarity differently. Typically, weight decay is to regularize the kernel by minimizing its 2 norm, while there is no regularization on the direction of the kernel. Therefore, MHE is able to complete this missing piece by promoting angular diversity. By combining MHE to a standard neural networks (e.g., CNNs), the regularization term becomes

Lreg =

λw ·

1

L Nj

L j=1

Nj

j=1

i=1

wi

+ λh

·

L−1 j=1

1 Nj (Nj

−

1) {Es}j

+

λo

·

1 NL(NL

−

1) Es(wˆiout|ci=1)

Weight decay: regularizing the magnitude of kernels

MHE: regularizing the direction of kernels

(17)

where xi is the feature of the i-th training sample entering the output layer, wiout is the classiﬁer neuron for the i-th class in the output fully-connected layer and wˆiout denotes its normalized version.

{Es}i denotes the hyperspherical energy for the neurons in the i-th layer. c is the number of classes,

m is the batch size, L is the number of layers of the neural network, and Ni is the number of neurons in the i-th layer. Es(wˆiout|ci=1) denotes the hyperspherical energy of neurons {wˆ1out, · · · , wˆcout} in the output layer. λw, λh and λo are weighting hyperparameters for these three regularization terms.

From the decoupled view, we can see that MHE is actually very meaningful in regularizing the neural networks, which also serves a complementary role to weight decay.

15

D Regularizing SphereNets with MHE

SphereNets [28] are a family of network networks that learns on hyperspheres. The ﬁlters in

SphereNets only focus on the hyperspherical (i.e., angular) difference. One can see that the intuition

of SphereNets well matches that of MHE, so MHE can serve as a natural and effective regularization

for SphereNets. Because SphereNets throw away all the magnitude information of ﬁlters, the weight

decay can no longer serve as a form of regularization for SphereNets, which makes MHE a very useful

regularization for SphereNets. Originally, we use the orthonormal regularization

W

W −I

2 F

to

regularize SphereNets, where W is the weight matrix of a layer with each column being a vectorized

ﬁlter and I is an identity matrix. We compare MHE, half-space MHE and orthonormal regularization

for SphereNets. In this section, all the SphereNets use the same architecture as the CNN-9 in Table 11,

the training setting is also the same as CNN-9. We only evaluate SphereNets with cosine SphereConv.

Note that, s = 0 is actually the logarithmic hyperspherical energy (a relaxation of the original

hyperspherical energy). From Table 13, we observe that SphereNets with MHE can outperform both

the SphereNet baseline and SphereNets with the orthonormal regularization, showing that MHE is

not only effective in standard CNNs but also very suitable for SphereNets.

Method
MHE Half-space MHE
A-MHE Half-space A-MHE SphereNet with Orthonormal Reg. SphereNet Baseline

CIFAR-10 s=2 s=1 s=0
5.71 5.99 5.95 6.12 6.33 6.31 5.91 5.98 6.06 6.14 5.87 6.11
6.13 6.37

CIFAR-100 s=2 s=1 s=0

27.28 27.17 27.07 27.35

26.99 27.77 27.27 27.68 27.95 28.10

27.03 27.46 26.70 27.58

Table 13: Testing error (%) of SphereNet with different MHE on CIFAR-10/100.

16

E Improving AM-Softmax with MHE

We also perform some preliminary experiments for applying MHE to additive margin softmax loss [47] which is a recently proposed well-performing objective function for face recognition. The loss function of AM-Softmax is given as follows:

1n

es· cosθ(xi,wyi )−mAMS

LAMS

=

− n

log

i=1

es·

cosθ(xi,wyi )−mAMS

+

e c

s·cosθ(xi,wj )

j=1,j=yi

(18)

where yi is the label of the training sample xi, n is the mini-batch size, mAMS is the hyperparameter that controls the degree of angular margin, and θ(xi,wj) denotes the angle between the training sample xi and the classiﬁer neuron wj. s is the hyperparameter that controls the projection radius of feature normalization [48, 47]. Similar to our SphereFace+, we combine full-space MHE to the output
layer to improve the inter-class feature separability. It is essentially following the same intuition of
SphereFace+ by adding an additional loss function to AM-Softmax loss.

Experiments. We perform a preliminary experiment to study the beneﬁts of MHE for improving AMSoftmax loss. We use the SphereFace-20 network and trained on CASIA-WebFace dataset (training settings are exactly the same as SphereFace+ in the main paper and [26]). The hyperparameters s, mAMS for AM-Softmax loss exactly follow the best setting in [47]. AM-Softmax achieves 99.26% accuracy on LFW, while combining MHE with AM-Softmax yields 99.37% accuracy on LFW. Such performance gain is actually very signiﬁcant in face veriﬁcation, which further validates the superiority of MHE.

17

F Improving GANs with MHE

We propose to improve the discriminator of GANs using MHE. It has been pointed out in [32] that the function space from which the discriminators are learned largely affects the performance of GANs. Therefore, it is of great importance to learn a good discriminator for GANs. As a recently proposed regularization to stablize the training of GANs, spectral normalization (SN) [32] encourages the Lipschitz constant of each layer’s weight matrix to be one. Since MHE exhibits signiﬁcant performance gain for CNNs as a regularization, we expect MHE can also improve the training of GANs by regularizing its discriminator. As a result, we perform a preliminary evaluation on applying MHE to GANs.

Speciﬁcally, for all methods except WGAN-GP [8], we use the standard objective function for the adversarial loss:

V (G, D) := Ex∼qdata(x)[log D(x)] + Ez∼p(z)[log(1 − D(G(z)))],

(19)

where z ∈ Rdz is a latent variable, p(z) is the normal distribution N (0, I), and G : Rdz → Rd0 is a
deterministic generator function. We set dz to 128 in all the experiments. For the updates of G, we used the alternate cost proposed by [6] −Ez∼p(z)[log(D(G(z)))] as used in [6, 50]. For the updates of D, we used the original cost function deﬁned in Eq. (19).

Recall from [32] that spectral normalization normalizes the spectral norm of the weight matrix W such that it makes the Lipschitz constraint σ(W ) to be one:

W¯ SN(W )

:=

W .
σ(W )

(20)

We apply MHE to the discriminator of standard GANs (with the original loss function in [6]) for image generation on CIFAR-10. In general, our experimental settings and training strategies (including architectures in Table 15) exactly follow spectral normalization [32]. For MHE, we use the half-space variant with Euclidean distance (Eq. (1)). We ﬁrst experiment regularizing the discriminator using MHE alone, and it yields comparable performance to SN and orthonormal regularization. Moreover, we also regularize the discriminator simultaneously using both MHE and SN, and it can give much better results than using either SN or MHE alone. The results in Table 14 show that MHE is potentially very useful for training GANs.

Method
Real data Weight clipping GAN-gradient penalty (GP) WGAN-GP [8] Batch Normalization [18] Layer Normalization [2] Weight Normalization [37] Orthonormal [3] SN-GANs [32]
MHE (ours) MHE+SN (ours) [32]

Inception score
11.24±.12 6.41±.11 6.93±.08 6.68±.06 6.27±.10 7.19±.12 6.84±.07 7.40±.12 7.42±.08 7.32±.10 7.59±.08

Table 14: Inception scores with unsupervised image generation on CIFAR-10.

18

F.1 Network Architecture for GAN
We give the detailed network architectures in Table 15 that are used in our experiments for the generator and the discriminator.

Table 15: Our CNN architectures for image Generation on CIFAR-10. The slopes of all leaky ReLU (lReLU) functions in the networks are set to 0.1.

z ∈ R128 ∼ N (0, I) dense → Mg × Mg × 512 4×4, stride=2 deconv. BN 256 ReLU 4×4, stride=2 deconv. BN 128 ReLU 4×4, stride=2 deconv. BN 64 ReLU 3×3, stride=1 conv. 3 Tanh
(a) Generator (Mg = 4 for CIFAR10).

RGB image x ∈ RM×M×3 3×3, stride=1 conv 64 lReLU 4×4, stride=2 conv 64 lReLU 3×3, stride=1 conv 128 lReLU 4×4, stride=2 conv 128 lReLU 3×3, stride=1 conv 256 lReLU 4×4, stride=2 conv 256 lReLU 3×3, stride=1 conv. 512 lReLU
dense → 1

(b) Discriminator (M = 32 CIFAR10).

F.2 Comparison of Random Generated Images
We provide some randomly generated images for comparison between baseline GAN and GAN regularized by both MHE and SN. The generated images are shown in Fig. 5.

Dataset

Baseline GAN Figure 5: Results of generated images.

GAN with MHE and SN

19

G More Results on Class-imbalance Learning

G.1 Class-imbalance learning on CIFAR-100
We perform additional experiments on CIFAR-100 to further validate the effectiveness of MHE in class-imbalance learning. In the CNN used in the experiment, we only apply MHE (i.e., full-space MHE) to the output layer, and use MHE or half-space MHE in the hidden layers. In general, the experimental settings are the same as the main paper. We still use CNN-9 (which is a 9-layer CNN from Table 11) in the experiment. Slightly differently from CIFAR-10 in the main paper, the two data imbalance settings on CIFAR-100 include 1) 10-class imbalance (denoted as Single in Table 16) - All classes have the same number of images but 10 classes (index from 0 to 9) have signiﬁcantly less number (only 10% training samples compared to the other normal classes), and 2) multiple class imbalance (denoted by Multiple in Table 16) - The number of images decreases as the class index decreases from 99 to 0. For the multiple class imbalance setting, we set the number of each class equals to 5 × (class_index + 1). Experiment details are similar to the CIFAR-10 experiment, which is speciﬁed in Appendix A. The results in Table 16 show that MHE consistently improves CNNs in class-imbalance learning on CIFAR-100. In most cases, half-space MHE performs better than full-space MHE.

Method
Baseline Orthonormal
MHE Half-space MHE
A-MHE Half-space A-MHE

Single
31.43 30.75 29.30 29.40 30.16 29.60

Multiple
38.39 37.89 37.07 36.52 37.54 37.07

Table 16: Error rate (%) on imbalanced CIFAR-100.

G.2 2D CNN Feature Visualization

(a) CNN without MHE (Training Set)

(b) CNN with MHE (Training Set)

(c) CNN without MHE (Testing Set)

(d) CNN features with MHE (Testing Set)

Figure 6: 2D CNN features with or without MHE on both training set and testing set. The features are computed by setting the output feature dimension as 2, similar to [27]. Each point denotes the 2D feature of a data point, and each color denotes a class. The red arrows are the classiﬁer neurons of the output layer.

20

The experimental settings are the same as the main paper. We supplement the 2D feature visualization on testing set in Fig. 6. The visualized features on both training set and testing set well demonstrate the superiority of MHE in class-imbalance learning. In the CNN without MHE, the classiﬁer neuron of the imbalanced training data is highly biased towards another class, and therefore can not be properly learned. In contrast, the CNN with MHE can learn uniformly distributed classiﬁer neurons, which greatly improves the network’s generalization ability.
21

