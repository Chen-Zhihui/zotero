<html style="" class=" js flexbox canvas canvastext webgl no-touch geolocation postmessage no-websqldatabase indexeddb hashchange history draganddrop websockets rgba hsla multiplebgs backgroundsize borderimage borderradius boxshadow textshadow opacity cssanimations csscolumns cssgradients no-cssreflections csstransforms csstransforms3d csstransitions fontface generatedcontent video audio localstorage sessionstorage webworkers applicationcache svg inlinesvg smil svgclippaths"><!--<![endif]--><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Extending PyTorch — PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/notes/extending.html">
  

  

  
  
    

  

  <link rel="stylesheet" href="theme.css" type="text/css">
  <link rel="stylesheet" href="pygments.css" type="text/css">
  <link rel="stylesheet" href="css.css" type="text/css">
  <link rel="stylesheet" href="pytorch_theme.css" type="text/css">
    <link rel="index" title="Index" href="https://pytorch.org/docs/stable/genindex.html">
    <link rel="search" title="Search" href="https://pytorch.org/docs/stable/search.html">
    <link rel="next" title="Frequently Asked Questions" href="https://pytorch.org/docs/stable/notes/faq.html">
    <link rel="prev" title="CUDA semantics" href="https://pytorch.org/docs/stable/notes/cuda.html"> 

  
  <script async="" src="analytics.js"></script><script async="" src="analytics.js"></script><script src="modernizr.js"></script>

<style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>

<body class="wy-body-for-nav"><div id="MathJax_Message" style="display: none;"></div><div id="MathJax_Message" style="display: none;"></div>

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="https://pytorch.org/docs/stable/index.html">
          

          
            
            <img src="pytorch-logo-dark.svg" class="logo" alt="Logo">
          
          </a>

          
            
            
              <div class="version">
                0.4.1 <br> <a href="http://pytorch.org/docs/versions.html"> version selector ▼</a>
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input name="q" placeholder="Search docs" type="text">
    <input name="check_keywords" value="yes" type="hidden">
    <input name="area" value="default" type="hidden">
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/autograd.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>Autograd mechanics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/autograd.html#excluding-subgraphs-from-backward"><span class="toctree-expand"></span><span class="toctree-expand"></span>Excluding subgraphs from backward</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/autograd.html#requires-grad"><code class="docutils literal notranslate"><span class="pre">requires_grad</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/autograd.html#how-autograd-encodes-the-history">How autograd encodes the history</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd">In-place operations with autograd</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/broadcasting.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>Broadcasting semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/broadcasting.html#general-semantics">General semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/broadcasting.html#in-place-semantics">In-place semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/broadcasting.html#backwards-compatibility">Backwards compatibility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/cuda.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>CUDA semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/cuda.html#asynchronous-execution"><span class="toctree-expand"></span><span class="toctree-expand"></span>Asynchronous execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/cuda.html#cuda-streams">CUDA streams</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/cuda.html#best-practices"><span class="toctree-expand"></span><span class="toctree-expand"></span>Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code">Device-agnostic code</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/cuda.html#use-pinned-memory-buffers">Use pinned memory buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/cuda.html#use-nn-dataparallel-instead-of-multiprocessing">Use nn.DataParallel instead of multiprocessing</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal current" href="#"><span class="toctree-expand"></span><span class="toctree-expand"></span>Extending PyTorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#extending-torch-autograd">Extending <code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#extending-torch-nn"><span class="toctree-expand"></span><span class="toctree-expand"></span>Extending <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="#adding-a-module">Adding a <code class="docutils literal notranslate"><span class="pre">Module</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#writing-custom-c-extensions">Writing custom C++ extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id1">Writing custom C extensions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/faq.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/faq.html#my-model-reports-cuda-runtime-error-2-out-of-memory">My model reports “cuda runtime error(2): out of memory”</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/faq.html#my-gpu-memory-isn-t-freed-properly">My GPU memory isn’t freed properly</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/faq.html#my-data-loader-workers-return-identical-random-numbers">My data loader workers return identical random numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/faq.html#my-recurrent-network-doesn-t-work-with-data-parallelism">My recurrent network doesn’t work with data parallelism</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/multiprocessing.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>Multiprocessing best practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/multiprocessing.html#best-practices-and-tips"><span class="toctree-expand"></span><span class="toctree-expand"></span>Best practices and tips</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/multiprocessing.html#avoiding-and-fighting-deadlocks">Avoiding and fighting deadlocks</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/multiprocessing.html#reuse-buffers-passed-through-a-queue">Reuse buffers passed through a Queue</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/multiprocessing.html#asynchronous-multiprocess-training-e-g-hogwild"><span class="toctree-expand"></span><span class="toctree-expand"></span>Asynchronous multiprocess training (e.g. Hogwild)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/multiprocessing.html#hogwild">Hogwild</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/serialization.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>Serialization semantics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/serialization.html#best-practices"><span class="toctree-expand"></span><span class="toctree-expand"></span>Best practices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/serialization.html#recommended-approach-for-saving-a-model">Recommended approach for saving a model</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>Windows FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#building-from-source"><span class="toctree-expand"></span><span class="toctree-expand"></span>Building from source</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#include-optional-components">Include optional components</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#speeding-cuda-build-for-windows">Speeding CUDA build for Windows</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#one-key-install-script">One key install script</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#extension"><span class="toctree-expand"></span><span class="toctree-expand"></span>Extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#cffi-extension">CFFI Extension</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#cpp-extension">Cpp Extension</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#installation"><span class="toctree-expand"></span><span class="toctree-expand"></span>Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#package-not-found-in-win-32-channel">Package not found in win-32 channel.</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#why-are-there-no-python-2-packages-for-windows">Why are there no Python 2 packages for Windows?</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#import-error">Import error</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#usage-multiprocessing"><span class="toctree-expand"></span><span class="toctree-expand"></span>Usage (multiprocessing)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection">Multiprocessing error without if-clause protection</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-broken-pipe">Multiprocessing error “Broken pipe”</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-driver-shut-down">Multiprocessing error “driver shut down”</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/notes/windows.html#cuda-ipc-operations">CUDA IPC operations</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#tensors"><span class="toctree-expand"></span><span class="toctree-expand"></span>Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#creation-ops">Creation Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#indexing-slicing-joining-mutating-ops">Indexing, Slicing, Joining, Mutating Ops</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#random-sampling"><span class="toctree-expand"></span><span class="toctree-expand"></span>Random sampling</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#in-place-random-sampling">In-place random sampling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#serialization">Serialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#parallelism">Parallelism</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#math-operations"><span class="toctree-expand"></span><span class="toctree-expand"></span>Math operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#pointwise-ops">Pointwise Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#reduction-ops">Reduction Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#comparison-ops">Comparison Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#spectral-ops">Spectral Ops</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#other-operations">Other Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torch.html#blas-and-lapack-operations">BLAS and LAPACK Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/tensor_attributes.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>Tensor Attributes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype">torch.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch-device">torch.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch-layout">torch.layout</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/cuda.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torch.cuda</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/cuda.html#random-number-generator">Random Number Generator</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/cuda.html#communication-collectives">Communication collectives</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/cuda.html#streams-and-events">Streams and events</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/cuda.html#memory-management">Memory management</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/cuda.html#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torch.nn</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#containers"><span class="toctree-expand"></span><span class="toctree-expand"></span>Containers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#module"><span class="hidden-section">Module</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#sequential"><span class="hidden-section">Sequential</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#modulelist"><span class="hidden-section">ModuleList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#moduledict"><span class="hidden-section">ModuleDict</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#parameterlist"><span class="hidden-section">ParameterList</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#parameterdict"><span class="hidden-section">ParameterDict</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#convolution-layers"><span class="toctree-expand"></span><span class="toctree-expand"></span>Convolution layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#conv1d"><span class="hidden-section">Conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#conv2d"><span class="hidden-section">Conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#conv3d"><span class="hidden-section">Conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#convtranspose1d"><span class="hidden-section">ConvTranspose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#convtranspose2d"><span class="hidden-section">ConvTranspose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#convtranspose3d"><span class="hidden-section">ConvTranspose3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#unfold"><span class="hidden-section">Unfold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#fold"><span class="hidden-section">Fold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#pooling-layers"><span class="toctree-expand"></span><span class="toctree-expand"></span>Pooling layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#maxpool1d"><span class="hidden-section">MaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#maxpool2d"><span class="hidden-section">MaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#maxpool3d"><span class="hidden-section">MaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#maxunpool1d"><span class="hidden-section">MaxUnpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#maxunpool2d"><span class="hidden-section">MaxUnpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#maxunpool3d"><span class="hidden-section">MaxUnpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#avgpool1d"><span class="hidden-section">AvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#avgpool2d"><span class="hidden-section">AvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#avgpool3d"><span class="hidden-section">AvgPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#fractionalmaxpool2d"><span class="hidden-section">FractionalMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#lppool1d"><span class="hidden-section">LPPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#lppool2d"><span class="hidden-section">LPPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptivemaxpool1d"><span class="hidden-section">AdaptiveMaxPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptivemaxpool2d"><span class="hidden-section">AdaptiveMaxPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptivemaxpool3d"><span class="hidden-section">AdaptiveMaxPool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptiveavgpool1d"><span class="hidden-section">AdaptiveAvgPool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptiveavgpool2d"><span class="hidden-section">AdaptiveAvgPool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptiveavgpool3d"><span class="hidden-section">AdaptiveAvgPool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#padding-layers"><span class="toctree-expand"></span><span class="toctree-expand"></span>Padding layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#reflectionpad1d"><span class="hidden-section">ReflectionPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#reflectionpad2d"><span class="hidden-section">ReflectionPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#replicationpad1d"><span class="hidden-section">ReplicationPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#replicationpad2d"><span class="hidden-section">ReplicationPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#replicationpad3d"><span class="hidden-section">ReplicationPad3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#zeropad2d"><span class="hidden-section">ZeroPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#constantpad1d"><span class="hidden-section">ConstantPad1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#constantpad2d"><span class="hidden-section">ConstantPad2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#constantpad3d"><span class="hidden-section">ConstantPad3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity"><span class="toctree-expand"></span><span class="toctree-expand"></span>Non-linear activations (weighted sum, nonlinearity)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#elu"><span class="hidden-section">ELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#hardshrink"><span class="hidden-section">Hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#hardtanh"><span class="hidden-section">Hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#leakyrelu"><span class="hidden-section">LeakyReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#logsigmoid"><span class="hidden-section">LogSigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#prelu"><span class="hidden-section">PReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#relu"><span class="hidden-section">ReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#relu6"><span class="hidden-section">ReLU6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#rrelu"><span class="hidden-section">RReLU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#selu"><span class="hidden-section">SELU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#sigmoid"><span class="hidden-section">Sigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#softplus"><span class="hidden-section">Softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#softshrink"><span class="hidden-section">Softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#softsign"><span class="hidden-section">Softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#tanh"><span class="hidden-section">Tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#tanhshrink"><span class="hidden-section">Tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#threshold"><span class="hidden-section">Threshold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-other"><span class="toctree-expand"></span><span class="toctree-expand"></span>Non-linear activations (other)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#softmin"><span class="hidden-section">Softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#softmax"><span class="hidden-section">Softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#softmax2d"><span class="hidden-section">Softmax2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#logsoftmax"><span class="hidden-section">LogSoftmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptivelogsoftmaxwithloss"><span class="hidden-section">AdaptiveLogSoftmaxWithLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#normalization-layers"><span class="toctree-expand"></span><span class="toctree-expand"></span>Normalization layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#batchnorm1d"><span class="hidden-section">BatchNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#batchnorm2d"><span class="hidden-section">BatchNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#batchnorm3d"><span class="hidden-section">BatchNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#groupnorm"><span class="hidden-section">GroupNorm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#instancenorm1d"><span class="hidden-section">InstanceNorm1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#instancenorm2d"><span class="hidden-section">InstanceNorm2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#instancenorm3d"><span class="hidden-section">InstanceNorm3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#layernorm"><span class="hidden-section">LayerNorm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#localresponsenorm"><span class="hidden-section">LocalResponseNorm</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#recurrent-layers"><span class="toctree-expand"></span><span class="toctree-expand"></span>Recurrent layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#rnn"><span class="hidden-section">RNN</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#lstm"><span class="hidden-section">LSTM</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#gru"><span class="hidden-section">GRU</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#rnncell"><span class="hidden-section">RNNCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#lstmcell"><span class="hidden-section">LSTMCell</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#grucell"><span class="hidden-section">GRUCell</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#linear-layers"><span class="toctree-expand"></span><span class="toctree-expand"></span>Linear layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#linear"><span class="hidden-section">Linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#bilinear"><span class="hidden-section">Bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#dropout-layers"><span class="toctree-expand"></span><span class="toctree-expand"></span>Dropout layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#dropout"><span class="hidden-section">Dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#dropout2d"><span class="hidden-section">Dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#dropout3d"><span class="hidden-section">Dropout3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#alphadropout"><span class="hidden-section">AlphaDropout</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#sparse-layers"><span class="toctree-expand"></span><span class="toctree-expand"></span>Sparse layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#embedding"><span class="hidden-section">Embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#embeddingbag"><span class="hidden-section">EmbeddingBag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#distance-functions"><span class="toctree-expand"></span><span class="toctree-expand"></span>Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#cosinesimilarity"><span class="hidden-section">CosineSimilarity</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#pairwisedistance"><span class="hidden-section">PairwiseDistance</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#loss-functions"><span class="toctree-expand"></span><span class="toctree-expand"></span>Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#l1loss"><span class="hidden-section">L1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#mseloss"><span class="hidden-section">MSELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#crossentropyloss"><span class="hidden-section">CrossEntropyLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#nllloss"><span class="hidden-section">NLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#poissonnllloss"><span class="hidden-section">PoissonNLLLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#kldivloss"><span class="hidden-section">KLDivLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#bceloss"><span class="hidden-section">BCELoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss"><span class="hidden-section">BCEWithLogitsLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#marginrankingloss"><span class="hidden-section">MarginRankingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#hingeembeddingloss"><span class="hidden-section">HingeEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#multilabelmarginloss"><span class="hidden-section">MultiLabelMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#smoothl1loss"><span class="hidden-section">SmoothL1Loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#softmarginloss"><span class="hidden-section">SoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#multilabelsoftmarginloss"><span class="hidden-section">MultiLabelSoftMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#cosineembeddingloss"><span class="hidden-section">CosineEmbeddingLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#multimarginloss"><span class="hidden-section">MultiMarginLoss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#tripletmarginloss"><span class="hidden-section">TripletMarginLoss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#vision-layers"><span class="toctree-expand"></span><span class="toctree-expand"></span>Vision layers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#pixelshuffle"><span class="hidden-section">PixelShuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#upsample"><span class="hidden-section">Upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#upsamplingnearest2d"><span class="hidden-section">UpsamplingNearest2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#upsamplingbilinear2d"><span class="hidden-section">UpsamplingBilinear2d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed"><span class="toctree-expand"></span><span class="toctree-expand"></span>DataParallel layers (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#dataparallel"><span class="hidden-section">DataParallel</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#distributeddataparallel"><span class="hidden-section">DistributedDataParallel</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#utilities"><span class="toctree-expand"></span><span class="toctree-expand"></span>Utilities</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#clip-grad-norm"><span class="hidden-section">clip_grad_norm_</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#clip-grad-value"><span class="hidden-section">clip_grad_value_</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#parameters-to-vector"><span class="hidden-section">parameters_to_vector</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#vector-to-parameters"><span class="hidden-section">vector_to_parameters</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#weight-norm"><span class="hidden-section">weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#remove-weight-norm"><span class="hidden-section">remove_weight_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#spectral-norm"><span class="hidden-section">spectral_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#remove-spectral-norm"><span class="hidden-section">remove_spectral_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#packedsequence"><span class="hidden-section">PackedSequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#pack-padded-sequence"><span class="hidden-section">pack_padded_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#pad-packed-sequence"><span class="hidden-section">pad_packed_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#pad-sequence"><span class="hidden-section">pad_sequence</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#pack-sequence"><span class="hidden-section">pack_sequence</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional"><span class="toctree-expand"></span><span class="toctree-expand"></span>torch.nn.functional</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#convolution-functions"><span class="toctree-expand"></span><span class="toctree-expand"></span>Convolution functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id20"><span class="hidden-section">conv1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id21"><span class="hidden-section">conv2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id22"><span class="hidden-section">conv3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#conv-transpose1d"><span class="hidden-section">conv_transpose1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#conv-transpose2d"><span class="hidden-section">conv_transpose2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#conv-transpose3d"><span class="hidden-section">conv_transpose3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id23"><span class="hidden-section">unfold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id24"><span class="hidden-section">fold</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#pooling-functions"><span class="toctree-expand"></span><span class="toctree-expand"></span>Pooling functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#avg-pool1d"><span class="hidden-section">avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#avg-pool2d"><span class="hidden-section">avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#avg-pool3d"><span class="hidden-section">avg_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#max-pool1d"><span class="hidden-section">max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#max-pool2d"><span class="hidden-section">max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#max-pool3d"><span class="hidden-section">max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#max-unpool1d"><span class="hidden-section">max_unpool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#max-unpool2d"><span class="hidden-section">max_unpool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#max-unpool3d"><span class="hidden-section">max_unpool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#lp-pool1d"><span class="hidden-section">lp_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#lp-pool2d"><span class="hidden-section">lp_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptive-max-pool1d"><span class="hidden-section">adaptive_max_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptive-max-pool2d"><span class="hidden-section">adaptive_max_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptive-max-pool3d"><span class="hidden-section">adaptive_max_pool3d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptive-avg-pool1d"><span class="hidden-section">adaptive_avg_pool1d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptive-avg-pool2d"><span class="hidden-section">adaptive_avg_pool2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#adaptive-avg-pool3d"><span class="hidden-section">adaptive_avg_pool3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#non-linear-activation-functions"><span class="toctree-expand"></span><span class="toctree-expand"></span>Non-linear activation functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id25"><span class="hidden-section">threshold</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id26"><span class="hidden-section">relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id27"><span class="hidden-section">hardtanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id28"><span class="hidden-section">relu6</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id29"><span class="hidden-section">elu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id30"><span class="hidden-section">selu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#leaky-relu"><span class="hidden-section">leaky_relu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id31"><span class="hidden-section">prelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id32"><span class="hidden-section">rrelu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#glu"><span class="hidden-section">glu</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id33"><span class="hidden-section">logsigmoid</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id34"><span class="hidden-section">hardshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id35"><span class="hidden-section">tanhshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id36"><span class="hidden-section">softsign</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id37"><span class="hidden-section">softplus</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id38"><span class="hidden-section">softmin</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id39"><span class="hidden-section">softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id40"><span class="hidden-section">softshrink</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#gumbel-softmax"><span class="hidden-section">gumbel_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#log-softmax"><span class="hidden-section">log_softmax</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id41"><span class="hidden-section">tanh</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id42"><span class="hidden-section">sigmoid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#normalization-functions"><span class="toctree-expand"></span><span class="toctree-expand"></span>Normalization functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#batch-norm"><span class="hidden-section">batch_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#instance-norm"><span class="hidden-section">instance_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#layer-norm"><span class="hidden-section">layer_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#local-response-norm"><span class="hidden-section">local_response_norm</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#normalize"><span class="hidden-section">normalize</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#linear-functions"><span class="toctree-expand"></span><span class="toctree-expand"></span>Linear functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id43"><span class="hidden-section">linear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id44"><span class="hidden-section">bilinear</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#dropout-functions"><span class="toctree-expand"></span><span class="toctree-expand"></span>Dropout functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id45"><span class="hidden-section">dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#alpha-dropout"><span class="hidden-section">alpha_dropout</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id46"><span class="hidden-section">dropout2d</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id47"><span class="hidden-section">dropout3d</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#sparse-functions"><span class="toctree-expand"></span><span class="toctree-expand"></span>Sparse functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id48"><span class="hidden-section">embedding</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#embedding-bag"><span class="hidden-section">embedding_bag</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id49"><span class="toctree-expand"></span><span class="toctree-expand"></span>Distance functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#pairwise-distance"><span class="hidden-section">pairwise_distance</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#cosine-similarity"><span class="hidden-section">cosine_similarity</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id50"><span class="toctree-expand"></span><span class="toctree-expand"></span>Loss functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#binary-cross-entropy"><span class="hidden-section">binary_cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#poisson-nll-loss"><span class="hidden-section">poisson_nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#cosine-embedding-loss"><span class="hidden-section">cosine_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#cross-entropy"><span class="hidden-section">cross_entropy</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#hinge-embedding-loss"><span class="hidden-section">hinge_embedding_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#kl-div"><span class="hidden-section">kl_div</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#l1-loss"><span class="hidden-section">l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#mse-loss"><span class="hidden-section">mse_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#margin-ranking-loss"><span class="hidden-section">margin_ranking_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#multilabel-margin-loss"><span class="hidden-section">multilabel_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#multilabel-soft-margin-loss"><span class="hidden-section">multilabel_soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#multi-margin-loss"><span class="hidden-section">multi_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#nll-loss"><span class="hidden-section">nll_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#binary-cross-entropy-with-logits"><span class="hidden-section">binary_cross_entropy_with_logits</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#smooth-l1-loss"><span class="hidden-section">smooth_l1_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#soft-margin-loss"><span class="hidden-section">soft_margin_loss</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#triplet-margin-loss"><span class="hidden-section">triplet_margin_loss</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#vision-functions"><span class="toctree-expand"></span><span class="toctree-expand"></span>Vision functions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#pixel-shuffle"><span class="hidden-section">pixel_shuffle</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#pad"><span class="hidden-section">pad</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#interpolate"><span class="hidden-section">interpolate</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#id51"><span class="hidden-section">upsample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#upsample-nearest"><span class="hidden-section">upsample_nearest</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#upsample-bilinear"><span class="hidden-section">upsample_bilinear</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#grid-sample"><span class="hidden-section">grid_sample</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#affine-grid"><span class="hidden-section">affine_grid</span></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#dataparallel-functions-multi-gpu-distributed"><span class="toctree-expand"></span><span class="toctree-expand"></span>DataParallel functions (multi-GPU, distributed)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#data-parallel"><span class="hidden-section">data_parallel</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#torch-nn-init">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/optim.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torch.optim</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/optim.html#how-to-use-an-optimizer"><span class="toctree-expand"></span><span class="toctree-expand"></span>How to use an optimizer</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/optim.html#constructing-it">Constructing it</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/optim.html#per-parameter-options">Per-parameter options</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step"><span class="toctree-expand"></span><span class="toctree-expand"></span>Taking an optimization step</a><ul>
<li class="toctree-l4"><a class="reference internal" href="https://pytorch.org/docs/stable/optim.html#optimizer-step"><code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="https://pytorch.org/docs/stable/optim.html#optimizer-step-closure"><code class="docutils literal notranslate"><span class="pre">optimizer.step(closure)</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/optim.html#algorithms">Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">How to adjust Learning Rate</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torch.autograd</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#locally-disabling-gradient-computation">Locally disabling gradient computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#in-place-operations-on-tensors"><span class="toctree-expand"></span><span class="toctree-expand"></span>In-place operations on Tensors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#in-place-correctness-checks">In-place correctness checks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#variable-deprecated">Variable (deprecated)</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#tensor-autograd-functions">Tensor autograd functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#function"><span class="hidden-section">Function</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#numerical-gradient-checking">Numerical gradient checking</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#profiler">Profiler</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#anomaly-detection">Anomaly detection</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torch.distributions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#score-function">Score function</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#pathwise-derivative">Pathwise derivative</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#distribution"><span class="hidden-section">Distribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#exponentialfamily"><span class="hidden-section">ExponentialFamily</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#bernoulli"><span class="hidden-section">Bernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#beta"><span class="hidden-section">Beta</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#binomial"><span class="hidden-section">Binomial</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#categorical"><span class="hidden-section">Categorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#cauchy"><span class="hidden-section">Cauchy</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#chi2"><span class="hidden-section">Chi2</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#dirichlet"><span class="hidden-section">Dirichlet</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#exponential"><span class="hidden-section">Exponential</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#fishersnedecor"><span class="hidden-section">FisherSnedecor</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#gamma"><span class="hidden-section">Gamma</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#geometric"><span class="hidden-section">Geometric</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#gumbel"><span class="hidden-section">Gumbel</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#halfcauchy"><span class="hidden-section">HalfCauchy</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#halfnormal"><span class="hidden-section">HalfNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#independent"><span class="hidden-section">Independent</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#laplace"><span class="hidden-section">Laplace</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#lognormal"><span class="hidden-section">LogNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#multinomial"><span class="hidden-section">Multinomial</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#multivariatenormal"><span class="hidden-section">MultivariateNormal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#normal"><span class="hidden-section">Normal</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#onehotcategorical"><span class="hidden-section">OneHotCategorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#pareto"><span class="hidden-section">Pareto</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#poisson"><span class="hidden-section">Poisson</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#relaxedbernoulli"><span class="hidden-section">RelaxedBernoulli</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#relaxedonehotcategorical"><span class="hidden-section">RelaxedOneHotCategorical</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#studentt"><span class="hidden-section">StudentT</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#transformeddistribution"><span class="hidden-section">TransformedDistribution</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#uniform"><span class="hidden-section">Uniform</span></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.kl"><cite>KL Divergence</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.transforms"><cite>Transforms</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.constraints"><cite>Constraints</cite></a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributions.html#module-torch.distributions.constraint_registry"><cite>Constraint Registry</cite></a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/multiprocessing.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torch.multiprocessing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/multiprocessing.html#strategy-management">Strategy management</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/multiprocessing.html#sharing-cuda-tensors">Sharing CUDA tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/multiprocessing.html#sharing-strategies"><span class="toctree-expand"></span><span class="toctree-expand"></span>Sharing strategies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/multiprocessing.html#file-descriptor-file-descriptor">File descriptor - <code class="docutils literal notranslate"><span class="pre">file_descriptor</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/multiprocessing.html#file-system-file-system">File system - <code class="docutils literal notranslate"><span class="pre">file_system</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/distributed.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torch.distributed</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributed.html#basics">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributed.html#initialization"><span class="toctree-expand"></span><span class="toctree-expand"></span>Initialization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/distributed.html#tcp-initialization">TCP initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/distributed.html#shared-file-system-initialization">Shared file-system initialization</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/distributed.html#environment-variable-initialization">Environment variable initialization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributed.html#groups">Groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributed.html#point-to-point-communication">Point-to-point communication</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributed.html#collective-functions">Collective functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributed.html#multi-gpu-collective-functions">Multi-GPU collective functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/distributed.html#launch-utility">Launch utility</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/ffi.html">torch.utils.ffi</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/onnx.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torch.onnx</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/onnx.html#example-end-to-end-alexnet-from-pytorch-to-caffe2">Example: End-to-end AlexNet from PyTorch to Caffe2</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/onnx.html#limitations">Limitations</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/onnx.html#supported-operators">Supported operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/onnx.html#functions">Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/legacy.html">torch.legacy</a></li>
</ul>
<p class="caption"><span class="caption-text">torchvision Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/index.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torchvision</a><ul>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torchvision.datasets</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#mnist">MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#fashion-mnist">Fashion-MNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#emnist">EMNIST</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#coco"><span class="toctree-expand"></span><span class="toctree-expand"></span>COCO</a><ul>
<li class="toctree-l4"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#captions">Captions</a></li>
<li class="toctree-l4"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#detection">Detection</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#lsun">LSUN</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder">ImageFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#datasetfolder">DatasetFolder</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#imagenet-12">Imagenet-12</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#cifar">CIFAR</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#stl10">STL10</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#svhn">SVHN</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/datasets.html#phototour">PhotoTour</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/models.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torchvision.models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/models.html#id1">Alexnet</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/models.html#id2">VGG</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/models.html#id3">ResNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/models.html#id4">SqueezeNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/models.html#id5">DenseNet</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/models.html#inception-v3">Inception v3</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/transforms.html"><span class="toctree-expand"></span><span class="toctree-expand"></span>torchvision.transforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/transforms.html#transforms-on-pil-image">Transforms on PIL Image</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/transforms.html#transforms-on-torch-tensor">Transforms on torch.*Tensor</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/transforms.html#conversion-transforms">Conversion Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/transforms.html#generic-transforms">Generic Transforms</a></li>
<li class="toctree-l3"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/transforms.html#module-torchvision.transforms.functional">Functional Transforms</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="https://pytorch.org/docs/stable/torchvision/utils.html">torchvision.utils</a></li>
</ul>
</li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a> »</li>
        
      <li>Extending PyTorch</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="https://pytorch.org/docs/stable/_sources/notes/extending.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="extending-pytorch">
<h1>Extending PyTorch<a class="headerlink" href="#extending-pytorch" title="Permalink to this headline">¶</a></h1>
<p>In this note we’ll cover ways of extending <a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn</span></code></a>,
<a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.autograd</span></code></a>, and writing custom C extensions utilizing our C
libraries.</p>
<div class="section" id="extending-torch-autograd">
<h2>Extending <a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.autograd</span></code></a><a class="headerlink" href="#extending-torch-autograd" title="Permalink to this headline">¶</a></h2>
<p>Adding operations to <a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal notranslate"><span class="pre">autograd</span></code></a> requires implementing a new
<a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a> subclass for each operation. Recall that <a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a> s
are what <a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal notranslate"><span class="pre">autograd</span></code></a> uses to compute the results and gradients, and
encode the operation history. Every new function requires you to implement 2
methods:</p>
<ul class="simple">
<li><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> - the code that performs the operation. It can take
as many arguments as you want, with some of them being optional, if you
specify the default values. All kinds of Python objects are accepted here.
<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> arguments that track history (i.e., with
<code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>) will be converted to ones that don’t track history
before the call, and their use will be registered in the graph. Note that this
logic won’t traverse lists/dicts/any other data structures and will only
consider <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> s that are direct arguments to the call. You can
return either a single <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> output, or a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a> of
<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> s if there are multiple outputs. Also, please refer to the
docs of <a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a> to find descriptions of useful methods that can be
called only from <a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a>.</li>
<li><a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function.backward" title="torch.autograd.Function.backward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a> - gradient formula. It will be given
as many <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> arguments as there were outputs, with each of them
representing gradient w.r.t. that output. It should return as many
<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code> s as there were inputs, with each of them containing the
gradient w.r.t. its corresponding input. If your inputs didn’t require
gradient (<code class="xref py py-attr docutils literal notranslate"><span class="pre">needs_input_grad</span></code> is a tuple of booleans indicating
whether each input needs gradient computation), or were non-<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>
objects, you can return <code class="xref py py-class docutils literal notranslate"><span class="pre">None</span></code>. Also, if you have optional
arguments to <a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function.forward" title="torch.autograd.Function.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> you can return more gradients than there
were inputs, as long as they’re all <a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.7)"><code class="docutils literal notranslate"><span class="pre">None</span></code></a>.</li>
</ul>
<p>Below you can find code for a <code class="docutils literal notranslate"><span class="pre">Linear</span></code> function from <a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn</span></code></a>, with
additional comments:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Inherit from Function</span>
<span class="k">class</span> <span class="nc">LinearFunction</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>

    <span class="c1"># Note that both forward and backward are @staticmethods</span>
    <span class="nd">@staticmethod</span>
    <span class="c1"># bias is an optional argument</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">+=</span> <span class="n">bias</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="c1"># This function has only a single output, so it gets only one gradient</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="c1"># This is a pattern that is very convenient - at the top of backward</span>
        <span class="c1"># unpack saved_tensors and initialize all gradients w.r.t. inputs to</span>
        <span class="c1"># None. Thanks to the fact that additional trailing Nones are</span>
        <span class="c1"># ignored, the return statement is simple even when the function has</span>
        <span class="c1"># optional inputs.</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_weight</span> <span class="o">=</span> <span class="n">grad_bias</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># These needs_input_grad checks are optional and there only to</span>
        <span class="c1"># improve efficiency. If you want to make your code simpler, you can</span>
        <span class="c1"># skip them. Returning gradients for inputs that don't require it is</span>
        <span class="c1"># not an error.</span>
        <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="n">grad_input</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">grad_weight</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">ctx</span><span class="o">.</span><span class="n">needs_input_grad</span><span class="p">[</span><span class="mi">2</span><span class="p">]:</span>
            <span class="n">grad_bias</span> <span class="o">=</span> <span class="n">grad_output</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_weight</span><span class="p">,</span> <span class="n">grad_bias</span>
</pre></div>
</div>
<p>Now, to make it easier to use these custom ops, we recommend aliasing their
<code class="docutils literal notranslate"><span class="pre">apply</span></code> method:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">linear</span> <span class="o">=</span> <span class="n">LinearFunction</span><span class="o">.</span><span class="n">apply</span>
</pre></div>
</div>
<p>Here, we give an additional example of a function that is parametrized by
non-Tensor arguments:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MulConstant</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">tensor</span><span class="p">,</span> <span class="n">constant</span><span class="p">):</span>
        <span class="c1"># ctx is a context object that can be used to stash information</span>
        <span class="c1"># for backward computation</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">constant</span> <span class="o">=</span> <span class="n">constant</span>
        <span class="k">return</span> <span class="n">tensor</span> <span class="o">*</span> <span class="n">constant</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
        <span class="c1"># We return as many input gradients as there were arguments.</span>
        <span class="c1"># Gradients of non-Tensor arguments to forward must be None.</span>
        <span class="k">return</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">ctx</span><span class="o">.</span><span class="n">constant</span><span class="p">,</span> <span class="kc">None</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Inputs to <code class="docutils literal notranslate"><span class="pre">backward</span></code>, i.e., <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code>, can also be Tensors that
track history. So if <code class="docutils literal notranslate"><span class="pre">backward</span></code> is implemented with differentiable
operations, (e.g., invocation of another custom
<code class="xref py py-class docutils literal notranslate"><span class="pre">function</span></code>), higher order derivatives will work.</p>
</div>
<p>You probably want to check if the backward method you implemented actually
computes the derivatives of your function. It is possible by comparing with
numerical approximations using small finite differences:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">gradcheck</span>

<span class="c1"># gradcheck takes a tuple of tensors as input, check if your gradient</span>
<span class="c1"># evaluated with these tensors are close enough to numerical</span>
<span class="c1"># approximations and returns True if they all verify this condition.</span>
<span class="nb">input</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">,</span><span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">gradcheck</span><span class="p">(</span><span class="n">linear</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#grad-check"><span class="std std-ref">Numerical gradient checking</span></a> for more details on finite-difference gradient comparisons.</p>
</div>
<div class="section" id="extending-torch-nn">
<h2>Extending <a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.nn</span></code></a><a class="headerlink" href="#extending-torch-nn" title="Permalink to this headline">¶</a></h2>
<p><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nn</span></code></a> exports two kinds of interfaces - modules and their functional
versions. You can extend it in both ways, but we recommend using modules for
all kinds of layers, that hold any parameters or buffers, and recommend using
a functional form parameter-less operations like activation functions, pooling,
etc.</p>
<p>Adding a functional version of an operation is already fully covered in the
section above.</p>
<div class="section" id="adding-a-module">
<h3>Adding a <a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a><a class="headerlink" href="#adding-a-module" title="Permalink to this headline">¶</a></h3>
<p>Since <a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#module-torch.nn" title="torch.nn"><code class="xref py py-mod docutils literal notranslate"><span class="pre">nn</span></code></a> heavily utilizes <a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#module-torch.autograd" title="torch.autograd"><code class="xref py py-mod docutils literal notranslate"><span class="pre">autograd</span></code></a>, adding a new
<a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" title="torch.nn.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> requires implementing a <a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a>
that performs the operation and can compute the gradient. From now on let’s
assume that we want to implement a <code class="docutils literal notranslate"><span class="pre">Linear</span></code> module and we have the function
implemented as in the listing above. There’s very little code required to
add this. Now, there are two functions that need to be implemented:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">__init__</span></code> (<em>optional</em>) - takes in arguments such as kernel sizes, numbers
of features, etc. and initializes parameters and buffers.</li>
<li><a class="reference internal" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward" title="torch.nn.Module.forward"><code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code></a> - instantiates a <a class="reference internal" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="torch.autograd.Function"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a> and
uses it to perform the operation. It’s very similar to a functional wrapper
shown above.</li>
</ul>
<p>This is how a <code class="docutils literal notranslate"><span class="pre">Linear</span></code> module can be implemented:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Linear</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">,</span> <span class="n">output_features</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Linear</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_features</span> <span class="o">=</span> <span class="n">output_features</span>

        <span class="c1"># nn.Parameter is a special kind of Tensor, that will get</span>
        <span class="c1"># automatically registered as Module's parameter once it's assigned</span>
        <span class="c1"># as an attribute. Parameters and buffers need to be registered, or</span>
        <span class="c1"># they won't appear in .parameters() (doesn't apply to buffers), and</span>
        <span class="c1"># won't be converted when e.g. .cuda() is called. You can use</span>
        <span class="c1"># .register_buffer() to register buffers.</span>
        <span class="c1"># nn.Parameters require gradients by default.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">output_features</span><span class="p">,</span> <span class="n">input_features</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">output_features</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># You should always register all possible parameters, but the</span>
            <span class="c1"># optional ones can be None if you want.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span><span class="s1">'bias'</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Not a very smart way to initialize weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># See the autograd section for explanation of what happens here.</span>
        <span class="k">return</span> <span class="n">LinearFunction</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># (Optional)Set the extra information about this module. You can test</span>
        <span class="c1"># it by printing an object of this class.</span>
        <span class="k">return</span> <span class="s1">'in_features=</span><span class="si">{}</span><span class="s1">, out_features=</span><span class="si">{}</span><span class="s1">, bias=</span><span class="si">{}</span><span class="s1">'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="writing-custom-c-extensions">
<h2>Writing custom C++ extensions<a class="headerlink" href="#writing-custom-c-extensions" title="Permalink to this headline">¶</a></h2>
<p>See this
<a class="reference external" href="https://pytorch.org/tutorials/advanced/cpp_extension.html">PyTorch tutorial</a>
for a detailed explanation and examples.</p>
<p>Documentations are available at <a class="reference internal" href="https://pytorch.org/docs/stable/cpp_extension.html"><span class="doc">torch.utils.cpp_extension</span></a>.</p>
</div>
<div class="section" id="id1">
<h2>Writing custom C extensions<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>Example available at
<a class="reference external" href="https://github.com/pytorch/extension-ffi">this GitHub repository</a>.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="https://pytorch.org/docs/stable/notes/faq.html" class="btn btn-neutral float-right" title="Frequently Asked Questions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="https://pytorch.org/docs/stable/notes/cuda.html" class="btn btn-neutral" title="CUDA semantics" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr>

  <div role="contentinfo">
    <p>
        © Copyright 2018, Torch Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org/">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'master',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="jquery.js"></script>
      <script type="text/javascript" src="underscore.js"></script>
      <script type="text/javascript" src="doctools.js"></script>
      <script type="text/javascript" src="MathJax.js"></script>

  

  <script type="text/javascript" src="theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>



</body></html>