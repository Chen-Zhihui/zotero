
Linear algebra
From Wikipedia, the free encyclopedia
This is the current revision of this page, as edited by D.Lazard ( talk  | contribs ) at 18:36, 27 August 2018 ( → ‎ Dual map: thanks for fixing my typos. However, using {{math}} instead of two {{mvar}} seems better, as implying {{nowrap}} ) . The present address (URL) is a permanent link to this version.
Revision as of 18:36, 27 August 2018 by D.Lazard ( talk  | contribs ) ( → ‎ Dual map: thanks for fixing my typos. However, using {{math}} instead of two {{mvar}} seems better, as implying {{nowrap}} )
( diff ) ← Previous revision  | Latest revision (diff) | Newer revision → (diff)
Jump to navigation Jump to search
Not to be confused with Elementary algebra .
In the three-dimensional Euclidean space , planes represent solutions of linear equations and their intersections represent the common solutions

Linear algebra is the branch of mathematics concerning linear equations such as

    a 1 x 1 + ⋯ + a n x n = b , {\displaystyle a_{1}x_{1}+\cdots +a_{n}x_{n}=b,} {\displaystyle a_{1}x_{1}+\cdots +a_{n}x_{n}=b,} 

linear functions such as

    ( x 1 , … , x n ) ↦ a 1 x 1 + … + a n x n , {\displaystyle (x_{1},\ldots ,x_{n})\mapsto a_{1}x_{1}+\ldots +a_{n}x_{n},} {\displaystyle (x_{1},\ldots ,x_{n})\mapsto a_{1}x_{1}+\ldots +a_{n}x_{n},} 

and their representations through matrices and vector spaces . [1] [2] [3]

Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry , including for defining basic objects such as lines , planes and rotations . Also, functional analysis may be basically viewed as the application of linear algebra to spaces of functions. Linear algebra is also used in most sciences and engineering areas, because it allows modeling many natural phenomena, and efficiently computing with such models. For nonlinear systems , which cannot be modeled with linear algebra, linear algebra is often used as a first-order approximation.
Contents

    1 History
    2 Vector spaces
        2.1 Linear maps
        2.2 Subspaces, span, and basis
    3 Matrices
    4 Linear systems
    5 Endomorphisms and square matrices
        5.1 Determinant
        5.2 Eigenvalues and eigenvectors
    6 Duality
        6.1 Dual map
        6.2 Inner-product spaces
    7 Some main useful theorems
    8 Applications
        8.1 Least-squares best-fit line
        8.2 Fourier series expansion
        8.3 Quantum mechanics
    9 Geometric introduction
    10 Generalizations and related topics
    11 See also
    12 Notes
    13 Further reading
        13.1 History
        13.2 Introductory textbooks
        13.3 Advanced textbooks
        13.4 Study guides and outlines
    14 External links
        14.1 Online Resources
        14.2 Online books

History [ edit ]

Systems of linear equations arose in Europe with the introduction in 1637 by René Descartes of coordinates in geometry . In fact, in this new geometry, now called Cartesian geometry , lines and planes are represented by linear equations, and computing their intersections amounts solving systems of linear equations.

The first systematic methods for solving linear systems used determinants , first considered by Leibniz in 1693. In 1750, Gabriel Cramer used them for giving explicit solutions of linear systems, now called Cramer's Rule . Later, Gauss further developed the theory of solving linear systems by using Gaussian elimination , which was initially listed as an advancement in geodesy . [4]

The study of matrix algebra first emerged in England in the mid-1800s. In 1844 Hermann Grassmann published his "Theory of Extension" which included foundational new topics of what is today called linear algebra. In 1848, James Joseph Sylvester introduced the term matrix, which is Latin for "womb". While studying compositions of linear transformations, Arthur Cayley was led to define matrix multiplication and inverses. Crucially, Cayley used a single letter to denote a matrix, thus treating a matrix as an aggregate object. He also realized the connection between matrices and determinants, and wrote "There would be many things to say about this theory of matrices which should, it seems to me, precede the theory of determinants". [4]

In 1882, Hüseyin Tevfik Pasha wrote the book titled "Linear Algebra". [5] [6] The first modern and more precise definition of a vector space was introduced by Peano in 1888; [4] by 1900, a theory of linear transformations of finite-dimensional vector spaces had emerged. Linear algebra took its modern form in the first half of the twentieth century, when many ideas and methods of previous centuries were generalized as abstract algebra . The development of computers led to increased research in efficient algorithms for Gaussian elimination and matrix decompositions, and linear algebra became an essential tool for modelling and simulations. [4]

See also Determinant § History and Gaussian elimination § History .
Vector spaces [ edit ]
Main article: Vector space

Until 19th century, linear algebra was introduced through systems of linear equations and matrices . In modern mathematics, the presentation through vector spaces is generally preferred, since it is more synthetic, more general (not limited to the finite-dimensional case), and conceptually simpler, although more abstract.

A vector space over a field F (often the field of the real numbers ) is a set V equipped with two binary operations satisfying the following axioms . Elements of V are called vectors , and elements of F are called scalars . The first operation, vector addition , takes any two vectors v and w and outputs a third vector v + w . The second operation, scalar multiplication , takes any scalar a and any vector v and outputs a new vector av . The axioms that addition and scalar multiplication must satisfy are the following (in the list below, u , v and w are arbitrary elements of V , and a and b are arbitrary scalars in the field F . [7]
Axiom 	Signification
Associativity of addition 	u + ( v + w ) = ( u + v ) + w
Commutativity of addition 	u + v = v + u
Identity element of addition 	There exists an element 0 in V , called the zero vector (or simply zero ) , such that v + 0 = v for all v in V .
Inverse elements of addition 	For every v in V , there exists an element − v in V , called the additive inverse of v , such that v + (− v ) = 0
Distributivity of scalar multiplication with respect to vector addition   	a ( u + v ) = au + av
Distributivity of scalar multiplication with respect to field addition 	( a + b ) v = av + bv
Compatibility of scalar multiplication with field multiplication 	a ( bv ) = ( ab ) v [nb 1]
Identity element of scalar multiplication 	1 v = v , where 1 denotes the multiplicative identity of F .

The first four axioms mean that V is an abelian group under addition.

Elements of a vector space may have various nature; for example, they can be sequences , functions , polynomials or matrices . Linear algebra is concerned with properties common to all vector spaces.
Linear maps [ edit ]
Main article: Linear map

Linear maps are mappings between vector spaces that preserve the vector-space structure. Given two vector spaces V and W over a field F , a linear map (also called, in some contexts, linear transformation, linear mapping or linear operator) is a map

    T : V → W {\displaystyle T:V\to W} T:V\to W 

that is compatible with addition and scalar multiplication, that is

    T ( u + v ) = T ( u ) + T ( v ) , T ( a v ) = a T ( v ) {\displaystyle T(u+v)=T(u)+T(v),\quad T(av)=aT(v)} T(u+v)=T(u)+T(v),\quad T(av)=aT(v) 

for any vectors u , v in V and scalar a in F .

This implies that for any vectors u , v in V and scalars a , b in F , one has

    T ( a u + b v ) = T ( a u ) + T ( b v ) = a T ( u ) + b T ( v ) {\displaystyle T(au+bv)=T(au)+T(bv)=aT(u)+bT(v)} {\displaystyle T(au+bv)=T(au)+T(bv)=aT(u)+bT(v)} 

When a bijective linear map exists between two vector spaces (that is, every vector from the second space is associated with exactly one in the first), the two spaces are isomorphic . Because an isomorphism preserves linear structure, two isomorphic vector spaces are "essentially the same" from the linear algebra point of view, in the sense that they cannot be distinguished by using vector space properties. An essential question in linear algebra is testing whether a linear map is an isomorphism or not, and, if it is not an isomorphism, finding its range (or image) and the set of elements that are mapped to the zero vector, called the kernel of the map. All these questions can be solved by using Gaussian elimination or some variant of this algorithm .
Subspaces, span, and basis [ edit ]
Main articles: Linear subspace , Linear span , and Basis (linear algebra)

The study of subsets of vector spaces that are themselves vector spaces for the induced operations is fundamental, similarly as for many mathematical structures. These subsets are called linear subspaces . More precisely, a linear subspace of a vector space V over a field F is a subset W of V such that u + v and au are in W , for every u , v in W , and every a in F . (These conditions suffices for implying that W is a vector space.)

For example, the image of a linear map, and the inverse image of 0 by a linear map (called kernel or null space ) are linear subspaces.

Another important way of forming a subspace is to consider linear combinations of a set S of vectors: the set of all sums

    a 1 v 1 + a 2 v 2 + ⋯ + a k v k , {\displaystyle a_{1}v_{1}+a_{2}v_{2}+\cdots +a_{k}v_{k},} a_{1}v_{1}+a_{2}v_{2}+\cdots +a_{k}v_{k}, 

where v 1 , v 2 , ..., v k are in V , and a 1 , a 2 , ..., a k are in F form a linear subspace called the span of S . The span of S is also the intersection of all linear subspaces containing S . In other words, it is the (smallest for the inclusion relation) linear subspace containing S .

A set of vectors is linearly independent if none is in the span of the others. Equivalently, a set S of vector is linearly independent if the only way to express the zero vector as a linear combination of elements of S is to take zero for every coefficient a i . {\displaystyle a_{i}.} a_i.

A set of vectors that spans a vector space is called a spanning set or generating set . If a spanning set S is linearly dependent (that is not linearly independent), then some element w of S is in the span of the other elements of S , and the span would remain the same if one remove w from S . One may continue to remove elements of S until getting a linearly independent spanning set . Such a linearly independent set that spans a vector space V is called a basis of V . The importance of bases lies in the fact that there are together minimal generating sets and maximal independent sets. More precisely, if S is a linearly independent set, and T is a spanning set such that S ⊆ T , {\displaystyle S\subseteq T,} {\displaystyle S\subseteq T,} then there is a basis B such that S ⊆ B ⊆ T . {\displaystyle S\subseteq B\subseteq T.} {\displaystyle S\subseteq B\subseteq T.}

Any two bases of a vector space V have the same cardinality , which is called the dimension of V ; this is the dimension theorem for vector spaces . Moreover, two vector spaces are isomorphic if and only they have the same dimension. [8]

If any basis of V (and therefore every basis) has a finite number of elements, V is a finite-dimensional vector space . If U is a subspace of V , then dim U ≤ dim V . In the case where V is finite-dimensional, the equality of the dimensions implies U = V .

If U 1 and U 2 are subspaces of V , then

    dim ⁡ ( U 1 + U 2 ) = dim ⁡ U 1 + dim ⁡ U 2 − dim ⁡ ( U 1 ∩ U 2 ) , {\displaystyle \dim(U_{1}+U_{2})=\dim U_{1}+\dim U_{2}-\dim(U_{1}\cap U_{2}),} {\displaystyle \dim(U_{1}+U_{2})=\dim U_{1}+\dim U_{2}-\dim(U_{1}\cap U_{2}),} 

where U 1 + U 2 {\displaystyle U_{1}+U_{2}} {\displaystyle U_{1}+U_{2}} denotes the span of U 1 ∪ U 2 . {\displaystyle U_{1}\cup U_{2}.} {\displaystyle U_{1}\cup U_{2}.} [9]
Matrices [ edit ]
Main article: Matrix (mathematics)

Matrices allow explicit manipulation of finite-dimensional vector spaces and linear maps . Their theory is thus an essential part of linear algebra.

Let V be a finite-dimensional vector space over a field F , and ( v 1 , v 2 , ..., v m ) be a basis of V (thus m is the dimension of V ). By definition of a basis, the map

    ( a 1 , … , a m ) ↦ a 1 v 1 + ⋯ a m v m F m → V {\displaystyle {\begin{aligned}(a_{1},\ldots ,a_{m})&\mapsto a_{1}v_{1}+\cdots a_{m}v_{m}\\F^{m}&\to V\end{aligned}}} {\displaystyle {\begin{aligned}(a_{1},\ldots ,a_{m})&\mapsto a_{1}v_{1}+\cdots a_{m}v_{m}\\F^{m}&\to V\end{aligned}}} 

is a bijection from F m , {\displaystyle F^{m},} {\displaystyle F^{m},} the set of the sequences of m elements of F , onto V . This is an isomorphism of vector spaces, if F m {\displaystyle F^{m}} {\displaystyle F^{m}} is equipped of its standard structure of vector space, where vector addition and scalar multiplication are done component by component.

This isomorphism allows representing a vector by its inverse image under this isomorphism, that is by the coordinates vector ( a 1 , … , a m ) {\displaystyle (a_{1},\ldots ,a_{m})} {\displaystyle (a_{1},\ldots ,a_{m})} or by the column matrix

    [ a 1 ⋮ a m ] . {\displaystyle {\begin{bmatrix}a_{1}\\\vdots \\a_{m}\end{bmatrix}}.} {\displaystyle {\begin{bmatrix}a_{1}\\\vdots \\a_{m}\end{bmatrix}}.} 

If W is another finite dimensional vector space (possibly the same), with a basis ( w 1 , … , w n ) , {\displaystyle (w_{1},\ldots ,w_{n}),} {\displaystyle (w_{1},\ldots ,w_{n}),} a linear map f from W to V is well defined by its values on the basis elements, that is ( f ( w 1 ) , … , f ( w n ) ) . {\displaystyle (f(w_{1}),\ldots ,f(w_{n})).} {\displaystyle (f(w_{1}),\ldots ,f(w_{n})).} Thus, f is well represented by the list of the corresponding column matrices. That is, if

    f ( w j ) = a 1 , j v 1 + ⋯ + a m , j v m , {\displaystyle f(w_{j})=a_{1,j}v_{1}+\cdots +a_{m,j}v_{m},} {\displaystyle f(w_{j})=a_{1,j}v_{1}+\cdots +a_{m,j}v_{m},} 

for j = 1, ..., n , then f is represented by the matrix

    [ a 1 , 1 … a 1 , n ⋮ … ⋮ a m , 1 … a m , n ] , {\displaystyle {\begin{bmatrix}a_{1,1}&\ldots &a_{1,n}\\\vdots &\ldots &\vdots \\a_{m,1}&\ldots &a_{m,n}\end{bmatrix}},} {\displaystyle {\begin{bmatrix}a_{1,1}&\ldots &a_{1,n}\\\vdots &\ldots &\vdots \\a_{m,1}&\ldots &a_{m,n}\end{bmatrix}},} 

with m rows and n columns.

Matrix multiplication is defined in such a way that the product of two matrices is the matrix of the composition of the corresponding linear maps, and the product of a matrix and a column matrix is the column matrix representing the result of applying the represented linear map to the represented vector. It follows that the theory of finite-dimensional vector spaces and the theory of matrices are two different languages for expressing exactly the same concepts.

Two matrices that encode the same linear transformation in different bases are called similar . Equivalently, two matrices are similar if one can transform one in the other by elementary row and column operations . For a matrix representing a linear map from W to V , the row operations correspond to change of bases in V and the column operations correspond to change of bases in W . Every matrix is similar to an identity matrix possibly bordered by zero rows and zero columns. In terms of vector space, this means that, for any linear map from W to V , there are bases such a part of the basis of W is mapped bijectively on a part of the basis of V , and that the remaining basis elements of W , if any, are mapped to zero (this is a way of expressing the fundamental theorem of linear algebra ). Gaussian elimination is the basic algorithm for finding these elementary operations, and proving this theorem.
Linear systems [ edit ]
Main article: System of linear equations

Systems of linear equations form a fundamental part of linear algebra. Historically, linear algebra and matrix theory has been developed for solving such systems. In the modern presentation of linear algebra through vector spaces and matrices, many problems may be interpreted in terms of linear systems.

For example, let

    2 x + y − z = 8 − 3 x − y + 2 z = − 11 − 2 x + y + 2 z = − 3 (S) {\displaystyle {\begin{alignedat}{7}2x&&\;+\;&&y&&\;-\;&&z&&\;=\;&&8\\-3x&&\;-\;&&y&&\;+\;&&2z&&\;=\;&&-11\\-2x&&\;+\;&&y&&\;+\;&&2z&&\;=\;&&-3\end{alignedat}}\qquad {\text{(S)}}} {\displaystyle {\begin{alignedat}{7}2x&&\;+\;&&y&&\;-\;&&z&&\;=\;&&8\\-3x&&\;-\;&&y&&\;+\;&&2z&&\;=\;&&-11\\-2x&&\;+\;&&y&&\;+\;&&2z&&\;=\;&&-3\end{alignedat}}\qquad {\text{(S)}}} 

be a linear system.

To such a system, one may associate its matrix

    M [ 2 1 − 1 − 3 − 1 2 − 2 1 2 ] . {\displaystyle M\left[{\begin{array}{rrr}2&1&-1\\-3&-1&2\\-2&1&2\end{array}}\right]{\text{.}}} {\displaystyle M\left[{\begin{array}{rrr}2&1&-1\\-3&-1&2\\-2&1&2\end{array}}\right]{\text{.}}} 

and its right member vector

    v = [ 8 − 11 3 ] . {\displaystyle v={\begin{bmatrix}8\\-11\\3\end{bmatrix}}.} {\displaystyle v={\begin{bmatrix}8\\-11\\3\end{bmatrix}}.} 

Let T be the linear transformation associated to the matrix M . A solution of the system (S) is a vector

    X = [ x y z ] {\displaystyle X={\begin{bmatrix}x\\y\\z\end{bmatrix}}} {\displaystyle X={\begin{bmatrix}x\\y\\z\end{bmatrix}}} 

such that

    T ( X ) = v , {\displaystyle T(X)=v,} {\displaystyle T(X)=v,} 

that is an element of the preimage of v by T .

Let (S') be the associated homogeneous system , where the right-hand sides of the equations are put to zero. The solutions of (S') are exactly the elements of the kernel of T or, equivalently, M .

The Gaussian-elimination consists of performing elementary row operations on the augmented matrix

    M [ 2 3 − 1 8 − 3 − 1 2 − 11 − 2 1 2 − 3 ] {\displaystyle M\left[{\begin{array}{rrr|r}2&3&-1&8\\-3&-1&2&-11\\-2&1&2&-3\end{array}}\right]} {\displaystyle M\left[{\begin{array}{rrr|r}2&3&-1&8\\-3&-1&2&-11\\-2&1&2&-3\end{array}}\right]} 

for putting it in reduced row echelon form . These row operations do not change the set of solutions of the system of equations. In the example, the reduced echelon form is

    M [ 1 0 0 2 0 1 0 3 0 0 1 − 1 ] , {\displaystyle M\left[{\begin{array}{rrr|r}1&0&0&2\\0&1&0&3\\0&0&1&-1\end{array}}\right],} {\displaystyle M\left[{\begin{array}{rrr|r}1&0&0&2\\0&1&0&3\\0&0&1&-1\end{array}}\right],} 

showing that the system (S) has the unique solution

    x = 2 y = 3 z = − 1. {\displaystyle {\begin{aligned}x&=2\\y&=3\\z&=-1.\end{aligned}}} {\displaystyle {\begin{aligned}x&=2\\y&=3\\z&=-1.\end{aligned}}} 

It follows from this matrix interpretation of linear systems that the same methods can be applied for solving linear systems and for many operations on matrices and linear transformations, which include the computation of the ranks , kernels , matrix inverses .
Endomorphisms and square matrices [ edit ]
Main article: Square matrix

A linear endomorphism is a linear map that maps a vector space V to itself. If V has a basis of n elements, such an endomorphism is represented by a square matrix of size n .

With respect to general linear maps, linear endomorphisms and square matrices have some specific properties that make their study an important part of linear algebra, which is used in many parts of mathematics, including geometric transformations , coordinate changes , quadratic forms , and many other part of mathematics.
Determinant [ edit ]
Main article: Determinant

The determinant of a square matrix is a polynomial function of the entries of the matrix, such that the matrix is invertible if and only if the determinant is not zero. This results from the fact that the determinant of a product of matrices is the product of the determinants, and thus that a matrix is invertible if and only if its determinant is invertible.

Cramer's rule is a closed-form expression , in terms of determinants, of the solution of a system of n linear equations in n unknowns . Cramer's rule is useful for reasoning about the solution, but, except for n = 2 or 3 , it is rarely used for computing a solution, since Gaussian elimination is a faster algorithm.

The determinant of an endomorphism is the determinant of the matrix representing the endomorphism in terms of some ordered basis. This definition makes sense, since this determinant is independent of the choice of the basis.
Eigenvalues and eigenvectors [ edit ]
Main article: Eigenvalues and eigenvectors

If f is a linear endomorphism of a vector space V over a field F , an eigenvector of f is a nonzero vector v of V such that f ( v ) = av for some scalar a in F . This scalar a is an eigenvalue of f .

If the dimension of V is finite, and a basis has been chosen, f and v may be represented, respectively, by a square matrix M and a column matrix and z ; the equation defining eigenvectors and eigenvalues becomes

    M z = a z . {\displaystyle Mz=az.} {\displaystyle Mz=az.} 

Using the identity matrix I , whose all entries are zero, except those of the main diagonal, which are equal to one, this may be rewritten

    ( M − a I ) z = 0. {\displaystyle (M-aI)z=0.} {\displaystyle (M-aI)z=0.} 

As z is supposed to be nonzero, this means that M – aI is a singular matrix , and thus that its determinant det ( M − a I ) {\displaystyle \det(M-aI)} {\displaystyle \det(M-aI)} equals zero. The eigenvalues are thus the roots of the polynomial

    det ( x I − M ) . {\displaystyle \det(xI-M).} {\displaystyle \det(xI-M).} 

If V is of dimension n , this is a monic polynomial of degree n , called the characteristic polynomial of the matrix (or of the endomorphism), and there are, at most, n eigenvalues.

If a basis exists that consists only of eigenvectors, the matrix of f on this basis has a very simple structure: it is a diagonal matrix such that the entries on the main diagonal are eigenvalues, and the other entries are zero. In this case, the endomorphism and the matrix are said diagonalizable . More generally, an endomorphism and a matrix are also said diagonalizable, if they become diagonalizable after extending the field of scalars. In this extended sense, if the characteristic polynomial is square-free , then the matrix is diagonalizable.

A symmetric matrix is always diagonalizable. There are non-diagonizable matrices, the simplest being

    [ 0 1 0 0 ] {\displaystyle {\begin{bmatrix}0&1\\0&0\end{bmatrix}}} {\begin{bmatrix}0&1\\0&0\end{bmatrix}} 

(it cannot be diagonalizable since its square is the zero matrix , and the square of a nonzero diagonal matrix is never zero).

When an endomorphism is not diagonalizable, there are bases on which it has a simple form, although not as simple as the diagonal form. The Frobenius normal form does not need of extending the field of scalars and makes the characteristic polynomial immediately readable on the matrix. The Jordan normal form requires to extend the field of scalar for containing all eigenvalues, and differs from the diagonal form only by some entries that are just above the main diagonal and are equal to 1.
Duality [ edit ]
Main article: Dual space

A linear form is a linear map from a vector space V over a field F to the field of scalars F , viewed as a vector space over itself. Equipped by pointwise addition and multiplication by a scalar, the linear forms form a vector space, called the dual space of V , and usually denoted V ∗ . {\displaystyle V^{*}.} {\displaystyle V^{*}.}

If v 1 , … , v n {\displaystyle v_{1},\ldots ,v_{n}} {\displaystyle v_{1},\ldots ,v_{n}} is a basis of V (this implies that V is finite-dimensional), then one can define, for i = 1, ..., n , a linear map v i ∗ {\displaystyle v_{i}^{*}} {\displaystyle v_{i}^{*}} such that v i ∗ ( e i ) = 1 {\displaystyle v_{i}^{*}(e_{i})=1} {\displaystyle v_{i}^{*}(e_{i})=1} and v i ∗ ( e j ) = 0 {\displaystyle v_{i}^{*}(e_{j})=0} {\displaystyle v_{i}^{*}(e_{j})=0} if j ≠ i . These linear maps form a basis of V ∗ , {\displaystyle V^{*},} V^{*}, called the dual basis of v 1 , … , v n . {\displaystyle v_{1},\ldots ,v_{n}.} {\displaystyle v_{1},\ldots ,v_{n}.} (If V is not finite-dimensional, the v i ∗ {\displaystyle v_{i}^{*}} v_{i}^{*} may be defined similarly; they are linearly independent, but do not form a basis.)

For v in V , the map

    f → f ( v ) {\displaystyle f\to f(v)} {\displaystyle f\to f(v)} 

is a linear form on V ∗ . {\displaystyle V^{*}.} {\displaystyle V^{*}.} This defines the canonical linear map from V into V ∗ ∗ , {\displaystyle V^{**},} {\displaystyle V^{**},} , the dual of V ∗ , {\displaystyle V^{*},} V^{*}, called the bidual of V . This canonical map is an isomorphism if V is finite-dimensional, and this allows identifying V with its bidual. (In the infinite dimensional case, the canonical map is injective, but not surjective.)

There is thus a complete symmetry between a finite-dimensional vector space and its dual. This motivates the frequent use, in this context, of the bra–ket notation

    ⟨ f , x ⟩ {\displaystyle \langle f,x\rangle } {\displaystyle \langle f,x\rangle } 

for denoting f   ( x ) .
Dual map [ edit ]
Main article: Transpose of a linear map

Let

    f : V → W {\displaystyle f:V\to W} f:V\to W 

be a linear map. For every linear form h on W , the composite function f ∘ h is a linear form on V . This defines a linear map

    f ∗ : W ∗ → V ∗ {\displaystyle f^{*}:W^{*}\to V^{*}} {\displaystyle f^{*}:W^{*}\to V^{*}} 

between the dual spaces, which is called the dual or the transpose of f .

If V and W are finite dimensional, and M is the matrix of f in terms of some ordered bases, then the matrix of f ∗ {\displaystyle f^{*}} f^{*} over the dual bases is the transpose M T {\displaystyle M^{\mathsf {T}}} {\displaystyle M^{\mathsf {T}}} of M , obtained by exchanging rows and columns.

If elements of vector spaces and their duals are represented by column vectors, this duality may be expressed in bra–ket notation by

    ⟨ h T , M v ⟩ = ⟨ h T M , v ⟩ . {\displaystyle \langle h^{\mathsf {T}},Mv\rangle =\langle h^{\mathsf {T}}M,v\rangle .} {\displaystyle \langle h^{\mathsf {T}},Mv\rangle =\langle h^{\mathsf {T}}M,v\rangle .} 

For highlighting this symmetry, the two members of this equality are sometimes written

    ⟨ h T ∣ M ∣ v ⟩ . {\displaystyle \langle h^{\mathsf {T}}\mid M\mid v\rangle .} {\displaystyle \langle h^{\mathsf {T}}\mid M\mid v\rangle .} 

Inner-product spaces [ edit ]
	
This section may require cleanup to meet Wikipedia's quality standards . The specific problem is: Need for a more encyclopedic style, which is homogeneous with the style of preceding sections. Also, some details do not belong to this general article but to more specialized ones. Also, inner product spaces should appear as a special instance of the more general concept of bilinear form . Finally, complex conjugation should appear in a specific section on linear algebra over the complexes. Please help improve this section if you can. (August 2018) ( Learn how and when to remove this template message )
Main article: Inner product space

Besides these basic concepts, linear algebra also studies vector spaces with additional structure, such as an inner product . The inner product is an example of a bilinear form , and it gives the vector space a geometric structure by allowing for the definition of length and angles. Formally, an inner product is a map

    ⟨ ⋅ , ⋅ ⟩ : V × V → F {\displaystyle \langle \cdot ,\cdot \rangle :V\times V\rightarrow F} \langle \cdot ,\cdot \rangle :V\times V\rightarrow F 

that satisfies the following three axioms for all vectors u , v , w in V and all scalars a in F : [10] [11]

    Conjugate symmetry:

        ⟨ u , v ⟩ = ⟨ v , u ⟩ ¯ . {\displaystyle \langle u,v\rangle ={\overline {\langle v,u\rangle }}.} \langle u,v\rangle ={\overline {\langle v,u\rangle }}. 

Note that in R , it is symmetric.

    Linearity in the first argument:

        ⟨ a u , v ⟩ = a ⟨ u , v ⟩ . {\displaystyle \langle au,v\rangle =a\langle u,v\rangle .} \langle au,v\rangle =a\langle u,v\rangle . 
        ⟨ u + v , w ⟩ = ⟨ u , w ⟩ + ⟨ v , w ⟩ . {\displaystyle \langle u+v,w\rangle =\langle u,w\rangle +\langle v,w\rangle .} \langle u+v,w\rangle =\langle u,w\rangle +\langle v,w\rangle . 

    Positive-definiteness :

        ⟨ v , v ⟩ ≥ 0 {\displaystyle \langle v,v\rangle \geq 0} \langle v,v\rangle \geq 0 with equality only for v = 0.

We can define the length of a vector v in V by

    ‖ v ‖ 2 = ⟨ v , v ⟩ , {\displaystyle \|v\|^{2}=\langle v,v\rangle ,} \|v\|^{2}=\langle v,v\rangle , 

and we can prove the Cauchy–Schwarz inequality :

    | ⟨ u , v ⟩ | ≤ ‖ u ‖ ⋅ ‖ v ‖ . {\displaystyle |\langle u,v\rangle |\leq \|u\|\cdot \|v\|.} |\langle u,v\rangle |\leq \|u\|\cdot \|v\|. 

In particular, the quantity

    | ⟨ u , v ⟩ | ‖ u ‖ ⋅ ‖ v ‖ ≤ 1 , {\displaystyle {\frac {|\langle u,v\rangle |}{\|u\|\cdot \|v\|}}\leq 1,} {\frac {|\langle u,v\rangle |}{\|u\|\cdot \|v\|}}\leq 1, 

and so we can call this quantity the cosine of the angle between the two vectors.

Two vectors are orthogonal if ⟨ u , v ⟩ = 0 {\displaystyle \langle u,v\rangle =0} \langle u,v\rangle =0 . An orthonormal basis is a basis where all basis vectors have length 1 and are orthogonal to each other. Given any finite-dimensional vector space, an orthonormal basis could be found by the Gram–Schmidt procedure. Orthonormal bases are particularly easy to deal with, since if v = a 1 v 1 + ... + a n v n , then a i = ⟨ v , v i ⟩ {\displaystyle a_{i}=\langle v,v_{i}\rangle } a_{i}=\langle v,v_{i}\rangle .

The inner product facilitates the construction of many useful concepts. For instance, given a transform T , we can define its Hermitian conjugate T* as the linear transform satisfying

    ⟨ T u , v ⟩ = ⟨ u , T ∗ v ⟩ . {\displaystyle \langle Tu,v\rangle =\langle u,T^{*}v\rangle .} \langle Tu,v\rangle =\langle u,T^{*}v\rangle . 

If T satisfies TT* = T*T , we call T normal . It turns out that normal matrices are precisely the matrices that have an orthonormal system of eigenvectors that span V .
Some main useful theorems [ edit ]

    A matrix is invertible, or non-singular, if and only if the linear map represented by the matrix is an isomorphism .
    Any vector space over a field F of dimension n is isomorphic to F n as a vector space over F .
    Corollary: Any two vector spaces over F of the same finite dimension are isomorphic to each other.
    A linear map is an isomorphism if and only if the determinant is nonzero.

Applications [ edit ]

Because of the ubiquity of vector spaces, linear algebra is used in many fields of mathematics, natural sciences, computer science, and social science. Below are just some examples of applications of linear algebra.
Least-squares best-fit line [ edit ]

The least squares method is used to determine the best-fit line for a set of data. [12] This line will minimize the sum of the squares of the residuals.
Fourier series expansion [ edit ]

Fourier series are a representation of a function f : [−π, π] → R as a trigonometric series:

    f ( x ) = a 0 2 + ∑ n = 1 ∞ [ a n cos ⁡ ( n x ) + b n sin ⁡ ( n x ) ] . {\displaystyle f(x)={\frac {a_{0}}{2}}+\sum _{n=1}^{\infty }\,[a_{n}\cos(nx)+b_{n}\sin(nx)].} f(x)={\frac {a_{0}}{2}}+\sum _{n=1}^{\infty }\,[a_{n}\cos(nx)+b_{n}\sin(nx)]. 

This series expansion is extremely useful in solving partial differential equations . In this article, we will not be concerned with convergence issues; all Lipschitz-continuous functions have a converging Fourier series expansion, and some discontinuous functions have a Fourier series that converges to the function value at most points.

The space of all functions that can be represented by a Fourier series form a vector space (technically speaking, we call functions that have the same Fourier series expansion the "same" function, since two different discontinuous functions might have the same Fourier series). Moreover, this space is also an inner product space with the inner product

    ⟨ f , g ⟩ = 1 π ∫ − π π f ( x ) g ( x ) d x . {\displaystyle \langle f,g\rangle ={\frac {1}{\pi }}\int _{-\pi }^{\pi }f(x)g(x)\,dx.} \langle f,g\rangle ={\frac {1}{\pi }}\int _{-\pi }^{\pi }f(x)g(x)\,dx. 

The functions g n ( x ) = sin( nx ) for n > 0 and h n ( x ) = cos( nx ) for n ≥ 0 are an orthonormal basis for the space of Fourier-expandable functions. We can thus use the tools of linear algebra to find the expansion of any function in this space in terms of these basis functions. For instance, to find the coefficient a k , we take the inner product with h k :

    ⟨ f , h k ⟩ = a 0 2 ⟨ h 0 , h k ⟩ + ∑ n = 1 ∞ [ a n ⟨ h n , h k ⟩ + b n ⟨   g n , h k ⟩ ] , {\displaystyle \langle f,h_{k}\rangle ={\frac {a_{0}}{2}}\langle h_{0},h_{k}\rangle +\sum _{n=1}^{\infty }\,[a_{n}\langle h_{n},h_{k}\rangle +b_{n}\langle \ g_{n},h_{k}\rangle ],} \langle f,h_{k}\rangle ={\frac {a_{0}}{2}}\langle h_{0},h_{k}\rangle +\sum _{n=1}^{\infty }\,[a_{n}\langle h_{n},h_{k}\rangle +b_{n}\langle \ g_{n},h_{k}\rangle ], 

and by orthonormality, ⟨ f , h k ⟩ = a k {\displaystyle \langle f,h_{k}\rangle =a_{k}} \langle f,h_{k}\rangle =a_{k} ; that is,

    a k = 1 π ∫ − π π f ( x ) cos ⁡ ( k x ) d x . {\displaystyle a_{k}={\frac {1}{\pi }}\int _{-\pi }^{\pi }f(x)\cos(kx)\,dx.} a_{k}={\frac {1}{\pi }}\int _{-\pi }^{\pi }f(x)\cos(kx)\,dx. 

Quantum mechanics [ edit ]

Quantum mechanics is highly inspired by notions in linear algebra. In quantum mechanics , the physical state of a particle is represented by a vector, and observables (such as momentum , energy , and angular momentum ) are represented by linear operators on the underlying vector space. More concretely, the wave function of a particle describes its physical state and lies in the vector space L 2 (the functions φ: R 3 → C such that ∫ − ∞ ∞ ∫ − ∞ ∞ ∫ − ∞ ∞ | ϕ | 2 d x d y d z {\displaystyle \int _{-\infty }^{\infty }\int _{-\infty }^{\infty }\int _{-\infty }^{\infty }|\phi |^{2}dxdydz} \int _{-\infty }^{\infty }\int _{-\infty }^{\infty }\int _{-\infty }^{\infty }|\phi |^{2}dxdydz is finite), and it evolves according to the Schrödinger equation . Energy is represented as the operator H = − ℏ 2 2 m ∇ 2 + V ( x , y , z ) {\displaystyle H=-{\frac {\hbar ^{2}}{2m}}\nabla ^{2}+V(x,y,z)} H=-{\frac {\hbar ^{2}}{2m}}\nabla ^{2}+V(x,y,z) , where V is the potential energy . H is also known as the Hamiltonian operator . The eigenvalues of H represent the possible energies that can be observed. Given a particle in some state φ, we can expand φ into a linear combination of eigenstates of H . The component of φ in each eigenstate determines the probability of measuring the corresponding eigenvalue, and the measurement forces the particle to assume that eigenstate (wave function collapse).
Geometric introduction [ edit ]
	
This section may require cleanup to meet Wikipedia's quality standards . The specific problem is: This section uses nonstandard notation, repeats things that appear earlier in the article, and says almost nothing about what should be its main subject, namely the relationship between linear algebra and geometry Please help improve this section if you can. (August 2018) ( Learn how and when to remove this template message )

Many of the principles and techniques of linear algebra can be seen in the geometry of lines in a real two-dimensional plane E . When formulated using vectors and matrices the geometry of points and lines in the plane can be extended to the geometry of points and hyperplanes in high-dimensional spaces.

Point coordinates in the plane E are ordered pairs of real numbers, ( x , y ), and a line is defined as the set of points ( x , y ) that satisfy the linear equation [13]

    λ : a x + b y + c = 0 , {\displaystyle \lambda :ax+by+c=0,} \lambda :ax+by+c=0, 

where a , b and c are not all zero. Then,

    λ : [ a b c ] [ x y 1 ] = 0 , {\displaystyle \lambda :{\begin{bmatrix}a&b&c\end{bmatrix}}{\begin{bmatrix}x\\y\\1\end{bmatrix}}=0,} {\displaystyle \lambda :{\begin{bmatrix}a&b&c\end{bmatrix}}{\begin{bmatrix}x\\y\\1\end{bmatrix}}=0,} 

or

    A x = 0 , {\displaystyle A\mathbf {x} =0,} A\mathbf {x} =0, 

where x = ( x , y , 1) is the 3 × 1 set of homogeneous coordinates associated with the point ( x , y ). [14]

Homogeneous coordinates identify the plane E with the z = 1 plane in three-dimensional space. The x−y coordinates in E are obtained from homogeneous coordinates y = ( y 1 , y 2 , y 3 ) by dividing by the third component (if it is nonzero) to obtain y = ( y 1 / y 3 , y 2 / y 3 , 1).

The linear equation, λ, has the important property, that if x 1 and x 2 are homogeneous coordinates of points on the line, then the point α x 1 + β x 2 is also on the line, for any real α and β .

Now consider the equations of the two lines λ 1 and λ 2 ,

    λ 1 : a 1 x + b 1 y + c 1 = 0 , λ 2 : a 2 x + b 2 y + c 2 = 0 , {\displaystyle \lambda _{1}:a_{1}x+b_{1}y+c_{1}=0,\quad \lambda _{2}:a_{2}x+b_{2}y+c_{2}=0,} \lambda _{1}:a_{1}x+b_{1}y+c_{1}=0,\quad \lambda _{2}:a_{2}x+b_{2}y+c_{2}=0, 

which forms a system of linear equations . The intersection of these two lines is defined by x = ( x , y , 1) that satisfy the matrix equation,

    λ 1 , 2 : [ a 1 b 1 c 1 a 2 b 2 c 2 ] [ x y 1 ] = [ 0 0 ] , {\displaystyle \lambda _{1,2}:{\begin{bmatrix}a_{1}&b_{1}&c_{1}\\a_{2}&b_{2}&c_{2}\end{bmatrix}}{\begin{bmatrix}x\\y\\1\end{bmatrix}}={\begin{bmatrix}0\\0\end{bmatrix}},} {\displaystyle \lambda _{1,2}:{\begin{bmatrix}a_{1}&b_{1}&c_{1}\\a_{2}&b_{2}&c_{2}\end{bmatrix}}{\begin{bmatrix}x\\y\\1\end{bmatrix}}={\begin{bmatrix}0\\0\end{bmatrix}},} 

or using homogeneous coordinates,

    B x = 0. {\displaystyle B\mathbf {x} =0.} B\mathbf {x} =0. 

The point of intersection of these two lines is the unique non-zero solution of these equations. In homogeneous coordinates, the solutions are multiples of the following solution: [14]

    x 1 = | b 1 c 1 b 2 c 2 | , x 2 = − | a 1 c 1 a 2 c 2 | , x 3 = | a 1 b 1 a 2 b 2 | {\displaystyle x_{1}={\begin{vmatrix}b_{1}&c_{1}\\b_{2}&c_{2}\end{vmatrix}},x_{2}=-{\begin{vmatrix}a_{1}&c_{1}\\a_{2}&c_{2}\end{vmatrix}},x_{3}={\begin{vmatrix}a_{1}&b_{1}\\a_{2}&b_{2}\end{vmatrix}}} x_{1}={\begin{vmatrix}b_{1}&c_{1}\\b_{2}&c_{2}\end{vmatrix}},x_{2}=-{\begin{vmatrix}a_{1}&c_{1}\\a_{2}&c_{2}\end{vmatrix}},x_{3}={\begin{vmatrix}a_{1}&b_{1}\\a_{2}&b_{2}\end{vmatrix}} 

if the rows of B are linearly independent (i.e., λ 1 and λ 2 represent distinct lines). Divide through by x 3 to get Cramer's rule for the solution of a set of two linear equations in two unknowns. [15] Notice that this yields a point in the z = 1 plane only when the 2 × 2 submatrix associated with x 3 has a non-zero determinant.

It is interesting to consider the case of three lines, λ 1 , λ 2 and λ 3 , which yield the matrix equation,

    λ 1 , 2 , 3 : [ a 1 b 1 c 1 a 2 b 2 c 2 a 3 b 3 c 3 ] [ x y 1 ] = [ 0 0 0 ] . {\displaystyle \lambda _{1,2,3}:{\begin{bmatrix}a_{1}&b_{1}&c_{1}\\a_{2}&b_{2}&c_{2}\\a_{3}&b_{3}&c_{3}\end{bmatrix}}{\begin{bmatrix}x\\y\\1\end{bmatrix}}={\begin{bmatrix}0\\0\\0\end{bmatrix}}.} {\displaystyle \lambda _{1,2,3}:{\begin{bmatrix}a_{1}&b_{1}&c_{1}\\a_{2}&b_{2}&c_{2}\\a_{3}&b_{3}&c_{3}\end{bmatrix}}{\begin{bmatrix}x\\y\\1\end{bmatrix}}={\begin{bmatrix}0\\0\\0\end{bmatrix}}.} 

which in homogeneous form yields,

    C x = 0. {\displaystyle C\mathbf {x} =0.} C\mathbf {x} =0. 

Clearly, this equation has the solution x = (0,0,0), which is not a point on the z = 1 plane E . For a solution to exist in the plane E , the coefficient matrix C must have rank 2, which means its determinant must be zero. Another way to say this is that the columns of the matrix must be linearly dependent.
Generalizations and related topics [ edit ]

Since linear algebra is a successful theory, its methods have been developed and generalized in other parts of mathematics. In module theory, one replaces the field of scalars by a ring. The concepts of linear independence, span, basis, and dimension (which is called rank in module theory) still make sense. Nevertheless, many theorems from linear algebra become false in module theory. For instance, not all modules have a basis (those that do are called free modules ), the rank of a free module is not necessarily unique, not every linearly independent subset of a module can be extended to form a basis, and not every subset of a module that spans the space contains a basis.

In multilinear algebra , one considers multivariable linear transformations, that is, mappings that are linear in each of a number of different variables. This line of inquiry naturally leads to the idea of the dual space , the vector space V ∗ consisting of linear maps f : V → F where F is the field of scalars. Multilinear maps T : V n → F can be described via tensor products of elements of V ∗ .

If, in addition to vector addition and scalar multiplication, there is a bilinear vector product V × V → V , the vector space is called an algebra ; for instance, associative algebras are algebras with an associate vector product (like the algebra of square matrices, or the algebra of polynomials).

Functional analysis mixes the methods of linear algebra with those of mathematical analysis and studies various function spaces, such as L p spaces .

Representation theory studies the actions of algebraic objects on vector spaces by representing these objects as matrices. It is interested in all the ways that this is possible, and it does so by finding subspaces invariant under all transformations of the algebra. The concept of eigenvalues and eigenvectors is especially important.

Algebraic geometry considers the solutions of systems of polynomial equations .

There are several related topics in the field of computer programming that utilize much of the techniques and theorems linear algebra encompasses and refers to.
See also [ edit ]

    Linear equation over a ring
    Fundamental matrix in computer vision
    Linear regression , a statistical estimation method
    List of linear algebra topics
    Numerical linear algebra
    Linear programming
    Transformation matrix

Notes [ edit ]

    Jump up ^ Banerjee, Sudipto; Roy, Anindya (2014), Linear Algebra and Matrix Analysis for Statistics , Texts in Statistical Science (1st ed.), Chapman and Hall/CRC, ISBN   978-1420095388  
    Jump up ^ Strang, Gilbert (July 19, 2005), Linear Algebra and Its Applications (4th ed.), Brooks Cole, ISBN   978-0-03-010567-8  
    Jump up ^ Weisstein, Eric. "Linear Algebra" . From MathWorld--A Wolfram Web Resource . Wolfram . Retrieved 16 April 2012 .  
    ^ Jump up to: a b c d Vitulli, Marie . "A Brief History of Linear Algebra and Matrix Theory" . Department of Mathematics . University of Oregon. Archived from the original on 2012-09-10 . Retrieved 2014-07-08 .  
    Jump up ^ http://www.journals.istanbul.edu.tr/tr/index.php/oba/article/download/9103/8452
    Jump up ^ Hussein Tevfik (21 April 1882). "Linear Algebra" . A.H. Boyajian . Retrieved 21 April 2018 – via Internet Archive.  
    Jump up ^ Roman  2005 , ch. 1, p. 27
    Jump up ^ Axler (2004), p. 55
    Jump up ^ Axler (2204), p. 33
    Jump up ^ P. K. Jain, Khalil Ahmad (1995). "5.1 Definitions and basic properties of inner product spaces and Hilbert spaces". Functional analysis (2nd ed.). New Age International. p. 203. ISBN   81-224-0801-X .  
    Jump up ^ Eduard Prugovec̆ki (1981). "Definition 2.1". Quantum mechanics in Hilbert space (2nd ed.). Academic Press. pp. 18 ff . ISBN   0-12-566060-X .  
    Jump up ^ Miller, Steven. "The Method of Least Squares" (PDF) . Brown University . Retrieved 1 May 2013 .  
    Jump up ^ Strang, Gilbert (July 19, 2005), Linear Algebra and Its Applications (4th ed.), Brooks Cole, ISBN   978-0-03-010567-8   ,
    ^ Jump up to: a b J. G. Semple and G. T. Kneebone, Algebraic Projective Geometry, Clarendon Press, London, 1952.
    Jump up ^ E. D. Nering, Linear Algebra and Matrix Theory, John-Wiley, New York, NY, 1963

    Jump up ^ This axiom is not asserting the associativity of an operation, since there are two operations in question, scalar multiplication: bv ; and field multiplication: ab .

Further reading [ edit ]
History [ edit ]

    Fearnley-Sander, Desmond, "Hermann Grassmann and the Creation of Linear Algebra", American Mathematical Monthly 86 (1979), pp. 809–817.
    Grassmann, Hermann (1844), Die lineale Ausdehnungslehre ein neuer Zweig der Mathematik: dargestellt und durch Anwendungen auf die übrigen Zweige der Mathematik, wie auch auf die Statik, Mechanik, die Lehre vom Magnetismus und die Krystallonomie erläutert , Leipzig: O. Wigand  

Introductory textbooks [ edit ]

    Banerjee, Sudipto; Roy, Anindya (2014), Linear Algebra and Matrix Analysis for Statistics , Texts in Statistical Science (1st ed.), Chapman and Hall/CRC, ISBN   978-1420095388  
    Strang, Gilbert (May 2016), Introduction to Linear Algebra (5th ed.), Wellesley-Cambridge Press, ISBN   978-09802327-7-6  
    Murty, Katta G. (2014) Computational and Algorithmic Linear Algebra and n-Dimensional Geometry , World Scientific Publishing, ISBN   978-981-4366-62-5 . Chapter 1: Systems of Simultaneous Linear Equations
    Bretscher, Otto (June 28, 2004), Linear Algebra with Applications (3rd ed.), Prentice Hall, ISBN   978-0-13-145334-0  
    Farin, Gerald; Hansford, Dianne (December 15, 2004), Practical Linear Algebra: A Geometry Toolbox , AK Peters, ISBN   978-1-56881-234-2  
    Hefferon, Jim (2008), Linear Algebra  
    Anton, Howard (2005), Elementary Linear Algebra (Applications Version) (9th ed.), Wiley International  
    Lay, David C. (August 22, 2005), Linear Algebra and Its Applications (3rd ed.), Addison Wesley, ISBN   978-0-321-28713-7  
    Kolman, Bernard; Hill, David R. (May 3, 2007), Elementary Linear Algebra with Applications (9th ed.), Prentice Hall, ISBN   978-0-13-229654-0  
    Leon, Steven J. (2006), Linear Algebra With Applications (7th ed.), Pearson Prentice Hall, ISBN   978-0-13-185785-8  
    Poole, David (2010), Linear Algebra: A Modern Introduction (3rd ed.), Cengage – Brooks/Cole, ISBN   978-0-538-73545-2  
    Ricardo, Henry (2010), A Modern Introduction To Linear Algebra (1st ed.), CRC Press, ISBN   978-1-4398-0040-9  
    Sadun, Lorenzo (2008), Applied Linear Algebra: the decoupling principle (2nd ed.), AMS, ISBN   978-0-8218-4441-0  

Advanced textbooks [ edit ]

    Axler, Sheldon (February 26, 2004), Linear Algebra Done Right (2nd ed.), Springer, ISBN   978-0-387-98258-8  
    Bhatia, Rajendra (November 15, 1996), Matrix Analysis , Graduate Texts in Mathematics , Springer, ISBN   978-0-387-94846-1  
    Demmel, James W. (August 1, 1997), Applied Numerical Linear Algebra , SIAM, ISBN   978-0-89871-389-3  
    Dym, Harry (2007), Linear Algebra in Action , AMS, ISBN   978-0-8218-3813-6  
    Gantmacher, Felix R. (2005), Applications of the Theory of Matrices , Dover Publications, ISBN   978-0-486-44554-0  
    Gantmacher, Felix R. (1990), Matrix Theory Vol. 1 (2nd ed.), American Mathematical Society, ISBN   978-0-8218-1376-8  
    Gantmacher, Felix R. (2000), Matrix Theory Vol. 2 (2nd ed.), American Mathematical Society, ISBN   978-0-8218-2664-5  
    Gelfand, Israel M. (1989), Lectures on Linear Algebra , Dover Publications, ISBN   978-0-486-66082-0  
    Glazman, I. M.; Ljubic, Ju. I. (2006), Finite-Dimensional Linear Analysis , Dover Publications, ISBN   978-0-486-45332-3  
    Golan, Johnathan S. (January 2007), The Linear Algebra a Beginning Graduate Student Ought to Know (2nd ed.), Springer, ISBN   978-1-4020-5494-5  
    Golan, Johnathan S. (August 1995), Foundations of Linear Algebra , Kluwer, ISBN   0-7923-3614-3  
    Golub, Gene H.; Van Loan, Charles F. (October 15, 1996), Matrix Computations , Johns Hopkins Studies in Mathematical Sciences (3rd ed.), The Johns Hopkins University Press, ISBN   978-0-8018-5414-9  
    Greub, Werner H. (October 16, 1981), Linear Algebra , Graduate Texts in Mathematics (4th ed.), Springer, ISBN   978-0-8018-5414-9  
    Hoffman, Kenneth; Kunze, Ray (1971), Linear algebra (2nd ed.), Englewood Cliffs, N.J.: Prentice-Hall, Inc., MR   0276251  
    Halmos, Paul R. (August 20, 1993), Finite-Dimensional Vector Spaces , Undergraduate Texts in Mathematics , Springer, ISBN   978-0-387-90093-3  
    Friedberg, Stephen H.; Insel, Arnold J.; Spence, Lawrence E. (November 11, 2002), Linear Algebra (4th ed.), Prentice Hall, ISBN   978-0-13-008451-4  
    Horn, Roger A.; Johnson, Charles R. (February 23, 1990), Matrix Analysis , Cambridge University Press, ISBN   978-0-521-38632-6  
    Horn, Roger A.; Johnson, Charles R. (June 24, 1994), Topics in Matrix Analysis , Cambridge University Press, ISBN   978-0-521-46713-1  
    Lang, Serge (March 9, 2004), Linear Algebra , Undergraduate Texts in Mathematics (3rd ed.), Springer, ISBN   978-0-387-96412-6  
    Marcus, Marvin; Minc, Henryk (2010), A Survey of Matrix Theory and Matrix Inequalities , Dover Publications, ISBN   978-0-486-67102-4  
    Meyer, Carl D. (February 15, 2001), Matrix Analysis and Applied Linear Algebra , Society for Industrial and Applied Mathematics (SIAM), ISBN   978-0-89871-454-8 , archived from the original on October 31, 2009  
    Mirsky, L. (1990), An Introduction to Linear Algebra , Dover Publications, ISBN   978-0-486-66434-7  
    Roman, Steven (March 22, 2005), Advanced Linear Algebra , Graduate Texts in Mathematics (2nd ed.), Springer, ISBN   978-0-387-24766-3  
    Shafarevich, I. R. ; Remizov, A. O (2012), Linear Algebra and Geometry , Springer , ISBN   978-3-642-30993-9  
    Shilov, Georgi E. (June 1, 1977), Linear algebra , Dover Publications, ISBN   978-0-486-63518-7  
    Shores, Thomas S. (December 6, 2006), Applied Linear Algebra and Matrix Analysis , Undergraduate Texts in Mathematics, Springer, ISBN   978-0-387-33194-2  
    Smith, Larry (May 28, 1998), Linear Algebra , Undergraduate Texts in Mathematics, Springer, ISBN   978-0-387-98455-1  
    Trefethen, Lloyd N.; Bau, David (1997), Numerical Linear Algebra , SIAM, ISBN   978-0-898-71361-9  

Study guides and outlines [ edit ]

    Leduc, Steven A. (May 1, 1996), Linear Algebra (Cliffs Quick Review) , Cliffs Notes, ISBN   978-0-8220-5331-6  
    Lipschutz, Seymour; Lipson, Marc (December 6, 2000), Schaum's Outline of Linear Algebra (3rd ed.), McGraw-Hill, ISBN   978-0-07-136200-9  
    Lipschutz, Seymour (January 1, 1989), 3,000 Solved Problems in Linear Algebra , McGraw–Hill, ISBN   978-0-07-038023-3  
    McMahon, David (October 28, 2005), Linear Algebra Demystified , McGraw–Hill Professional, ISBN   978-0-07-146579-3  
    Zhang, Fuzhen (April 7, 2009), Linear Algebra: Challenging Problems for Students , The Johns Hopkins University Press, ISBN   978-0-8018-9125-0  

External links [ edit ]
Online Resources [ edit ]
	Wikibooks has a book on the topic of: Linear Algebra
	Wikimedia Commons has media related to Linear algebra .

MIT Linear Algebra Video Lectures , a serie of 34 recorded lectures by professor Gilbert Strang (Spring 2010)

    International Linear Algebra Society
    Hazewinkel, Michiel , ed. (2001) [1994], "Linear algebra" , Encyclopedia of Mathematics , Springer Science+Business Media B.V. / Kluwer Academic Publishers, ISBN   978-1-55608-010-4  
    Linear Algebra on MathWorld .
    Matrix and Linear Algebra Terms on Earliest Known Uses of Some of the Words of Mathematics
    Earliest Uses of Symbols for Matrices and Vectors on Earliest Uses of Various Mathematical Symbols
    Essence of linear algebra , a video presentation of the basics of linear algebra, with emphasis on the relationship between the geometric, the matrix and the abstract points of view

Online books [ edit ]

    Beezer, Rob, A First Course in Linear Algebra
    Connell, Edwin H., Elements of Abstract and Linear Algebra
    Hefferon, Jim, Linear Algebra
    Matthews, Keith, Elementary Linear Algebra
    Sharipov, Ruslan, Course of linear algebra and multidimensional geometry
    Treil, Sergei, Linear Algebra Done Wrong

    v
    t
    e

Linear algebra
Basic concepts 	

    Scalar
    Vector
    Vector space
    Scalar multiplication
    Vector projection
    Linear span
    Linear map
    Linear projection
    Linear independence
    Linear combination
    Basis
    Column space
    Row space
    Orthogonality
    Kernel
    Eigenvalues and eigenvectors
    Outer product
    Inner product space
    Dot product
    Transpose
    Gram–Schmidt process
    Linear equations

	
Three dimensional Euclidean space
Vector algebra 	

    Cross product
    Triple product
    Seven-dimensional cross product

Multilinear algebra 	

    Geometric algebra
    Exterior algebra
    Bivector
    Multivector

Matrices 	

    Block
    Decomposition
    Invertible
    Minor
    Multiplication
    Rank
    Transformation
    Cramer's rule
    Gaussian elimination

Algebraic constructions 	

    Dual
    Direct sum
    Function space
    Quotient
    Subspace
    Tensor product

Numerical 	

    Floating point
    Matrix Laboratory
    Numerical stability
    Basic Linear Algebra Subprograms (BLAS)
    Sparse matrix
    Comparison of linear algebra libraries
    Comparison of numerical analysis software

    Category Category
    List-Class article Outline
    Portal Portal
    Wikibooks page Wikibook
    Wikiversity page Wikiversity

    v
    t
    e

Areas of mathematics

    outline
    topic lists

Branches 	
Arithmetic 	

    Number theory

Algebra 	

    Elementary
    Linear
    Multilinear
    Abstract
    Combinatorics
    Group theory
    Representation theory
    Lie theory

Calculus 	

    Analysis
    Differential equations  / Dynamical systems
    Numerical analysis
    Optimization
    Functional analysis

Geometry 	

    Discrete
    Algebraic
    Analytic
    Differential
    Finite
    Topology
    Trigonometry

Foundations 	

    Philosophy of mathematics
    Mathematical logic
    Set theory
    Category theory

Applied 	

    Mathematical physics
    Probability
    Mathematical statistics
    Statistics
    Game theory
    Information theory
    Computer science
    Computation
    Control theory

Others 	

    History of mathematics
    Recreational mathematics
    Mathematics and art
    Mathematics education

    Order theory
    Graph theory

Divisions 	

    Pure
    Applied
    Discrete
    Computational

    Category Category
    Portal Portal
    Commons page Commons
    WikiProject WikiProject

Authority control Edit this at Wikidata 	

    BNF : cb11937509n (data)
    GND : 4035811-2
    LCCN : sh85003441
    NDL : 00570681

Retrieved from " https://en.wikipedia.org/w/index.php?title=Linear_algebra&oldid=856819078 "
Categories :

    Linear algebra
    Numerical analysis

Hidden categories:

    CS1: Julian–Gregorian uncertainty
    Articles needing cleanup from August 2018
    All pages needing cleanup
    Cleanup tagged articles with a reason field from August 2018
    Wikipedia pages needing cleanup from August 2018
    Wikipedia articles with BNF identifiers
    Wikipedia articles with GND identifiers
    Wikipedia articles with LCCN identifiers
    Wikipedia articles with NDL identifiers

Navigation menu
Personal tools

    Not logged in
    Talk
    Contributions
    Create account
    Log in

Namespaces

    Article
    Talk

Variants

Views

    Read
    Edit
    View history

More

Search
Navigation

    Main page
    Contents
    Featured content
    Current events
    Random article
    Donate to Wikipedia
    Wikipedia store

Interaction

    Help
    About Wikipedia
    Community portal
    Recent changes
    Contact page

Tools

    What links here
    Related changes
    Upload file
    Special pages
    Permanent link
    Page information
    Wikidata item
    Cite this page

Print/export

    Create a book
    Download as PDF
    Printable version

In other projects

    Wikimedia Commons
    Wikibooks
    Wikiversity

Languages

    Afrikaans
    Alemannisch
    العربية
    Aragonés
    Asturianu
    Azərbaycanca
    বাংলা
    Башҡортса
    Беларуская
    Беларуская (тарашкевіца)‎
    Български
    Bosanski
    Català
    Чӑвашла
    Čeština
    Cymraeg
    Dansk
    Deutsch
    Eesti
    Ελληνικά
    Español
    Esperanto
    Euskara
    فارسی
    Français
    Galego
    贛語
    한국어
    Հայերեն
    हिन्दी
    Hrvatski
    Bahasa Indonesia
    Íslenska
    Italiano
    עברית
    ქართული
    Қазақша
    Лезги
    Latina
    Latviešu
    Lietuvių
    Magyar
    Македонски
    മലയാളം
    Bahasa Melayu
    Nederlands
    日本語
    Norsk
    Norsk nynorsk
    Occitan
    Oʻzbekcha/ўзбекча
    Patois
    Piemontèis
    Polski
    Português
    Română
    Русский
    Scots
    Shqip
    Sicilianu
    Simple English
    Slovenčina
    Slovenščina
    کوردی
    Српски / srpski
    Srpskohrvatski / српскохрватски
    Suomi
    Svenska
    Tagalog
    தமிழ்
    ไทย
    Тоҷикӣ
    Türkçe
    Українська
    اردو
    Tiếng Việt
    Winaray
    吴语
    ייִדיש
    Yorùbá
    粵語
    中文
    Lingua Franca Nova

Edit links

    This page was last edited on 27 August 2018, at 18:36  (UTC) .
    Text is available under the Creative Commons Attribution-ShareAlike License ; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Developers
    Cookie statement
    Mobile view

    Wikimedia Foundation
    Powered by MediaWiki

