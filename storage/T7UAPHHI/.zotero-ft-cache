
Loading [MathJax]/extensions/MathMenu.js
Cornell University
Cornell University Library
We gratefully acknowledge support from
the Simons Foundation
and member institutions
arXiv.org > cs > arXiv:1412.6980
( Help | Advanced search )
Full-text links:
Download:

    PDF
    Other formats

( license )
Current browse context:
cs.LG
< prev  |  next >
new  | recent  | 1412
Change to browse by:
cs
References & Citations

    NASA ADS

DBLP - CS Bibliography
listing | bibtex
Diederik P. Kingma
Jimmy Ba
Bookmark
( what is this? )
CiteULike logo BibSonomy logo Mendeley logo del.icio.us logo Digg logo Reddit logo ScienceWISE logo
Computer Science > Learning
Title: Adam: A Method for Stochastic Optimization
Authors: Diederik P. Kingma , Jimmy Ba
(Submitted on 22 Dec 2014 ( v1 ), last revised 30 Jan 2017 (this version, v9))

    Abstract: We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm. 

Comments: 	Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015
Subjects: 	Learning (cs.LG)
Cite as: 	arXiv:1412.6980 [cs.LG]
  	(or arXiv:1412.6980v9 [cs.LG] for this version)
Submission history
From: Diederik P Kingma M.Sc. [ view email ]
[v1] Mon, 22 Dec 2014 13:54:29 GMT (280kb,D)
[v2] Sat, 17 Jan 2015 20:26:06 GMT (283kb,D)
[v3] Fri, 27 Feb 2015 21:04:48 GMT (289kb,D)
[v4] Tue, 3 Mar 2015 17:51:27 GMT (289kb,D)
[v5] Thu, 23 Apr 2015 16:46:07 GMT (289kb,D)
[v6] Tue, 23 Jun 2015 19:57:17 GMT (958kb,D)
[v7] Mon, 20 Jul 2015 09:43:23 GMT (519kb,D)
[v8] Thu, 23 Jul 2015 20:27:47 GMT (526kb,D)
[v9] Mon, 30 Jan 2017 01:27:54 GMT (490kb,D)
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

Link back to: arXiv , form interface , contact .
Twitter
