
Matrix decomposition
From Wikipedia, the free encyclopedia
This is the current revision of this page, as edited by 128.206.20.25 ( talk ) at 18:47, 28 August 2018 ( → ‎ Scale-invariant decompositions: Clarified difference between matrices of same name in two different decompositions. ) . The present address (URL) is a permanent link to this version.
Revision as of 18:47, 28 August 2018 by 128.206.20.25 ( talk ) ( → ‎ Scale-invariant decompositions: Clarified difference between matrices of same name in two different decompositions. )
( diff ) ← Previous revision  | Latest revision (diff) | Newer revision → (diff)
Jump to navigation Jump to search

In the mathematical discipline of linear algebra , a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.
Contents

    1 Example
    2 Decompositions related to solving systems of linear equations
        2.1 LU decomposition
        2.2 LU reduction
        2.3 Block LU decomposition
        2.4 Rank factorization
        2.5 Cholesky decomposition
        2.6 QR decomposition
        2.7 RRQR factorization
        2.8 Interpolative decomposition
    3 Decompositions based on eigenvalues and related concepts
        3.1 Eigendecomposition
        3.2 Jordan decomposition
        3.3 Schur decomposition
        3.4 Real Schur decomposition
        3.5 QZ decomposition
        3.6 Takagi's factorization
        3.7 Singular value decomposition
        3.8 Scale-invariant decompositions
    4 Other decompositions
        4.1 Polar decomposition
        4.2 Algebraic polar decomposition
        4.3 Mostow's decomposition
        4.4 Sinkhorn normal form
        4.5 Sectoral decomposition [10]
        4.6 Williamson's normal form [12]
    5 Generalizations
    6 See also
    7 Notes
    8 References
    9 External links

Example [ edit ]

In numerical analysis , different decompositions are used to implement efficient matrix algorithms .

For instance, when solving a system of linear equations A x = b {\displaystyle Ax=b} Ax=b , the matrix A can be decomposed via the LU decomposition . The LU decomposition factorizes a matrix into a lower triangular matrix L and an upper triangular matrix U . The systems L ( U x ) = b {\displaystyle L(Ux)=b} L(Ux)=b and U x = L − 1 b {\displaystyle Ux=L^{-1}b} Ux=L^{{-1}}b require fewer additions and multiplications to solve, compared with the original system A x = b {\displaystyle Ax=b} Ax=b , though one might require significantly more digits in inexact arithmetic such as floating point .

Similarly, the QR decomposition expresses A as QR with Q an orthogonal matrix and R an upper triangular matrix. The system Q ( Rx ) = b is solved by Rx = Q T b = c , and the system Rx = c is solved by ' back substitution '. The number of additions and multiplications required is about twice that of using the LU solver, but no more digits are required in inexact arithmetic because the QR decomposition is numerically stable .
Decompositions related to solving systems of linear equations [ edit ]
LU decomposition [ edit ]
Main article: LU decomposition

    Applicable to: square matrix A
    Decomposition: A = L U {\displaystyle A=LU} A=LU , where L is lower triangular and U is upper triangular
    Related: the LDU decomposition is A = L D U {\displaystyle A=LDU} A=LDU , where L is lower triangular with ones on the diagonal, U is upper triangular with ones on the diagonal, and D is a diagonal matrix .
    Related: the LUP decomposition is A = L U P {\displaystyle A=LUP} A=LUP , where L is lower triangular , U is upper triangular , and P is a permutation matrix .
    Existence: An LUP decomposition exists for any square matrix A . When P is an identity matrix , the LUP decomposition reduces to the LU decomposition. If the LU decomposition exists, then the LDU decomposition exists. [1]
    Comments: The LUP and LU decompositions are useful in solving an n -by- n system of linear equations A x = b {\displaystyle Ax=b} Ax=b . These decompositions summarize the process of Gaussian elimination in matrix form. Matrix P represents any row interchanges carried out in the process of Gaussian elimination. If Gaussian elimination produces the row echelon form without requiring any row interchanges, then P  =  I , so an LU decomposition exists.

LU reduction [ edit ]
Main article: LU reduction
Block LU decomposition [ edit ]
Main article: Block LU decomposition
Rank factorization [ edit ]
Main article: Rank factorization

    Applicable to: m -by- n matrix A of rank r
    Decomposition: A = C F {\displaystyle A=CF} A=CF where C is an m -by- r full column rank matrix and F is an r -by- n full row rank matrix
    Comment: The rank factorization can be used to compute the Moore–Penrose pseudoinverse of A , [2] which one can apply to obtain all solutions of the linear system A x = b {\displaystyle Ax=b} Ax=b .

Cholesky decomposition [ edit ]
Main article: Cholesky decomposition

    Applicable to: square , hermitian , positive definite matrix A
    Decomposition: A = U ∗ U {\displaystyle A=U^{*}U} {\displaystyle A=U^{*}U} , where U is upper triangular with real positive diagonal entries
    Comment: if the matrix A is Hermitian and positive semi-definite, then it has a decomposition of the form A = U ∗ U {\displaystyle A=U^{*}U} {\displaystyle A=U^{*}U} if the diagonal entries of U {\displaystyle U} U are allowed to be zero
    Uniqueness: for positive definite matrices Cholesky decomposition is unique. However, it is not unique in the positive semi-definite case.
    Comment: if A is real and symmetric, U {\displaystyle U} U has all real elements
    Comment: An alternative is the LDL decomposition , which can avoid extracting square roots.

QR decomposition [ edit ]
Main article: QR decomposition

    Applicable to: m -by- n matrix A
    Decomposition: A = Q R {\displaystyle A=QR} A=QR where Q is a unitary matrix of size m -by- m , and R is an upper triangular matrix of size m -by- n
    Uniqueness: In general it is not unique, but if A {\displaystyle A} A is of full rank , then there exists a single R {\displaystyle R} R that has all positive diagonal elements. If A {\displaystyle A} A is square, also Q {\displaystyle Q} Q is unique.
    Comment: The QR decomposition provides an alternative way of solving the system of equations A x = b {\displaystyle Ax=b} Ax=b without inverting the matrix A . The fact that Q is orthogonal means that Q T Q = I {\displaystyle Q^{T}Q=I} Q^{T}Q=I , so that A x = b {\displaystyle Ax=b} Ax=b is equivalent to R x = Q T b {\displaystyle Rx=Q^{T}b} Rx=Q^{T}b , which is easier to solve since R is triangular .

RRQR factorization [ edit ]
Main article: RRQR factorization
Interpolative decomposition [ edit ]
Main article: Interpolative decomposition
Decompositions based on eigenvalues and related concepts [ edit ]
Eigendecomposition [ edit ]
Main article: Eigendecomposition (matrix)

    Also called spectral decomposition .
    Applicable to: square matrix A with linearly independent eigenvectors (not necessarily distinct eigenvalues).
    Decomposition: A = V D V − 1 {\displaystyle A=VDV^{-1}} A=VDV^{{-1}} , where D is a diagonal matrix formed from the eigenvalues of A , and the columns of V are the corresponding eigenvectors of A .
    Existence: An n -by- n matrix A always has n (complex) eigenvalues, which can be ordered (in more than one way) to form an n -by- n diagonal matrix D and a corresponding matrix of nonzero columns V that satisfies the eigenvalue equation A V = V D {\displaystyle AV=VD} AV=VD . V {\displaystyle V} V is invertible if and only if the n eigenvectors are linearly independent (i.e., each eigenvalue has geometric multiplicity equal to its algebraic multiplicity ). A sufficient (but not necessary) condition for this to happen is that all the eigenvalues are different (in this case geometric and algebraic multiplicity are equal to 1)
    Comment: One can always normalize the eigenvectors to have length one (see the definition of the eigenvalue equation)
    Comment: Every normal matrix A (i.e., matrix for which A A ∗ = A ∗ A {\displaystyle AA^{*}=A^{*}A} {\displaystyle AA^{*}=A^{*}A} , where A ∗ {\displaystyle A^{*}} A^{*} is a conjugate transpose ) can be eigendecomposed. For a normal matrix A (and only for a normal matrix), the eigenvectors can also be made orthonormal ( V V ∗ = I {\displaystyle VV^{*}=I} {\displaystyle VV^{*}=I} ) and the eigendecomposition reads as A = V D V ∗ {\displaystyle A=VDV^{*}} {\displaystyle A=VDV^{*}} . In particular all unitary , Hermitian , or skew-Hermitian (in the real-valued case, all orthogonal , symmetric , or skew-symmetric , respectively) matrices are normal and therefore possess this property.
    Comment: For any real symmetric matrix A , the eigendecomposition always exists and can be written as A = V D V T {\displaystyle A=VDV^{T}} A=VDV^{T} , where both D and V are real-valued.
    Comment: The eigendecomposition is useful for understanding the solution of a system of linear ordinary differential equations or linear difference equations. For example, the difference equation x t + 1 = A x t {\displaystyle x_{t+1}=Ax_{t}} x_{{t+1}}=Ax_{t} starting from the initial condition x 0 = c {\displaystyle x_{0}=c} x_{0}=c is solved by x t = A t c {\displaystyle x_{t}=A^{t}c} x_{t}=A^{t}c , which is equivalent to x t = V D t V − 1 c {\displaystyle x_{t}=VD^{t}V^{-1}c} x_{t}=VD^{t}V^{{-1}}c , where V and D are the matrices formed from the eigenvectors and eigenvalues of A . Since D is diagonal, raising it to power D t {\displaystyle D^{t}} D^{t} , just involves raising each element on the diagonal to the power t . This is much easier to do and understand than raising A to power t , since A is usually not diagonal.

Jordan decomposition [ edit ]

The Jordan normal form and the Jordan–Chevalley decomposition

    Applicable to: square matrix A
    Comment: the Jordan normal form generalizes the eigendecomposition to cases where there are repeated eigenvalues and cannot be diagonalized, the Jordan–Chevalley decomposition does this without choosing a basis.

Schur decomposition [ edit ]
Main article: Schur decomposition

    Applicable to: square matrix A
    Decomposition (complex version): A = U T U ∗ {\displaystyle A=UTU^{*}} {\displaystyle A=UTU^{*}} , where U is a unitary matrix , U ∗ {\displaystyle U^{*}} U^{*} is the conjugate transpose of U , and T is an upper triangular matrix called the complex Schur form which has the eigenvalues of A along its diagonal.
    Comment: if A is a normal matrix , then T is diagonal and the Schur decomposition coincides with the spectral decomposition.

Real Schur decomposition [ edit ]

    Applicable to: square matrix A
    Decomposition: This is a version of Schur decomposition where V {\displaystyle V} V and S {\displaystyle S} S only contain real numbers. One can always write A = V S V T {\displaystyle A=VSV^{T}} A=VSV^{T} where V is a real orthogonal matrix , V T {\displaystyle V^{T}} V^{T} is the transpose of V , and S is a block upper triangular matrix called the real Schur form . The blocks on the diagonal of S are of size 1×1 (in which case they represent real eigenvalues) or 2×2 (in which case they are derived from complex conjugate eigenvalue pairs).

QZ decomposition [ edit ]
Main article: QZ decomposition

    Also called: generalized Schur decomposition
    Applicable to: square matrices A and B
    Comment: there are two versions of this decomposition: complex and real.
    Decomposition (complex version): A = Q S Z H {\displaystyle A=QSZ^{H}} A=QSZ^{H} and B = Q T Z H {\displaystyle B=QTZ^{H}} B=QTZ^{H} where Q and Z are unitary matrices , the H superscript represents conjugate transpose , and S and T are upper triangular matrices.
    Comment: in the complex QZ decomposition, the ratios of the diagonal elements of S to the corresponding diagonal elements of T , λ i = S i i / T i i {\displaystyle \lambda _{i}=S_{ii}/T_{ii}} \lambda _{i}=S_{{ii}}/T_{{ii}} , are the generalized eigenvalues that solve the generalized eigenvalue problem A v = λ B v {\displaystyle Av=\lambda Bv} Av=\lambda Bv (where λ {\displaystyle \lambda } \lambda is an unknown scalar and v is an unknown nonzero vector).
    Decomposition (real version): A = Q S Z T {\displaystyle A=QSZ^{T}} A=QSZ^{T} and B = Q T Z T {\displaystyle B=QTZ^{T}} B=QTZ^{T} where A , B , Q , Z , S , and T are matrices containing real numbers only. In this case Q and Z are orthogonal matrices , the T superscript represents transposition , and S and T are block upper triangular matrices. The blocks on the diagonal of S and T are of size 1×1 or 2×2.

Takagi's factorization [ edit ]

    Applicable to: square, complex, symmetric matrix A .
    Decomposition: A = V D V T {\displaystyle A=VDV^{T}} A=VDV^{T} , where D is a real nonnegative diagonal matrix , and V is unitary . V T {\displaystyle V^{T}} V^{T} denotes the matrix transpose of V .
    Comment: The diagonal elements of D are the nonnegative square roots of the eigenvalues of A A H {\displaystyle AA^{H}} AA^{H} .
    Comment: V may be complex even if A is real.
    Comment: This is not a special case of the eigendecomposition (see above), which uses V − 1 {\displaystyle V^{-1}} V^{{-1}} instead of V T {\displaystyle V^{T}} V^{T} .

Singular value decomposition [ edit ]
Main article: Singular value decomposition

    Applicable to: m -by- n matrix A .
    Decomposition: A = U D V ∗ {\displaystyle A=UDV^{*}} {\displaystyle A=UDV^{*}} , where D is a nonnegative diagonal matrix , and U and V are unitary matrices , and V ∗ {\displaystyle V^{*}} V^{*} is the conjugate transpose of V (or simply the transpose , if V contains real numbers only).
    Comment: The diagonal elements of D are called the singular values of A .
    Comment: Like the eigendecomposition above, the singular value decomposition involves finding basis directions along which matrix multiplication is equivalent to scalar multiplication, but it has greater generality since the matrix under consideration need not be square.
    Uniqueness: the singular values of A {\displaystyle A} A are always uniquely determined. U {\displaystyle U} U and V {\displaystyle V} V need not to be unique in general.

Scale-invariant decompositions [ edit ]

Refers to variants of existing matrix decompositions, such as the SVD, that are invariant with respect to diagonal scaling.

    Applicable to: m -by- n matrix A .
    Unit-Scale-Invariant Singular-Value Decomposition: A = D U S V ∗ E {\displaystyle A=DUSV^{*}E} {\displaystyle A=DUSV^{*}E} , where S is a unique nonnegative diagonal matrix of scale-invariant singular values, U and V are unitary matrices , V ∗ {\displaystyle V^{*}} V^{*} is the conjugate transpose of V , and positive diagonal matrices D and E .
    Comment: Is analogous to the SVD except that the diagonal elements of S are invariant with respect to left and/or right multiplication of A by arbitrary nonsingular diagonal matrices, as opposed to the standard SVD for which the singular values are invariant with respect to left and/or right multiplication of A by arbitrary unitary matrices.
    Comment: Is an alternative to the standard SVD when invariance is required with respect to diagonal rather than unitary transformations of A .
    Uniqueness: The scale-invariant singular values of A {\displaystyle A} A (given by the diagonal elements of S ) are always uniquely determined. Diagonal matrices D and E , and unitary U and V , are not necessarily unique in general.
    Comment: U and V matrices are not the same as those from the SVD.

Analogous scale-invariant decompositions can be derived from other matrix decompositions, e.g., to obtain scale-invariant eigenvalues. [3] [4]
Other decompositions [ edit ]
Polar decomposition [ edit ]
Main article: Polar decomposition

    Applicable to: any square complex matrix A .
    Decomposition: A = U P {\displaystyle A=UP} A=UP (right polar decomposition) or A = P ′ U {\displaystyle A=P'U} {\displaystyle A=P'U} (left polar decomposition), where U is a unitary matrix and P and P' are positive semidefinite Hermitian matrices .
    Uniqueness: P {\displaystyle P} P is always unique and equal to A ∗ A {\displaystyle {\sqrt {A^{*}A}}} \sqrt{A^*A} (which is always hermitian and positive semidefinite). If A {\displaystyle A} A is invertible, then U {\displaystyle U} U is unique.
    Comment: Since any Hermitian matrix admits a spectral decomposition with a unitary matrix, P {\displaystyle P} P can be written as P = V D V ∗ {\displaystyle P=VDV^{*}} {\displaystyle P=VDV^{*}} . Since P {\displaystyle P} P is positive semidefinite, all elements in D {\displaystyle D} D are non-negative. Since the product of two unitary matrices is unitary, taking W = U V {\displaystyle W=UV} {\displaystyle W=UV} one can write A = U ( V D V ∗ ) = W D V ∗ {\displaystyle A=U(VDV^{*})=WDV^{*}} {\displaystyle A=U(VDV^{*})=WDV^{*}} which is the singular value decomposition. Hence, the existence of the polar decomposition is equivalent to the existence of the singular value decomposition.

Algebraic polar decomposition [ edit ]

    Applicable to: square, complex, non-singular matrix A . [5]
    Decomposition: A = Q S {\displaystyle A=QS} A=QS , where Q is a complex orthogonal matrix and S is complex symmetric matrix.
    Uniqueness: If A T A {\displaystyle A^{T}A} A^{{T}}A has no negative real eigenvalues, then the decomposition is unique. [6]
    Comment: The existence of this decomposition is equivalent to A A T {\displaystyle AA^{T}} AA^{{T}} being similar to A T A {\displaystyle A^{T}A} A^{{T}}A . [7]
    Comment: A variant of this decomposition is A = R C {\displaystyle A=RC} {\displaystyle A=RC} , where R is a real matrix and C is a circular matrix . [6]

Mostow's decomposition [ edit ]
Main article: Mostow decomposition

    Applicable to: square, complex, non-singular matrix A . [8] [9]
    Decomposition: A = U e i M e S {\displaystyle A=Ue^{iM}e^{S}} {\displaystyle A=Ue^{iM}e^{S}} , where U is unitary, M is real anti-symmetric and S is real symmetric.
    Comment: The matrix A can also be decomposed as A = U 2 e S 2 e i M 2 {\displaystyle A=U_{2}e^{S_{2}}e^{iM_{2}}} {\displaystyle A=U_{2}e^{S_{2}}e^{iM_{2}}} , where U 2 is unitary, M 2 is real anti-symmetric and S 2 is real symmetric. [6]

Sinkhorn normal form [ edit ]
Main article: Sinkhorn's theorem

    Applicable to: square real matrix A with strictly positive elements.
    Decomposition: A = D 1 S D 2 {\displaystyle A=D_{1}SD_{2}} A=D_{{1}}SD_{{2}} , where S is doubly stochastic and D 1 and D 2 are real diagonal matrices with strictly positive elements.

Sectoral decomposition [10] [ edit ]

    Applicable to: square, complex matrix A with numerical range contained in the sector S α = { r e i θ ∈ C ∣ r > 0 , | θ | ≤ α < π 2 } {\displaystyle S_{\alpha }=\{re^{i\theta }\in \mathbb {C} \mid r>0,|\theta |\leq \alpha <{\frac {\pi }{2}}\}} {\displaystyle S_{\alpha }=\{re^{i\theta }\in \mathbb {C} \mid r>0,|\theta |\leq \alpha <{\frac {\pi }{2}}\}} .
    Decomposition: A = C Z C H {\displaystyle A=CZC^{H}} A=CZC^{{H}} , where C is an invertible complex matrix and Z = d i a g ( e i θ 1 , … , e i θ n ) {\displaystyle Z=diag(e^{i\theta _{1}},\ldots ,e^{i\theta _{n}})} Z=diag(e^{{i\theta _{1}}},\ldots ,e^{{i\theta _{n}}}) with all | θ j | ≤ α {\displaystyle |\theta _{j}|\leq \alpha } |\theta _{{j}}|\leq \alpha . [10] [11]

Williamson's normal form [12] [ edit ]

    Applicable to: square, positive-definite real matrix A with order 2 n -by-2 n .
    Decomposition: A = S T diag ( D , D ) S {\displaystyle A=S^{T}{\textrm {diag}}(D,D)S} {\displaystyle A=S^{T}{\textrm {diag}}(D,D)S} , where S is a symplectic matrix and D is a nonnegative diagonal matrix.

Generalizations [ edit ]
[icon] 	
This section needs expansion with: examples and additional citations. You can help by adding to it . (December 2014)

There exist analogues of the SVD, QR, LU and Cholesky factorizations for quasimatrices and cmatrices or continuous matrices . [13] A ‘quasimatrix’ is, like a matrix, a rectangular scheme whose elements are indexed, but one discrete index is replaced by a continuous index. Likewise, a ‘cmatrix’, is continuous in both indices. As an example of a cmatrix, one can think of the kernel of an integral operator .

These factorizations are based on early work by Fredholm (1903) , Hilbert (1904) and Schmidt (1907) . For an account, and a translation to English of the seminal papers, see Stewart (2011) .
See also [ edit ]

    Matrix splitting
    Non-negative matrix factorization
    Principal component analysis

Notes [ edit ]

    Jump up ^ Simon & Blume 1994 Chapter 7.
    Jump up ^ Piziak, R.; Odell, P. L. (1 June 1999). "Full Rank Factorization of Matrices". Mathematics Magazine . 72 (3): 193. doi : 10.2307/2690882 .  
    Jump up ^ Uhlmann, J.K. (2018), A Generalized Matrix Inverse that is Consistent with Respect to Diagonal Transformations , SIAM Journal on Matrix Analysis, 239:2, pp. 781–800  
    Jump up ^ Uhlmann, J.K. (2018), A Rank-Preserving Generalized Matrix Inverse for Consistency with Respect to Similarity , IEEE Control Systems Letters, ISSN   2475-1456  
    Jump up ^ Choudhury & Horn 1987 , pp. 219–225
    ^ Jump up to: a b c Bhatia, Rajendra (2013-11-15). "The bipolar decomposition" . Linear Algebra and its Applications . 439 (10): 3031–3037. doi : 10.1016/j.laa.2013.09.006 .  
    Jump up ^ Horn & merino 1995 , pp. 43–92
    Jump up ^ Mostow, G. D. (1955), Some new decomposition theorems for semi-simple groups , Mem. Amer. Math. Soc., 14 , pp. 31–54  
    Jump up ^ Nielsen, Frank; Bhatia, Rajendra (2012). Matrix Information Geometry . Springer. p. 224. doi : 10.1007/978-3-642-30232-9 . ISBN   9783642302329 .  
    ^ Jump up to: a b Zhang, Fuzhen (30 June 2014). "A matrix decomposition and its applications". Linear and Multilinear Algebra : 1–10. doi : 10.1080/03081087.2014.933219 .  
    Jump up ^ Drury, S.W. (November 2013). "Fischer determinantal inequalities and Highamʼs Conjecture". Linear Algebra and its Applications . 439 (10): 3129–3133. doi : 10.1016/j.laa.2013.08.031 .  
    Jump up ^ Idel, Martin; Soto Gaona, Sebastián; Wolf, Michael M. (2017-07-15). "Perturbation bounds for Williamson's symplectic normal form" . Linear Algebra and its Applications . 525 : 45–58. arXiv : 1609.01338  Freely accessible . doi : 10.1016/j.laa.2017.03.013 .  
    Jump up ^ Townsend & Trefethen 2015

References [ edit ]

    Choudhury, Dipa; Horn, Roger A. (April 1987). "A Complex Orthogonal-Symmetric Analog of the Polar Decomposition". SIAM Journal on Algebraic Discrete Methods . 8 (2). doi : 10.1137/0608019 .  
    Fredholm, I. (1903), "Sur une classe d'´equations fonctionnelles", Acta Mathematica (in French), 27 : 365–390, doi : 10.1007/bf02421317  
    Hilbert, D. (1904), "Grundzüge einer allgemeinen Theorie der linearen Integralgleichungen", Nachr. Königl. Ges. Gött (in German), 1904 : 49–91  
    Horn, Roger A.; Merino, Dennis I. (January 1995). "Contragredient equivalence: A canonical form and some applications". Linear Algebra and its Applications . 214 . doi : 10.1016/0024-3795(93)00056-6 .  
    Meyer, C. D. (2000), Matrix Analysis and Applied Linear Algebra , SIAM , ISBN   978-0-89871-454-8  
    Schmidt, E. (1907), "Zur Theorie der linearen und nichtlinearen Integralgleichungen. I Teil. Entwicklung willkürlichen Funktionen nach System vorgeschriebener", Mathematische Annalen (in German), 63 : 433–476, doi : 10.1007/bf01449770  
    Simon, C.; Blume, L. (1994). Mathematics for Economists . Norton. ISBN   0-393-95733-0 .  
    Stewart, G. W. (2011), Fredholm, Hilbert, Schmidt: three fundamental papers on integral equations (PDF) , retrieved 2015-01-06  
    Townsend, A.; Trefethen, L. N. (2015), "Continuous analogues of matrix factorizations", Proc. R. Soc. A , 471 (2173), Bibcode : 2014RSPSA.47140585T , doi : 10.1098/rspa.2014.0585 , PMC   4277194  Freely accessible  

External links [ edit ]

    Online Matrix Calculator
    Wolfram Alpha Matrix Decomposition Computation » LU and QR Decomposition
    Springer Encyclopaedia of Mathematics » Matrix factorization
    GraphLab GraphLab collaborative filtering library, large scale parallel implementation of matrix decomposition methods (in C++) for multicore.

    v
    t
    e

Linear algebra
Basic concepts 	

    Scalar
    Vector
    Vector space
    Scalar multiplication
    Vector projection
    Linear span
    Linear map
    Linear projection
    Linear independence
    Linear combination
    Basis
    Column space
    Row space
    Orthogonality
    Kernel
    Eigenvalues and eigenvectors
    Outer product
    Inner product space
    Dot product
    Transpose
    Gram–Schmidt process
    Linear equations

	
Three dimensional Euclidean space
Vector algebra 	

    Cross product
    Triple product
    Seven-dimensional cross product

Multilinear algebra 	

    Geometric algebra
    Exterior algebra
    Bivector
    Multivector

Matrices 	

    Block
    Decomposition
    Invertible
    Minor
    Multiplication
    Rank
    Transformation
    Cramer's rule
    Gaussian elimination

Algebraic constructions 	

    Dual
    Direct sum
    Function space
    Quotient
    Subspace
    Tensor product

Numerical 	

    Floating point
    Matrix Laboratory
    Numerical stability
    Basic Linear Algebra Subprograms (BLAS)
    Sparse matrix
    Comparison of linear algebra libraries
    Comparison of numerical analysis software

    Category Category
    List-Class article Outline
    Portal Portal
    Wikibooks page Wikibook
    Wikiversity page Wikiversity

Retrieved from " https://en.wikipedia.org/w/index.php?title=Matrix_decomposition&oldid=856974701 "
Categories :

    Matrix theory
    Matrix decompositions

Hidden categories:

    Articles to be expanded from December 2014
    All articles to be expanded
    Articles using small message boxes
    CS1 French-language sources (fr)
    CS1 German-language sources (de)

Navigation menu
Personal tools

    Not logged in
    Talk
    Contributions
    Create account
    Log in

Namespaces

    Article
    Talk

Variants

Views

    Read
    Edit
    View history

More

Search
Navigation

    Main page
    Contents
    Featured content
    Current events
    Random article
    Donate to Wikipedia
    Wikipedia store

Interaction

    Help
    About Wikipedia
    Community portal
    Recent changes
    Contact page

Tools

    What links here
    Related changes
    Upload file
    Special pages
    Permanent link
    Page information
    Wikidata item
    Cite this page

Print/export

    Create a book
    Download as PDF
    Printable version

Languages

    العربية
    Català
    Español
    한국어
    Íslenska
    Italiano
    Nederlands
    日本語
    Polski
    Русский
    Svenska
    Українська
    中文

Edit links

    This page was last edited on 28 August 2018, at 18:47  (UTC) .
    Text is available under the Creative Commons Attribution-ShareAlike License ; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Developers
    Cookie statement
    Mobile view
    Enable previews

    Wikimedia Foundation
    Powered by MediaWiki

