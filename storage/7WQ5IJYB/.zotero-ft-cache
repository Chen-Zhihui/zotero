
Sparse dictionary learning
From Wikipedia, the free encyclopedia
This is the current revision of this page, as edited by Nemo bis ( talk  | contribs ) at 17:43, 16 May 2018 (Added free to read link in citations with OAbot #oabot) . The present address (URL) is a permanent link to this version.
Revision as of 17:43, 16 May 2018 by Nemo bis ( talk  | contribs ) (Added free to read link in citations with OAbot #oabot)
( diff ) ← Previous revision  | Latest revision (diff) | Newer revision → (diff)
Jump to navigation Jump to search
Machine learning and
data mining
Kernel Machine.svg
Problems [show]

    Classification
    Clustering
    Regression
    Anomaly detection
    AutoML
    Association rules
    Reinforcement learning
    Structured prediction
    Feature engineering
    Feature learning
    Online learning
    Semi-supervised learning
    Unsupervised learning
    Learning to rank
    Grammar induction

Supervised learning
( classification  • regression )
[show]

    Decision trees
    Ensembles ( Bagging , Boosting , Random forest )
    k -NN
    Linear regression
    Naive Bayes
    Neural networks
    Logistic regression
    Perceptron
    Relevance vector machine (RVM)
    Support vector machine (SVM)

Clustering [show]

    BIRCH
    CURE
    Hierarchical
    k -means
    Expectation–maximization (EM)

    DBSCAN
    OPTICS
    Mean-shift

Dimensionality reduction [show]

    Factor analysis
    CCA
    ICA
    LDA
    NMF
    PCA
    t-SNE

Structured prediction [show]

    Graphical models ( Bayes net , CRF , HMM )

Anomaly detection [show]

    k -NN
    Local outlier factor

Neural nets [show]

    Autoencoder
    Deep learning
    Multilayer perceptron
    RNN ( LSTM , GRU )
    Restricted Boltzmann machine
    SOM
    Convolutional neural network ( U-Net )

Reinforcement learning [show]

    Q-learning
    SARSA
    Temporal difference (TD)

Theory [show]

    Bias-variance dilemma
    Computational learning theory
    Empirical risk minimization
    Occam learning
    PAC learning
    Statistical learning
    VC theory

Machine-learning venues [show]

    NIPS
    ICML
    ML
    JMLR
    ArXiv:cs.LG

Glossary of artificial intelligence [show]

    Glossary of artificial intelligence

Related articles [show]

    List of datasets for machine-learning research
    Outline of machine learning

    Portal-puzzle.svg Machine learning portal

    v
    t
    e

Sparse dictionary learning is a representation learning method which aims at finding a sparse representation of the input data (also known as sparse coding ) in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms and they compose a dictionary . Atoms in the dictionary are not required to be orthogonal , and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation.

One of the most important applications of sparse dictionary learning is in the field of compressed sensing or signal recovery . In compressed sensing, a high dimensional signal can be recovered with only a few linear measurements provided that the signal is sparse or nearly sparse. Since not all signals satisfy this sparsity condition, it is of great importance to find a sparse representation of that signal such as the wavelet transform or the directional gradient of a rasterized matrix. Once a matrix or a high dimensional vector is transferred to a sparse space, different recovery algorithms like basis pursuit , CoSaMP [1] or fast non-iterative algorithms [2] can be used to recover the signal.

One of the key principles of dictionary learning is that the dictionary has to be inferred from the input data. The emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing one typically wants to represent the input data using as few components as possible. Before this approach the general practice was to use predefined dictionaries (such as fourier or wavelet transforms). However, in certain cases a dictionary that is trained to fit the input data can significantly improve the sparsity, which has applications in data decomposition, compression and analysis and has been used in the fields of image denoising and classification , video and audio processing . Sparsity and overcomplete dictionaries have immense applications in image compression, image fusion and inpainting.
Image Denoising by Dictionary Learning
Contents

    1 Problem statement
        1.1 Properties of the dictionary
    2 Algorithms
        2.1 Method of optimal directions (MOD)
        2.2 K-SVD
        2.3 Stochastic gradient descent
        2.4 Lagrange dual method
        2.5 Parametric training methods
        2.6 Online dictionary learning
    3 Applications
    4 See also
    5 References

Problem statement [ edit ]

Given the input dataset X = [ x 1 , . . . , x K ] , x i ∈ R d {\displaystyle X=[x_{1},...,x_{K}],x_{i}\in \mathbb {R} ^{d}} {\displaystyle X=[x_{1},...,x_{K}],x_{i}\in \mathbb {R} ^{d}} we wish to find a dictionary D ∈ R d × n : D = [ d 1 , . . . , d n ] {\displaystyle \mathbf {D} \in \mathbb {R} ^{d\times n}:D=[d_{1},...,d_{n}]} {\displaystyle \mathbf {D} \in \mathbb {R} ^{d\times n}:D=[d_{1},...,d_{n}]} and a representation R = [ r 1 , . . . , r K ] , r i ∈ R n {\displaystyle R=[r_{1},...,r_{K}],r_{i}\in \mathbb {R} ^{n}} {\displaystyle R=[r_{1},...,r_{K}],r_{i}\in \mathbb {R} ^{n}} such that both ‖ X − D R ‖ F 2 {\displaystyle \|X-\mathbf {D} R\|_{F}^{2}} {\displaystyle \|X-\mathbf {D} R\|_{F}^{2}} is minimized and the representations r i {\displaystyle r_{i}} r_{i} are sparse enough. This can be formulated as the following optimization problem :

argmin D ∈ C , r i ∈ R n ∑ i = 1 K ‖ x i − D r i ‖ 2 2 + λ ‖ r i ‖ 0 {\displaystyle {\underset {\mathbf {D} \in {\mathcal {C}},r_{i}\in \mathbb {R} ^{n}}{\text{argmin}}}\sum _{i=1}^{K}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{0}} {\displaystyle {\underset {\mathbf {D} \in {\mathcal {C}},r_{i}\in \mathbb {R} ^{n}}{\text{argmin}}}\sum _{i=1}^{K}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{0}} , where C ≡ { D ∈ R d × n : ‖ d i ‖ 2 ≤ 1 ∀ i = 1 , . . . , n } {\displaystyle {\mathcal {C}}\equiv \{\mathbb {D} \in \mathbb {R} ^{d\times n}:\|d_{i}\|_{2}\leq 1\,\,\forall i=1,...,n\}} {\displaystyle {\mathcal {C}}\equiv \{\mathbb {D} \in \mathbb {R} ^{d\times n}:\|d_{i}\|_{2}\leq 1\,\,\forall i=1,...,n\}}

C {\displaystyle {\mathcal {C}}} {\mathcal {C}} is required to constrain D {\displaystyle \mathbf {D} } \mathbf{D} so that its atoms would not reach arbitrarily high values allowing for arbitrarily low (but non-zero) values of r i {\displaystyle r_{i}} r_{i} .

The minimization problem above is not convex because of the ℓ 0 -"norm" and solving this problem is NP-hard. [3] In some cases L 1 -norm is known to ensure sparsity [4] and so the above becomes a convex optimization problem with respect to each of the variables D {\displaystyle \mathbf {D} } \mathbf{D} and R {\displaystyle \mathbf {R} } \mathbf {R} when the other one is fixed, but it is not jointly convex in ( D , R ) {\displaystyle (\mathbf {D} ,\mathbf {R} )} {\displaystyle (\mathbf {D} ,\mathbf {R} )} .


Properties of the dictionary [ edit ]

The dictionary D {\displaystyle \mathbf {D} } \mathbf{D} defined above can be "undercomplete" if n < d {\displaystyle n<d} {\displaystyle n<d} or "overcomplete" in case n > d {\displaystyle n>d} {\displaystyle n>d} with the latter being a typical assumption for a sparse dictionary learning problem. The case of a complete dictionary does not provide any improvement from a representational point of view and thus isn't considered.

Undercomplete dictionaries represent the setup in which the actual input data lies in a lower-dimensional space. This case is strongly related to dimensionality reduction and techniques like principal component analysis which require atoms d 1 , . . . , d n {\displaystyle d_{1},...,d_{n}} {\displaystyle d_{1},...,d_{n}} to be orthogonal. The choice of these subspaces is crucial for efficient dimensionality reduction, but it is not trivial. And dimensionality reduction based on dictionary representation can be extended to address specific tasks such as data analysis or classification. However, their main downside is limiting the choice of atoms.

Overcomplete dictionaries, however, do not require the atoms to be orthogonal (they will never be a basis anyway) thus allowing for more flexible dictionaries and richer data representations.

An overcomplete dictionary which allows for sparse representation of signal can be a famous transform matrix (wavelets transform, fourier transform) or it can be formulated so that its elements are changed in such a way that it sparsely represents given signal in a best way. Learned dictionaries are capable to give more sparse solution as compared to predefined transform matrices.
Algorithms [ edit ]

As the optimization problem described above can be solved as a convex problem with respect to either dictionary or sparse coding while the other one of the two is fixed, most of the algorithms are based on the idea of iteratively updating one and then the other.

The problem of finding an optimal sparse coding R {\displaystyle R} R with a given dictionary D {\displaystyle \mathbf {D} } \mathbf{D} is known as sparse approximation (or sometimes just sparse coding problem). There has been developed a number of algorithms to solve it (such as matching pursuit and LASSO ) which are incorporated into the algorithms described below.
Method of optimal directions (MOD) [ edit ]

The method of optimal directions (or MOD) was one of the first methods introduced to tackle the sparse dictionary learning problem. [5] The core idea of it is to solve the minimization problem subject to the limited number of non-zero components of the representation vector:

min D , R { ‖ X − D R ‖ F 2 } s.t. ∀ i ‖ r i ‖ 0 ≤ T {\displaystyle \min _{\mathbf {D} ,R}\{\|X-\mathbf {D} R\|_{F}^{2}\}\,\,{\text{s.t.}}\,\,\forall i\,\,\|r_{i}\|_{0}\leq T} {\displaystyle \min _{\mathbf {D} ,R}\{\|X-\mathbf {D} R\|_{F}^{2}\}\,\,{\text{s.t.}}\,\,\forall i\,\,\|r_{i}\|_{0}\leq T}

Here, F {\displaystyle F} F denotes the Frobenius norm . MOD alternates between getting the sparse coding using a method such as matching pursuit and updating the dictionary by computing the analytical solution of the problem given by D = X R + {\displaystyle \mathbf {D} =XR^{+}} {\displaystyle \mathbf {D} =XR^{+}} where R + {\displaystyle R^{+}} {\displaystyle R^{+}} is a Moore-Penrose pseudoinverse . After this update D {\displaystyle \mathbf {D} } \mathbf{D} is renormalized to fit the constraints and the new sparse coding is obtained again. The process is repeated until convergence (or until a sufficiently small residue).

MOD has proved to be a very efficient method for low-dimensional input data X {\displaystyle X} X requiring just a few iterations to converge. However, due to the high complexity of the matrix-inversion operation, computing the pseudoinverse in high-dimensional cases is in many cases intractable. This shortcoming has inspired the development of other dictionary learning methods.
K-SVD [ edit ]
Main article: K-SVD

K-SVD is an algorithm that performs SVD at its core to update the atoms of the dictionary one by one and basically is a generalization of K-means . It enforces that each element of the input data x i {\displaystyle x_{i}} x_{i} is encoded by a linear combination of not more than T 0 {\displaystyle T_{0}} T_{0} elements in a way identical to the MOD approach:

min D , R { ‖ X − D R ‖ F 2 } s.t. ∀ i ‖ r i ‖ 0 ≤ T 0 {\displaystyle \min _{\mathbf {D} ,R}\{\|X-\mathbf {D} R\|_{F}^{2}\}\,\,{\text{s.t.}}\,\,\forall i\,\,\|r_{i}\|_{0}\leq T_{0}} {\displaystyle \min _{\mathbf {D} ,R}\{\|X-\mathbf {D} R\|_{F}^{2}\}\,\,{\text{s.t.}}\,\,\forall i\,\,\|r_{i}\|_{0}\leq T_{0}}

This algorithm's essence is to first fix the dictionary, find the best possible R {\displaystyle R} R under the above constraint (using Orthogonal Matching Pursuit ) and then iteratively update the atoms of dictionary D {\displaystyle \mathbf {D} } \mathbf{D} in the following manner:

‖ X − D R ‖ F 2 = | X − ∑ i = 1 K d i x T i | F 2 = ‖ E k − d k x T k ‖ F 2 {\displaystyle \|X-\mathbf {D} R\|_{F}^{2}=\left|X-\sum _{i=1}^{K}d_{i}x_{T}^{i}\right|_{F}^{2}=\|E_{k}-d_{k}x_{T}^{k}\|_{F}^{2}} {\displaystyle \|X-\mathbf {D} R\|_{F}^{2}=\left|X-\sum _{i=1}^{K}d_{i}x_{T}^{i}\right|_{F}^{2}=\|E_{k}-d_{k}x_{T}^{k}\|_{F}^{2}}

The next steps of the algorithm include rank-1 approximation of the residual matrix E k {\displaystyle E_{k}} {\displaystyle E_{k}} , updating d k {\displaystyle d_{k}} {\displaystyle d_{k}} and enforcing the sparsity of x k {\displaystyle x_{k}} {\displaystyle x_{k}} after the update. This algorithm is considered to be standard for dictionary learning and is used in a variety of applications. However, it shares weaknesses with MOD being efficient only for signals with relatively low dimensionality and having the possibility for being stuck at local minima.
Stochastic gradient descent [ edit ]
Main article: Stochastic gradient descent

One can also apply a widespread stochastic gradient descent method with iterative projection to solve this problem. [6] [7] The idea of this method is to update the dictionary using the first order stochastic gradient and project it on the constraint set C {\displaystyle {\mathcal {C}}} {\mathcal {C}} . The step that occurs at i-th iteration is described by this expression:

D i = proj C { D i − 1 − δ i ∇ D ∑ i ∈ S ‖ x i − D r i ‖ 2 2 + λ ‖ r i ‖ 1 } {\displaystyle \mathbf {D} _{i}={\text{proj}}_{\mathcal {C}}\left\{\mathbf {D} _{i-1}-\delta _{i}\nabla _{\mathbf {D} }\sum _{i\in S}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{1}\right\}} {\displaystyle \mathbf {D} _{i}={\text{proj}}_{\mathcal {C}}\left\{\mathbf {D} _{i-1}-\delta _{i}\nabla _{\mathbf {D} }\sum _{i\in S}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{1}\right\}} , where S {\displaystyle S} S is a random subset of { 1... K } {\displaystyle \{1...K\}} {\displaystyle \{1...K\}} and δ i {\displaystyle \delta _{i}} \delta _{i} is a gradient step.
Lagrange dual method [ edit ]

An algorithm based on solving a dual Lagrangian problem provides an efficient way to solve for the dictionary having no complications induced by the sparsity function. [8] Consider the following Lagrangian:

L ( D , Λ ) = tr ( ( X − D R ) T ( X − D R ) ) + ∑ j = 1 n λ i ( ∑ i = 1 d D i j 2 − c ) {\displaystyle {\mathcal {L}}(\mathbf {D} ,\Lambda )={\text{tr}}\left((X-\mathbf {D} R)^{T}(X-\mathbf {D} R)\right)+\sum _{j=1}^{n}\lambda _{i}\left({\sum _{i=1}^{d}\mathbf {D} _{ij}^{2}-c}\right)} {\displaystyle {\mathcal {L}}(\mathbf {D} ,\Lambda )={\text{tr}}\left((X-\mathbf {D} R)^{T}(X-\mathbf {D} R)\right)+\sum _{j=1}^{n}\lambda _{i}\left({\sum _{i=1}^{d}\mathbf {D} _{ij}^{2}-c}\right)} , where c {\displaystyle c} c is a constraint on the norm of the atoms and λ i {\displaystyle \lambda _{i}} \lambda _{i} are the so-called dual variables forming the diagonal matrix Λ {\displaystyle \Lambda } \Lambda .

We can then provide an analytical expression for the Lagrange dual after minimization over D {\displaystyle \mathbf {D} } \mathbf{D} :

D ( Λ ) = min D L ( D , Λ ) = tr ( X T X − X R T ( R R T + Λ ) − 1 ( X R T ) T − c Λ ) {\displaystyle {\mathcal {D}}(\Lambda )=\min _{\mathbf {D} }{\mathcal {L}}(\mathbf {D} ,\Lambda )={\text{tr}}(X^{T}X-XR^{T}(RR^{T}+\Lambda )^{-1}(XR^{T})^{T}-c\Lambda )} {\displaystyle {\mathcal {D}}(\Lambda )=\min _{\mathbf {D} }{\mathcal {L}}(\mathbf {D} ,\Lambda )={\text{tr}}(X^{T}X-XR^{T}(RR^{T}+\Lambda )^{-1}(XR^{T})^{T}-c\Lambda )} .

After applying one of the optimization methods to the value of the dual (such as Newton's method or conjugate gradient ) we get the value of D {\displaystyle \mathbf {D} } \mathbf{D} :

D T = ( R R T + Λ ) − 1 ( X R T ) T {\displaystyle \mathbf {D} ^{T}=(RR^{T}+\Lambda )^{-1}(XR^{T})^{T}} {\displaystyle \mathbf {D} ^{T}=(RR^{T}+\Lambda )^{-1}(XR^{T})^{T}}

Solving this problem is less computational hard because the amount of dual variables n {\displaystyle n} n is a lot of times much less than the amount of variables in the primal problem.
Parametric training methods [ edit ]

Parametric training methods are aimed to incorporate the best of both worlds — the realm of analytically constructed dictionaries and the learned ones. [9] This allows to construct more powerful generalized dictionaries that can potentially be applied to the cases of arbitrary-sized signals. Notable approaches include:

    Translation-invariant dictionaries. [10] These dictionaries are composed by the translations of the atoms originating from the dictionary constructed for a finite-size signal patch. This allows the resulting dictionary to provide a representation for the arbitrary-sized signal.
    Multiscale dictionaries. [11] This method focuses on constructing a dictionary that is composed of differently scaled dictionaries to improve sparsity.
    Sparse dictionaries. [12] This method focuses on not only providing a sparse representation but also constructing a sparse dictionary which is enforced by the expression D = B A {\displaystyle \mathbf {D} =\mathbf {B} \mathbf {A} } {\displaystyle \mathbf {D} =\mathbf {B} \mathbf {A} } where B {\displaystyle \mathbf {B} } \mathbf {B} is some pre-defined analytical dictionary with desirable properties such as fast computation and A {\displaystyle \mathbf {A} } \mathbf {A} is a sparse matrix. Such formulation allows to directly combine the fast implementation of analytical dictionaries with the flexibility of sparse approaches.

Online dictionary learning [ edit ]

Many common approaches to sparse dictionary learning rely on the fact that the whole input data X {\displaystyle X} X (or at least a large enough training dataset) is available for the algorithm. However, this might not be the case in the real-world scenario as the size of the input data might be too big to fit it into memory. The other case where this assumption can not be made is when the input data comes in a form of a stream . Such cases lie in the field of study of online learning which essentially suggests iteratively updating the model upon the new data points x {\displaystyle x} x becoming available.

A dictionary can be learned in an online manner the following way: [13]

    For t = 1... T : {\displaystyle t=1...T:} {\displaystyle t=1...T:}
    Draw a new sample x t {\displaystyle x_{t}} x_{t}
    Find a sparse coding using LARS : r t = argmin r ∈ R n ( 1 2 ‖ x t − D t − 1 r ‖ + λ ‖ r ‖ 1 ) {\displaystyle r_{t}={\underset {r\in \mathbb {R} ^{n}}{\text{argmin}}}\left({\frac {1}{2}}\|x_{t}-\mathbf {D} _{t-1}r\|+\lambda \|r\|_{1}\right)} {\displaystyle r_{t}={\underset {r\in \mathbb {R} ^{n}}{\text{argmin}}}\left({\frac {1}{2}}\|x_{t}-\mathbf {D} _{t-1}r\|+\lambda \|r\|_{1}\right)}
    Update dictionary using block-coordinate approach: D t = argmin D ∈ C 1 t ∑ i = 1 t ( 1 2 ‖ x i − D r i ‖ 2 2 + λ ‖ r i ‖ 1 ) {\displaystyle \mathbf {D} _{t}={\underset {\mathbf {D} \in {\mathcal {C}}}{\text{argmin}}}{\frac {1}{t}}\sum _{i=1}^{t}\left({\frac {1}{2}}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{1}\right)} {\displaystyle \mathbf {D} _{t}={\underset {\mathbf {D} \in {\mathcal {C}}}{\text{argmin}}}{\frac {1}{t}}\sum _{i=1}^{t}\left({\frac {1}{2}}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{1}\right)}

This method allows us to gradually update the dictionary as new data becomes available for sparse representation learning and helps drastically reduce the amount of memory needed to store the dataset (which often has a huge size).
Applications [ edit ]

The dictionary learning framework, namely the linear decomposition of an input signal using a few basis elements learned from data itself, has led to state-of-art results in various image and video processing tasks. This technique can be applied to classification problems in a way that if we have built specific dictionaries for each class, the input signal can be classified by finding the dictionary corresponding to the sparsest representation.

It also has properties that are useful for signal denoising since usually one can learn a dictionary to represent the meaningful part of the input signal in a sparse way but the noise in the input will have a much less sparse representation. [14]

Sparse dictionary learning has been successfully applied to various image, video and audio processing tasks as well as to texture synthesis [15] and unsupervised clustering. [16] In evaluations with the Bag-of-Words model, [17] [18] sparse coding was found empirically to outperform other coding approaches on the object category recognition tasks.

Dictionary learning is used to analyse medical signals in detail. Such medical signals include those from electroencephalography (EEG), electrocardiography (ECG), magnetic resonance imaging (MRI), functional MRI (fMRI), and ultrasound computer tomography (USCT), where different assumptions are used to analyze each signal.
See also [ edit ]

    Sparse approximation
    Sparse PCA
    Matrix factorization
    K-SVD
    Neural sparse coding

References [ edit ]

    Jump up ^ Needell, D.; Tropp, J.A. "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples" . Applied and Computational Harmonic Analysis . 26 (3): 301–321. doi : 10.1016/j.acha.2008.07.002 .  
    Jump up ^ Lotfi, M.; Vidyasagar, M." A Fast Non-iterative Algorithm for Compressive Sensing Using Binary Measurement Matrices "
    Jump up ^ A. M. Tillmann, " On the Computational Intractability of Exact and Approximate Dictionary Learning ", IEEE Signal Processing Letters 22(1), 2015: 45–49.
    Jump up ^ Donoho, David L. (2006-06-01). "For most large underdetermined systems of linear equations the minimal 𝓁1-norm solution is also the sparsest solution" . Communications on Pure and Applied Mathematics . 59 (6): 797–829. doi : 10.1002/cpa.20132 . ISSN   1097-0312 .  
    Jump up ^ Engan, K.; Aase, S.O.; Hakon Husoy, J. (1999-01-01). "Method of optimal directions for frame design" . 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing, 1999. Proceedings . 5 : 2443–2446 vol.5. doi : 10.1109/ICASSP.1999.760624 .  
    Jump up ^ Aharon, Michal; Elad, Michael. "Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary" . SIAM Journal on Imaging Sciences . 1 (3): 228–247. doi : 10.1137/07070156x .  
    Jump up ^ Pintér, János D. (2000-01-01). "Yair Censor and Stavros A. Zenios, Parallel Optimization — Theory, Algorithms, and Applications. Oxford University Press, New York/Oxford, 1997, xxviii+539 pages. (US $ 85.00)" . Journal of Global Optimization . 16 (1): 107–108. doi : 10.1023/A:1008311628080 . ISBN   0-19-510062-X . ISSN   0925-5001 .  
    Jump up ^ Lee, Honglak, et al. "Efficient sparse coding algorithms." Advances in neural information processing systems . 2006.
    Jump up ^ Rubinstein, R.; Bruckstein, A.M.; Elad, M. (2010-06-01). "Dictionaries for Sparse Representation Modeling" . Proceedings of the IEEE . 98 (6): 1045–1057. doi : 10.1109/JPROC.2010.2040551 . ISSN   0018-9219 .  
    Jump up ^ Engan, Kjersti; Skretting, Karl; Husøy, John H\a akon (2007-01-01). "Family of Iterative LS-based Dictionary Learning Algorithms, ILS-DLA, for Sparse Signal Representation" . Digit. Signal Process . 17 (1): 32–49. doi : 10.1016/j.dsp.2006.02.002 . ISSN   1051-2004 .  
    Jump up ^ Mairal, J.; Sapiro, G.; Elad, M. (2008-01-01). "Learning Multiscale Sparse Representations for Image and Video Restoration" . Multiscale Modeling & Simulation . 7 (1): 214–241. CiteSeerX   10.1.1.95.6239  Freely accessible . doi : 10.1137/070697653 . ISSN   1540-3459 .  
    Jump up ^ Rubinstein, R.; Zibulevsky, M.; Elad, M. (2010-03-01). "Double Sparsity: Learning Sparse Dictionaries for Sparse Signal Approximation" . IEEE Transactions on Signal Processing . 58 (3): 1553–1564. doi : 10.1109/TSP.2009.2036477 . ISSN   1053-587X .  
    Jump up ^ Mairal, Julien; Bach, Francis; Ponce, Jean; Sapiro, Guillermo (2010-03-01). "Online Learning for Matrix Factorization and Sparse Coding" . J. Mach. Learn. Res . 11 : 19–60. ISSN   1532-4435 .  
    Jump up ^ Aharon, M, M Elad, and A Bruckstein. 2006. "K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation." Signal Processing, IEEE Transactions on 54 (11): 4311-4322
    Jump up ^ Peyré, Gabriel (2008-11-06). "Sparse Modeling of Textures" . Journal of Mathematical Imaging and Vision . 34 (1): 17–31. doi : 10.1007/s10851-008-0120-3 . ISSN   0924-9907 .  
    Jump up ^ Ramirez, Ignacio; Sprechmann, Pablo; Sapiro, Guillermo (2010-01-01). "Classification and clustering via dictionary learning with structured incoherence and shared features" . 2014 IEEE Conference on Computer Vision and Pattern Recognition . Los Alamitos, CA, USA: IEEE Computer Society. 0 : 3501–3508. doi : 10.1109/CVPR.2010.5539964 . ISBN   978-1-4244-6984-0 .  
    Jump up ^ Koniusz, Piotr; Yan, Fei; Mikolajczyk, Krystian (2013-05-01). "Comparison of mid-level feature coding approaches and pooling strategies in visual concept detection" . Computer Vision and Image Understanding . 117 (5): 479–492. doi : 10.1016/j.cviu.2012.10.010 . ISSN   1077-3142 .  
    Jump up ^ Koniusz, Piotr; Yan, Fei; Gosselin, Philippe Henri; Mikolajczyk, Krystian (2017-02-24). "Higher-order occurrence pooling for bags-of-words: Visual concept detection" . IEEE Transactions on Pattern Analysis and Machine Intelligence . 39 (2): 313–326. doi : 10.1109/TPAMI.2016.2545667 . ISSN   0162-8828 .  

Retrieved from " https://en.wikipedia.org/w/index.php?title=Sparse_dictionary_learning&oldid=841576132 "
Categories :

    Machine learning
    Unsupervised learning

Navigation menu
Personal tools

    Not logged in
    Talk
    Contributions
    Create account
    Log in

Namespaces

    Article
    Talk

Variants

Views

    Read
    Edit
    View history

More

Search
Navigation

    Main page
    Contents
    Featured content
    Current events
    Random article
    Donate to Wikipedia
    Wikipedia store

Interaction

    Help
    About Wikipedia
    Community portal
    Recent changes
    Contact page

Tools

    What links here
    Related changes
    Upload file
    Special pages
    Permanent link
    Page information
    Wikidata item
    Cite this page

Print/export

    Create a book
    Download as PDF
    Printable version

Languages

Add links

    This page was last edited on 16 May 2018, at 17:43  (UTC) .
    Text is available under the Creative Commons Attribution-ShareAlike License ; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy . Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc. , a non-profit organization.

    Privacy policy
    About Wikipedia
    Disclaimers
    Contact Wikipedia
    Developers
    Cookie statement
    Mobile view

    Wikimedia Foundation
    Powered by MediaWiki

